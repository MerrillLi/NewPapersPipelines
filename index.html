<!DOCTYPE html>
<html lang="en">

<head>
    <title>New Papers Pipelines</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                New arxiv Papers in 7 Days
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-27T00:00:00Z">2024-03-27</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mini-Gemini: Mining the Potential of Multi-modality Vision Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, Jiaya Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce Mini-Gemini, a simple and effective framework
enhancing multi-modality Vision Language Models (VLMs). Despite the
advancements in VLMs facilitating basic visual dialog and reasoning, a
performance gap persists compared to advanced models like GPT-4 and Gemini. We
try to narrow the gap by mining the potential of VLMs for better performance
and any-to-any workflow from three aspects, i.e., high-resolution visual
tokens, high-quality data, and VLM-guided generation. To enhance visual tokens,
we propose to utilize an additional visual encoder for high-resolution
refinement without increasing the visual token count. We further construct a
high-quality dataset that promotes precise image comprehension and
reasoning-based generation, expanding the operational scope of current VLMs. In
general, Mini-Gemini further mines the potential of VLMs and empowers current
frameworks with image understanding, reasoning, and generation simultaneously.
Mini-Gemini supports a series of dense and MoE Large Language Models (LLMs)
from 2B to 34B. It is demonstrated to achieve leading performance in several
zero-shot benchmarks and even surpasses the developed private models. Code and
models are available at https://github.com/dvlab-research/MiniGemini.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and models are available at
  https://github.com/dvlab-research/MiniGemini</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth
  Estimation <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suraj Patni, Aradhye Agarwal, Chetan Arora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the absence of parallax cues, a learning-based single image depth
estimation (SIDE) model relies heavily on shading and contextual cues in the
image. While this simplicity is attractive, it is necessary to train such
models on large and varied datasets, which are difficult to capture. It has
been shown that using embeddings from pre-trained foundational models, such as
CLIP, improves zero shot transfer in several applications. Taking inspiration
from this, in our paper we explore the use of global image priors generated
from a pre-trained ViT model to provide more detailed contextual information.
We argue that the embedding vector from a ViT model, pre-trained on a large
dataset, captures greater relevant information for SIDE than the usual route of
generating pseudo image captions, followed by CLIP based text embeddings. Based
on this idea, we propose a new SIDE model using a diffusion backbone which is
conditioned on ViT embeddings. Our proposed design establishes a new
state-of-the-art (SOTA) for SIDE on NYUv2 dataset, achieving Abs Rel error of
0.059(14% improvement) compared to 0.069 by the current SOTA (VPD). And on
KITTI dataset, achieving Sq Rel error of 0.139 (2% improvement) compared to
0.142 by the current SOTA (GEDepth). For zero-shot transfer with a model
trained on NYUv2, we report mean relative improvement of (20%, 23%, 81%, 25%)
over NeWCRFs on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%,
18%, 45%, 9%) by ZoeDepth. The code is available at
https://github.com/Aradhye2002/EcoDepth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-form factuality in large language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, Quoc V. Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often generate content that contains factual
errors when responding to fact-seeking prompts on open-ended topics. To
benchmark a model's long-form factuality in open domains, we first use GPT-4 to
generate LongFact, a prompt set comprising thousands of questions spanning 38
topics. We then propose that LLM agents can be used as automated evaluators for
long-form factuality through a method which we call Search-Augmented Factuality
Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into
a set of individual facts and to evaluate the accuracy of each fact using a
multi-step reasoning process comprising sending search queries to Google Search
and determining whether a fact is supported by the search results. Furthermore,
we propose extending F1 score as an aggregated metric for long-form factuality.
To do so, we balance the percentage of supported facts in a response
(precision) with the percentage of provided facts relative to a hyperparameter
representing a user's preferred response length (recall).
  Empirically, we demonstrate that LLM agents can achieve superhuman rating
performance - on a set of ~16k individual facts, SAFE agrees with crowdsourced
human annotators 72% of the time, and on a random subset of 100 disagreement
cases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times
cheaper than human annotators. We also benchmark thirteen language models on
LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding
that larger language models generally achieve better long-form factuality.
LongFact, SAFE, and all experimental code are available at
https://github.com/google-deepmind/long-form-factuality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gamba: Marry Gaussian Splatting with Mamba for single view 3D
  reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiuhong Shen, Xuanyu Yi, Zike Wu, Pan Zhou, Hanwang Zhang, Shuicheng Yan, Xinchao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We tackle the challenge of efficiently reconstructing a 3D asset from a
single image with growing demands for automated 3D content creation pipelines.
Previous methods primarily rely on Score Distillation Sampling (SDS) and Neural
Radiance Fields (NeRF). Despite their significant success, these approaches
encounter practical limitations due to lengthy optimization and considerable
memory usage. In this report, we introduce Gamba, an end-to-end amortized 3D
reconstruction model from single-view images, emphasizing two main insights:
(1) 3D representation: leveraging a large number of 3D Gaussians for an
efficient 3D Gaussian splatting process; (2) Backbone design: introducing a
Mamba-based sequential network that facilitates context-dependent reasoning and
linear scalability with the sequence (token) length, accommodating a
substantial number of Gaussians. Gamba incorporates significant advancements in
data preprocessing, regularization design, and training methodologies. We
assessed Gamba against existing optimization-based and feed-forward 3D
generation approaches using the real-world scanned OmniObject3D dataset. Here,
Gamba demonstrates competitive generation capabilities, both qualitatively and
quantitatively, while achieving remarkable speed, approximately 0.6 second on a
single NVIDIA A100 GPU.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ImageNet-D: <span class="highlight-title">Benchmark</span>ing Neural Network Robustness on Diffusion
  Synthetic Object <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenshuang Zhang, Fei Pan, Junmo Kim, In So Kweon, Chengzhi Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We establish rigorous benchmarks for visual perception robustness. Synthetic
images such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific
type of evaluation over synthetic corruptions, backgrounds, and textures, yet
those robustness benchmarks are restricted in specified variations and have low
synthetic quality. In this work, we introduce generative model as a data source
for synthesizing hard images that benchmark deep models' robustness. Leveraging
diffusion models, we are able to generate images with more diversified
backgrounds, textures, and materials than any prior work, where we term this
benchmark as ImageNet-D. Experimental results show that ImageNet-D results in a
significant accuracy drop to a range of vision models, from the standard ResNet
visual classifier to the latest foundation models like CLIP and MiniGPT-4,
significantly reducing their accuracy by up to 60\%. Our work suggests that
diffusion models can be an effective source to test vision models. The code and
dataset are available at https://github.com/chenshuang-zhang/imagenet_d.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Superior Parallel Big Data Clustering through Competitive Stochastic
  Sample Size Optimization in Big-means 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rustam Mussabayev, Ravil Mussabayev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel K-means clustering algorithm, an advancement on
the conventional Big-means methodology. The proposed method efficiently
integrates parallel processing, stochastic sampling, and competitive
optimization to create a scalable variant designed for big data applications.
It addresses scalability and computation time challenges typically faced with
traditional techniques. The algorithm adjusts sample sizes dynamically for each
worker during execution, optimizing performance. Data from these sample sizes
are continually analyzed, facilitating the identification of the most efficient
configuration. By incorporating a competitive element among workers using
different sample sizes, efficiency within the Big-means algorithm is further
stimulated. In essence, the algorithm balances computational time and
clustering quality by employing a stochastic, competitive sampling strategy in
a parallel computing setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ModaLink: Unifying Modalities for Efficient Image-to-PointCloud Place
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weidong Xie, Lun Luo, Nanfei Ye, Yi Ren, Shaoyi Du, Minhang Wang, Jintao Xu, Rui Ai, Weihao Gu, Xieyuanli Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Place recognition is an important task for robots and autonomous cars to
localize themselves and close loops in pre-built maps. While single-modal
sensor-based methods have shown satisfactory performance, cross-modal place
recognition that retrieving images from a point-cloud database remains a
challenging problem. Current cross-modal methods transform images into 3D
points using depth estimation for modality conversion, which are usually
computationally intensive and need expensive labeled data for depth
supervision. In this work, we introduce a fast and lightweight framework to
encode images and point clouds into place-distinctive descriptors. We propose
an effective Field of View (FoV) transformation module to convert point clouds
into an analogous modality as images. This module eliminates the necessity for
depth estimation and helps subsequent modules achieve real-time performance. We
further design a non-negative factorization-based encoder to extract mutually
consistent semantic features between point clouds and images. This encoder
yields more distinctive global descriptors for retrieval. Experimental results
on the KITTI dataset show that our proposed methods achieve state-of-the-art
performance while running in real time. Additional evaluation on the HAOMO
dataset covering a 17 km trajectory further shows the practical generalization
capabilities. We have released the implementation of our methods as open source
at: https://github.com/haomo-ai/ModaLink.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 11 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detection of subclinical atherosclerosis by image-based deep learning on
  chest x-ray 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guglielmo Gallone, Francesco Iodice, Alberto Presta, Davide Tore, Ovidio de Filippo, Michele Visciano, Carlo Alberto Barbano, Alessandro Serafini, Paola Gorrini, Alessandro Bruno, Walter Grosso Marra, James Hughes, Mario Iannaccone, Paolo Fonio, Attilio Fiandrotti, Alessandro Depaoli, Marco Grangetto, Gaetano Maria de Ferrari, Fabrizio D'Ascenzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aims. To develop a deep-learning based system for recognition of subclinical
atherosclerosis on a plain frontal chest x-ray. Methods and Results. A
deep-learning algorithm to predict coronary artery calcium (CAC) score (the
AI-CAC model) was developed on 460 chest x-ray (80% training cohort, 20%
internal validation cohort) of primary prevention patients (58.4% male, median
age 63 [51-74] years) with available paired chest x-ray and chest computed
tomography (CT) indicated for any clinical reason and performed within 3
months. The CAC score calculated on chest CT was used as ground truth. The
model was validated on an temporally-independent cohort of 90 patients from the
same institution (external validation). The diagnostic accuracy of the AI-CAC
model assessed by the area under the curve (AUC) was the primary outcome.
Overall, median AI-CAC score was 35 (0-388) and 28.9% patients had no AI-CAC.
AUC of the AI-CAC model to identify a CAC>0 was 0.90 in the internal validation
cohort and 0.77 in the external validation cohort. Sensitivity was consistently
above 92% in both cohorts. In the overall cohort (n=540), among patients with
AI-CAC=0, a single ASCVD event occurred, after 4.3 years. Patients with
AI-CAC>0 had significantly higher Kaplan Meier estimates for ASCVD events
(13.5% vs. 3.4%, log-rank=0.013). Conclusion. The AI-CAC model seems to
accurately detect subclinical atherosclerosis on chest x-ray with elevated
sensitivity, and to predict ASCVD events with elevated negative predictive
value. Adoption of the AI-CAC model to refine CV risk stratification or as an
opportunistic screening tool requires prospective evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to European Heart Journal - Cardiovascular Imaging Added
  also the additional material 44 pages (30 main paper, 14 additional
  material), 14 figures (5 main manuscript, 9 additional material)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Many-Objective Evolutionary Influence Maximization: Balancing Spread,
  Budget, Fairness, and Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elia Cunegatti, Leonardo Lucio Custode, Giovanni Iacca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Influence Maximization (IM) problem seeks to discover the set of nodes in
a graph that can spread the information propagation at most. This problem is
known to be NP-hard, and it is usually studied by maximizing the influence
(spread) and, optionally, optimizing a second objective, such as minimizing the
seed set size or maximizing the influence fairness. However, in many practical
scenarios multiple aspects of the IM problem must be optimized at the same
time. In this work, we propose a first case study where several IM-specific
objective functions, namely budget, fairness, communities, and time, are
optimized on top of the maximization of influence and minimization of the seed
set size. To this aim, we introduce MOEIM (Many-Objective Evolutionary
Algorithm for Influence Maximization) a Multi-Objective Evolutionary Algorithm
(MOEA) based on NSGA-II incorporating graph-aware operators and a smart
initialization. We compare MOEIM in two experimental settings, including a
total of nine graph datasets, two heuristic methods, a related MOEA, and a
state-of-the-art Deep Learning approach. The experiments show that MOEIM
overall outperforms the competitors in most of the tested many-objective
settings. To conclude, we also investigate the correlation between the
objectives, leading to novel insights into the topic. The codebase is available
at https://github.com/eliacunegatti/MOEIM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Genetic and Evolutionary Computation Conference (GECCO
  24 Companion), July 14 18, 2024, Melbourne, VIC, Australia. ACM, New York,
  NY, USA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding the Learning Dynamics of Alignment with Human Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shawn Im, Yixuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning large language models (LLMs) with human intentions has become a
critical task for safely deploying models in real-world systems. While existing
alignment approaches have seen empirical success, theoretically understanding
how these methods affect model behavior remains an open question. Our work
provides an initial attempt to theoretically analyze the learning dynamics of
human preference alignment. We formally show how the distribution of preference
datasets influences the rate of model updates and provide rigorous guarantees
on the training accuracy. Our theory also reveals an intricate phenomenon where
the optimization is prone to prioritizing certain behaviors with higher
preference distinguishability. We empirically validate our findings on
contemporary LLMs and alignment tasks, reinforcing our theoretical insights and
shedding light on considerations for future alignment approaches. Disclaimer:
This paper contains potentially offensive text; reader discretion is advised.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Manufacturing Quality <span class="highlight-title">Predict</span>ion Models through the
  Integration of Explainability Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Gross, Helge Spieker, Arnaud Gotlieb, Ricardo Knoblauch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research presents a method that utilizes explainability techniques to
amplify the performance of machine learning (ML) models in forecasting the
quality of milling processes, as demonstrated in this paper through a
manufacturing use case. The methodology entails the initial training of ML
models, followed by a fine-tuning phase where irrelevant features identified
through explainability methods are eliminated. This procedural refinement
results in performance enhancements, paving the way for potential reductions in
manufacturing costs and a better understanding of the trained ML models. This
study highlights the usefulness of explainability techniques in both explaining
and optimizing predictive models in the manufacturing realm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probabilistic Model Checking of Stochastic Reinforcement Learning
  Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Gross, Helge Spieker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a method to verify stochastic reinforcement learning (RL)
policies. This approach is compatible with any RL algorithm as long as the
algorithm and its corresponding environment collectively adhere to the Markov
property. In this setting, the future state of the environment should depend
solely on its current state and the action executed, independent of any
previous states or actions. Our method integrates a verification technique,
referred to as model checking, with RL, leveraging a Markov decision process, a
trained RL policy, and a probabilistic computation tree logic (PCTL) formula to
build a formal model that can be subsequently verified via the model checker
Storm. We demonstrate our method's applicability across multiple benchmarks,
comparing it to baseline methods called deterministic safety estimates and
naive monolithic model checking. Our results show that our method is suited to
verify stochastic RL policies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-Supervised Learning for Deep Causal Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasin Ibrahim, Hermione Warr, Konstantinos Kamnitsas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing models that can answer questions of the form "How would $x$ change
if $y$ had been $z$?" is fundamental for advancing medical image analysis.
Training causal generative models that address such counterfactual questions,
though, currently requires that all relevant variables have been observed and
that corresponding labels are available in training data. However, clinical
data may not have complete records for all patients and state of the art causal
generative models are unable to take full advantage of this. We thus develop,
for the first time, a semi-supervised deep causal generative model that
exploits the causal relationships between variables to maximise the use of all
available data. We explore this in the setting where each sample is either
fully labelled or fully unlabelled, as well as the more clinically realistic
case of having different labels missing for each sample. We leverage techniques
from causal inference to infer missing values and subsequently generate
realistic counterfactuals, even for samples with incomplete labels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Hallucinations in Large Vision-Language Models with
  Instruction Contrastive Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xintong Wang, Jingheng Pan, Liang Ding, Chris Biemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) are increasingly adept at generating
contextually detailed and coherent responses from visual inputs. However, their
application in multimodal decision-making and open-ended generation is hindered
by a notable rate of hallucinations, where generated text inaccurately
represents the visual contents. To address this issue, this paper introduces
the Instruction Contrastive Decoding (ICD) method, a novel approach designed to
reduce hallucinations during LVLM inference. Our method is inspired by our
observation that what we call disturbance instructions significantly exacerbate
hallucinations in multimodal fusion modules. ICD contrasts distributions from
standard and instruction disturbance, thereby increasing alignment uncertainty
and effectively subtracting hallucinated concepts from the original
distribution. Through comprehensive experiments on discriminative benchmarks
(POPE and MME) and a generative benchmark (LLaVa-Bench), we demonstrate that
ICD significantly mitigates both object-level and attribute-level
hallucinations. Moreover, our method not only addresses hallucinations but also
significantly enhances the general perception and recognition capabilities of
LVLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAT-NGP : Unleashing Neural Graphics Primitives for Fast Relightable
  Transient-Free 3D reconstruction from Satellite Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Camille Billouard, Dawa Derksen, Emmanuelle Sarrazin, Bruno Vallet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current stereo-vision pipelines produce high accuracy 3D reconstruction when
using multiple pairs or triplets of satellite images. However, these pipelines
are sensitive to the changes between images that can occur as a result of
multi-date acquisitions. Such variations are mainly due to variable shadows,
reflexions and transient objects (cars, vegetation). To take such changes into
account, Neural Radiance Fields (NeRF) have recently been applied to multi-date
satellite imagery. However, Neural methods are very compute-intensive, taking
dozens of hours to learn, compared with minutes for standard stereo-vision
pipelines. Following the ideas of Instant Neural Graphics Primitives we propose
to use an efficient sampling strategy and multi-resolution hash encoding to
accelerate the learning. Our model, Satellite Neural Graphics Primitives
(SAT-NGP) decreases the learning time to 15 minutes while maintaining the
quality of the 3D reconstruction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, 1 table; Accepted to International Geoscience and
  Remote Sensing Symposium (IGARSS) 2024; Code available at
  https://github.com/Ellimac0/SAT-NGP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contrastive Learning with Orthonormal Anchors (CLOA) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanran Li, Daniel Pimentel-Alarcón
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study focuses on addressing the instability issues prevalent in
contrastive learning, specifically examining the InfoNCE loss function and its
derivatives. We reveal a critical observation that these loss functions exhibit
a restrictive behavior, leading to a convergence phenomenon where embeddings
tend to merge into a singular point. This "over-fusion" effect detrimentally
affects classification accuracy in subsequent supervised-learning tasks.
Through theoretical analysis, we demonstrate that embeddings, when equalized or
confined to a rank-1 linear subspace, represent a local minimum for InfoNCE. In
response to this challenge, our research introduces an innovative strategy that
leverages the same or fewer labeled data than typically used in the fine-tuning
phase. The loss we proposed, Orthonormal Anchor Regression Loss, is designed to
disentangle embedding clusters, significantly enhancing the distinctiveness of
each embedding while simultaneously ensuring their aggregation into dense,
well-defined clusters. Our method demonstrates remarkable improvements with
just a fraction of the conventional label requirements, as evidenced by our
results on CIFAR10 and CIFAR100 datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Annolid: Annotate, Segment, and Track Anything You Need 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Yang, Thomas A. Cleland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Annolid is a deep learning-based software package designed for the
segmentation, labeling, and tracking of research targets within video files,
focusing primarily on animal behavior analysis. Based on state-of-the-art
instance segmentation methods, Annolid now harnesses the Cutie video object
segmentation model to achieve resilient, markerless tracking of multiple
animals from single annotated frames, even in environments in which they may be
partially or entirely concealed by environmental features or by one another.
Our integration of Segment Anything and Grounding-DINO strategies additionally
enables the automatic masking and segmentation of recognizable animals and
objects by text command, removing the need for manual annotation. Annolid's
comprehensive approach to object segmentation flexibly accommodates a broad
spectrum of behavior analysis applications, enabling the classification of
diverse behavioral states such as freezing, digging, pup huddling, and social
interactions in addition to the tracking of animals and their body parts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TransFusion: Contrastive Learning with Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanran Li, Daniel Pimentel-Alarcón
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel framework, TransFusion, designed to make the
process of contrastive learning more analytical and explainable. TransFusion
consists of attention blocks whose softmax being replaced by ReLU, and its
final block's weighted-sum operation is truncated to leave the adjacency matrix
as the output. The model is trained by minimizing the Jensen-Shannon Divergence
between its output and the target affinity matrix, which indicates whether each
pair of samples belongs to the same or different classes. The main contribution
of TransFusion lies in defining a theoretical limit for answering two
fundamental questions in the field: the maximum level of data augmentation and
the minimum batch size required for effective contrastive learning.
Furthermore, experimental results indicate that TransFusion successfully
extracts features that isolate clusters from complex real-world data, leading
to improved classification accuracy in downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 4 figures,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aiming for Relevance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bar Eini Porat, Danny Eytan, Uri Shalit
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vital signs are crucial in intensive care units (ICUs). They are used to
track the patient's state and to identify clinically significant changes.
Predicting vital sign trajectories is valuable for early detection of adverse
events. However, conventional machine learning metrics like RMSE often fail to
capture the true clinical relevance of such predictions. We introduce novel
vital sign prediction performance metrics that align with clinical contexts,
focusing on deviations from clinical norms, overall trends, and trend
deviations. These metrics are derived from empirical utility curves obtained in
a previous study through interviews with ICU clinicians. We validate the
metrics' usefulness using simulated and real clinical datasets (MIMIC and
eICU). Furthermore, we employ these metrics as loss functions for neural
networks, resulting in models that excel in predicting clinically significant
events. This research paves the way for clinically relevant machine learning
model evaluation and optimization, promising to improve ICU patient care. 10
pages, 9 figures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures, AMIA Informatics 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ INEXA: Interactive and Explainable Process Model Abstraction Through
  Object-Centric Process Mining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Janik-Vasily Benzin, Gyunam Park, Juergen Mangler, Stefanie Rinderle-Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Process events are recorded by multiple information systems at different
granularity levels. Based on the resulting event logs, process models are
discovered at different granularity levels, as well. Events stored at a
fine-grained granularity level, for example, may hinder the discovered process
model to be displayed due the high number of resulting model elements. The
discovered process model of a real-world manufacturing process, for example,
consists of 1,489 model elements and over 2,000 arcs. Existing process model
abstraction techniques could help reducing the size of the model, but would
disconnect it from the underlying event log. Existing event abstraction
techniques do neither support the analysis of mixed granularity levels, nor
interactive exploration of a suitable granularity level. To enable the
exploration of discovered process models at different granularity levels, we
propose INEXA, an interactive, explainable process model abstraction method
that keeps the link to the event log. As a starting point, INEXA aggregates
large process models to a "displayable" size, e.g., for the manufacturing use
case to a process model with 58 model elements. Then, the process analyst can
explore granularity levels interactively, while applied abstractions are
automatically traced in the event log for explainability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spikewhisper: Temporal Spike Backdoor Attacks on <span class="highlight-title">Federated</span> Neuromorphic
  Learning over Low-power Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanqing Fu, Gaolei Li, Jun Wu, Jianhua Li, Xi Lin, Kai Zhou, Yuchen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated neuromorphic learning (FedNL) leverages event-driven spiking neural
networks and federated learning frameworks to effectively execute intelligent
analysis tasks over amounts of distributed low-power devices but also perform
vulnerability to poisoning attacks. The threat of backdoor attacks on
traditional deep neural networks typically comes from time-invariant data.
However, in FedNL, unknown threats may be hidden in time-varying spike signals.
In this paper, we start to explore a novel vulnerability of FedNL-based systems
with the concept of time division multiplexing, termed Spikewhisper, which
allows attackers to evade detection as much as possible, as multiple malicious
clients can imperceptibly poison with different triggers at different
timeslices. In particular, the stealthiness of Spikewhisper is derived from the
time-domain divisibility of global triggers, in which each malicious client
pastes only one local trigger to a certain timeslice in the neuromorphic
sample, and also the polarity and motion of each local trigger can be
configured by attackers. Extensive experiments based on two different
neuromorphic datasets demonstrate that the attack success rate of Spikewispher
is higher than the temporally centralized attacks. Besides, it is validated
that the effect of Spikewispher is sensitive to the trigger duration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAP: Retrieval-Augmented Planner for Adaptive Procedure Planning in
  Instructional Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Zare, Yulei Niu, Hammad Ayyubi, Shih-fu Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Procedure Planning in instructional videos entails generating a sequence of
action steps based on visual observations of the initial and target states.
Despite the rapid progress in this task, there remain several critical
challenges to be solved: (1) Adaptive procedures: Prior works hold an
unrealistic assumption that the number of action steps is known and fixed,
leading to non-generalizable models in real-world scenarios where the sequence
length varies. (2) Temporal relation: Understanding the step temporal relation
knowledge is essential in producing reasonable and executable plans. (3)
Annotation cost: Annotating instructional videos with step-level labels (i.e.,
timestamp) or sequence-level labels (i.e., action category) is demanding and
labor-intensive, limiting its generalizability to large-scale datasets.In this
work, we propose a new and practical setting, called adaptive procedure
planning in instructional videos, where the procedure length is not fixed or
pre-determined. To address these challenges we introduce Retrieval-Augmented
Planner (RAP) model. Specifically, for adaptive procedures, RAP adaptively
determines the conclusion of actions using an auto-regressive model
architecture. For temporal relation, RAP establishes an external memory module
to explicitly retrieve the most relevant state-action pairs from the training
videos and revises the generated procedures. To tackle high annotation cost,
RAP utilizes a weakly-supervised learning manner to expand the training dataset
to other task-relevant, unannotated videos by generating pseudo labels for
action steps. Experiments on CrossTask and COIN benchmarks show the superiority
of RAP over traditional fixed-length models, establishing it as a strong
baseline solution for adaptive procedure planning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 6 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Homogeneous Tokenizer Matters: Homogeneous Visual Tokenizer for Remote
  Sensing Image Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Run Shao, Zhaoyang Zhang, Chao Tao, Yunsheng Zhang, Chengli Peng, Haifeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The tokenizer, as one of the fundamental components of large models, has long
been overlooked or even misunderstood in visual tasks. One key factor of the
great comprehension power of the large language model is that natural language
tokenizers utilize meaningful words or subwords as the basic elements of
language. In contrast, mainstream visual tokenizers, represented by patch-based
methods such as Patch Embed, rely on meaningless rectangular patches as basic
elements of vision, which cannot serve as effectively as words or subwords in
language. Starting from the essence of the tokenizer, we defined semantically
independent regions (SIRs) for vision. We designed a simple HOmogeneous visual
tOKenizer: HOOK. HOOK mainly consists of two modules: the Object Perception
Module (OPM) and the Object Vectorization Module (OVM). To achieve homogeneity,
the OPM splits the image into 4*4 pixel seeds and then utilizes the attention
mechanism to perceive SIRs. The OVM employs cross-attention to merge seeds
within the same SIR. To achieve adaptability, the OVM defines a variable number
of learnable vectors as cross-attention queries, allowing for the adjustment of
token quantity. We conducted experiments on the NWPU-RESISC45, WHU-RS19
classification dataset, and GID5 segmentation dataset for sparse and dense
tasks. The results demonstrate that the visual tokens obtained by HOOK
correspond to individual objects, which demonstrates homogeneity. HOOK
outperformed Patch Embed by 6\% and 10\% in the two tasks and achieved
state-of-the-art performance compared to the baselines used for comparison.
Compared to Patch Embed, which requires more than one hundred tokens for one
image, HOOK requires only 6 and 8 tokens for sparse and dense tasks,
respectively, resulting in efficiency improvements of 1.5 to 2.8 times. The
code is available at https://github.com/GeoX-Lab/Hook.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 8 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-Informed <span class="highlight-title">Graph Neural Network</span>s for Water Distribution Systems <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inaam Ashraf, Janine Strotherm, Luca Hermes, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Water distribution systems (WDS) are an integral part of critical
infrastructure which is pivotal to urban development. As 70% of the world's
population will likely live in urban environments in 2050, efficient simulation
and planning tools for WDS play a crucial role in reaching UN's sustainable
developmental goal (SDG) 6 - "Clean water and sanitation for all". In this
realm, we propose a novel and efficient machine learning emulator, more
precisely, a physics-informed deep learning (DL) model, for hydraulic state
estimation in WDS. Using a recursive approach, our model only needs a few graph
convolutional neural network (GCN) layers and employs an innovative algorithm
based on message passing. Unlike conventional machine learning tasks, the model
uses hydraulic principles to infer two additional hydraulic state features in
the process of reconstructing the available ground truth feature in an
unsupervised manner. To the best of our knowledge, this is the first DL
approach to emulate the popular hydraulic simulator EPANET, utilizing no
additional information. Like most DL models and unlike the hydraulic simulator,
our model demonstrates vastly faster emulation times that do not increase
drastically with the size of the WDS. Moreover, we achieve high accuracy on the
ground truth and very similar results compared to the hydraulic simulator as
demonstrated through experiments on five real-world WDS datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of the paper with the same title published at
  Proceedings of the AAAI Conference on Artificial Intelligence 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PDNNet: PDN-Aware GNN-CNN Heterogeneous Network for Dynamic IR Drop
  <span class="highlight-title">Predict</span>ion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Zhao, Zhuomin Chai, Xun Jiang, Yibo Lin, Runsheng Wang, Ru Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  IR drop on the power delivery network (PDN) is closely related to PDN's
configuration and cell current consumption. As the integrated circuit (IC)
design is growing larger, dynamic IR drop simulation becomes computationally
unaffordable and machine learning based IR drop prediction has been explored as
a promising solution. Although CNN-based methods have been adapted to IR drop
prediction task in several works, the shortcomings of overlooking PDN
configuration is non-negligible. In this paper, we consider not only how to
properly represent cell-PDN relation, but also how to model IR drop following
its physical nature in the feature aggregation procedure. Thus, we propose a
novel graph structure, PDNGraph, to unify the representations of the PDN
structure and the fine-grained cell-PDN relation. We further propose a
dual-branch heterogeneous network, PDNNet, incorporating two parallel GNN-CNN
branches to favorably capture the above features during the learning process.
Several key designs are presented to make the dynamic IR drop prediction highly
effective and interpretable. We are the first work to apply graph structure to
deep-learning based dynamic IR drop prediction method. Experiments show that
PDNNet outperforms the state-of-the-art CNN-based methods by up to 39.3%
reduction in prediction error and achieves 545x speedup compared to the
commercial tool, which demonstrates the superiority of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Architecture Search for Sentence Classification with BERT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18547v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18547v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Kenneweg, Sarah Schröder, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre training of language models on large text corpora is common practice in
Natural Language Processing. Following, fine tuning of these models is
performed to achieve the best results on a variety of tasks. In this paper we
question the common practice of only adding a single output layer as a
classification head on top of the network. We perform an AutoML search to find
architectures that outperform the current single layer at only a small compute
cost. We validate our classification architecture on a variety of NLP
benchmarks from the GLUE dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Heatmap-Guided 6-Dof Grasp Detection in Cluttered Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18546v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18546v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siang Chen, Wei Tang, Pengwei Xie, Wenming Yang, Guijin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fast and robust object grasping in clutter is a crucial component of
robotics. Most current works resort to the whole observed point cloud for 6-Dof
grasp generation, ignoring the guidance information excavated from global
semantics, thus limiting high-quality grasp generation and real-time
performance. In this work, we show that the widely used heatmaps are
underestimated in the efficiency of 6-Dof grasp generation. Therefore, we
propose an effective local grasp generator combined with grasp heatmaps as
guidance, which infers in a global-to-local semantic-to-point way.
Specifically, Gaussian encoding and the grid-based strategy are applied to
predict grasp heatmaps as guidance to aggregate local points into graspable
regions and provide global semantic information. Further, a novel non-uniform
anchor sampling mechanism is designed to improve grasp accuracy and diversity.
Benefiting from the high-efficiency encoding in the image space and focusing on
points in local graspable regions, our framework can perform high-quality grasp
detection in real-time and achieve state-of-the-art results. In addition, real
robot experiments demonstrate the effectiveness of our method with a success
rate of 94% and a clutter completion rate of 100%. Our code is available at
https://github.com/THU-VCLab/HGGD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extensive results on GraspNet-1B dataset</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Path Towards Legal Autonomy: An interoperable and explainable approach
  to extracting, transforming, loading and computing legal information using
  large language models, expert systems and Bayesian networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Axel Constant, Hannes Westermann, Bryan Wilson, Alex Kiefer, Ines Hipolito, Sylvain Pronovost, Steven Swanson, Mahault Albarracin, Maxwell J. D. Ramstead
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Legal autonomy - the lawful activity of artificial intelligence agents - can
be achieved in one of two ways. It can be achieved either by imposing
constraints on AI actors such as developers, deployers and users, and on AI
resources such as data, or by imposing constraints on the range and scope of
the impact that AI agents can have on the environment. The latter approach
involves encoding extant rules concerning AI driven devices into the software
of AI agents controlling those devices (e.g., encoding rules about limitations
on zones of operations into the agent software of an autonomous drone device).
This is a challenge since the effectivity of such an approach requires a method
of extracting, loading, transforming and computing legal information that would
be both explainable and legally interoperable, and that would enable AI agents
to reason about the law. In this paper, we sketch a proof of principle for such
a method using large language models (LLMs), expert legal systems known as
legal decision paths, and Bayesian networks. We then show how the proposed
method could be applied to extant regulation in matters of autonomous cars,
such as the California Vehicle Code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Behavior-Based <span class="highlight-title">Recommendation</span> System for E-commerce 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reza Barzegar Nozari, Mahdi Divsalar, Sepehr Akbarzadeh Abkenar, Mohammadreza Fadavi Amiri, Ali Divsalar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The majority of existing recommender systems rely on user ratings, which are
limited by the lack of user collaboration and the sparsity problem. To address
these issues, this study proposes a behavior-based recommender system that
leverages customers' natural behaviors, such as browsing and clicking, on
e-commerce platforms. The proposed recommendation system involves clustering
active customers, determining neighborhoods, collecting similar users,
calculating product reputation based on similar users, and recommending
high-reputation products. To overcome the complexity of customer behaviors and
traditional clustering methods, an unsupervised clustering approach based on
product categories is developed to enhance the recommendation methodology. This
study makes notable contributions in several aspects. Firstly, a groundbreaking
behavior-based recommendation methodology is developed, incorporating customer
behavior to generate accurate and tailored recommendations leading to improved
customer satisfaction and engagement. Secondly, an original unsupervised
clustering method, focusing on product categories, enables more precise
clustering and facilitates accurate recommendations. Finally, an approach to
determine neighborhoods for active customers within clusters is established,
ensuring grouping of customers with similar behavioral patterns to enhance
recommendation accuracy and relevance. The proposed recommendation methodology
and clustering method contribute to improved recommendation performance,
offering valuable insights for researchers and practitioners in the field of
e-commerce recommendation systems. Additionally, the proposed method
outperforms benchmark methods in experiments conducted using a behavior dataset
from the well-known e-commerce site Alibaba.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Line Search Methods for Large Scale Neural Network Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18519v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18519v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Kenneweg, Tristan Kenneweg, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent studies, line search methods have shown significant improvements in
the performance of traditional stochastic gradient descent techniques,
eliminating the need for a specific learning rate schedule. In this paper, we
identify existing issues in state-of-the-art line search methods, propose
enhancements, and rigorously evaluate their effectiveness. We test these
methods on larger datasets and more complex data domains than before.
Specifically, we improve the Armijo line search by integrating the momentum
term from ADAM in its search direction, enabling efficient large-scale
training, a task that was previously prone to failure using Armijo line search
methods. Our optimization approach outperforms both the previous Armijo
implementation and tuned learning rate schedules for Adam. Our evaluation
focuses on Transformers and CNNs in the domains of NLP and image data. Our work
is publicly available as a Python package, which provides a hyperparameter free
Pytorch optimizer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Faster Convergence for Transformer Fine-tuning with Line Search Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Kenneweg, Leonardo Galli, Tristan Kenneweg, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have shown that line search methods greatly increase performance
of traditional stochastic gradient descent methods on a variety of datasets and
architectures [1], [2]. In this work we succeed in extending line search
methods to the novel and highly popular Transformer architecture and dataset
domains in natural language processing. More specifically, we combine the
Armijo line search with the Adam optimizer and extend it by subdividing the
networks architecture into sensible units and perform the line search
separately on these local units. Our optimization method outperforms the
traditional Adam optimizer and achieves significant performance improvements
for small data sets or small training budgets, while performing equal or better
for other tested cases. Our work is publicly available as a python package,
which provides a hyperparameter-free pytorch optimizer that is compatible with
arbitrary network architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Impact of Employing Weather <span class="highlight-title">Forecast</span> Data as Input to the Estimation of
  Evapotranspiration by Deep Neural Network Models <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro J. Vaz, Gabriela Schütz, Carlos Guerrero, Pedro J. S. Cardoso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reference Evapotranspiration (ET0) is a key parameter for designing smart
irrigation scheduling, since it is related by a coefficient to the water needs
of a crop. The United Nations Food and Agriculture Organization, proposed a
standard method for ET0 computation (FAO56PM), based on the parameterization of
the Penman-Monteith equation, that is widely adopted in the literature. To
compute ET0 using the FAO56-PM method, four main weather parameters are needed:
temperature, humidity, wind, and solar radiation (SR). One way to make daily
ET0 estimations for future days is to use freely available weather forecast
services (WFSs), where many meteorological parameters are estimated up to the
next 15 days. A problem with this method is that currently, SR is not provided
as a free forecast parameter on most of those online services or, normally,
such forecasts present a financial cost penalty. For this reason, several ET0
estimation models using machine and deep learning were developed and presented
in the literature, that use as input features a reduced set of carefully
selected weather parameters, that are compatible with common freely available
WFSs. However, most studies on this topic have only evaluated model performance
using data from weather stations (WSs), without considering the effect of using
weather forecast data. In this study, the performance of authors' previous
models is evaluated when using weather forecast data from two online WFSs, in
the following scenarios: (i) direct ET0 estimation by an ANN model, and (ii)
estimate SR by ANN model, and then use that estimation for ET0 computation,
using the FAO56-PM method. Employing data collected from two WFSs and a WS
located in Vale do Lobo, Portugal, the latter approach achieved the best
result, with a coefficient of determination (R2) ranging between 0.893 and
0.667, when considering forecasts up to 15 days.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A partial version of the work submitted to ESRE/INTERNATIONAL
  CONFERENCE ON ENVIRONMENTAL SCIENCES AND RENEWABLE ENERGY</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthesizing EEG Signals from Event-Related Potential Paradigms with
  Conditional Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guido Klein, Pierre Guetschel, Gianluigi Silvestri, Michael Tangermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data scarcity in the brain-computer interface field can be alleviated through
the use of generative models, specifically diffusion models. While diffusion
models have previously been successfully applied to electroencephalogram (EEG)
data, existing models lack flexibility w.r.t.~sampling or require alternative
representations of the EEG data. To overcome these limitations, we introduce a
novel approach to conditional diffusion models that utilizes classifier-free
guidance to directly generate subject-, session-, and class-specific EEG data.
In addition to commonly used metrics, domain-specific metrics are employed to
evaluate the specificity of the generated samples. The results indicate that
the proposed model can generate EEG data that resembles real data for each
subject, session, and class.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to 9th Graz BCI conference, 6 pages, 3 figures, first
  figure is split into two subfigures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Density-guided Translator Boosts Synthetic-to-Real Unsupervised Domain
  Adaptive Segmentation of 3D Point Clouds <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhimin Yuan, Wankang Zeng, Yanfei Su, Weiquan Liu, Ming Cheng, Yulan Guo, Cheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D synthetic-to-real unsupervised domain adaptive segmentation is crucial to
annotating new domains. Self-training is a competitive approach for this task,
but its performance is limited by different sensor sampling patterns (i.e.,
variations in point density) and incomplete training strategies. In this work,
we propose a density-guided translator (DGT), which translates point density
between domains, and integrates it into a two-stage self-training pipeline
named DGT-ST. First, in contrast to existing works that simultaneously conduct
data generation and feature/output alignment within unstable adversarial
training, we employ the non-learnable DGT to bridge the domain gap at the input
level. Second, to provide a well-initialized model for self-training, we
propose a category-level adversarial network in stage one that utilizes the
prototype to prevent negative transfer. Finally, by leveraging the designs
above, a domain-mixed self-training method with source-aware consistency loss
is proposed in stage two to narrow the domain gap further. Experiments on two
synthetic-to-real segmentation tasks (SynLiDAR $\rightarrow$ semanticKITTI and
SynLiDAR $\rightarrow$ semanticPOSS) demonstrate that DGT-ST outperforms
state-of-the-art methods, achieving 9.4$\%$ and 4.3$\%$ mIoU improvements,
respectively. Code is available at \url{https://github.com/yuan-zm/DGT-ST}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoBOS: Constraint-Based Online Scheduler for Human-Robot Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18459v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18459v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marina Ionova, Jan Kristof Behrens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assembly processes involving humans and robots are challenging scenarios
because the individual activities and access to shared workspace have to be
coordinated. Fixed robot programs leave no room to diverge from a fixed
protocol. Working on such a process can be stressful for the user and lead to
ineffective behavior or failure. We propose a novel approach of online
constraint-based scheduling in a reactive execution control framework
facilitating behavior trees called CoBOS. This allows the robot to adapt to
uncertain events such as delayed activity completions and activity selection
(by the human). The user will experience less stress as the robotic coworkers
adapt their behavior to best complement the human-selected activities to
complete the common task. In addition to the improved working conditions, our
algorithm leads to increased efficiency, even in highly uncertain scenarios. We
evaluate our algorithm using a probabilistic simulation study with 56000
experiments. We outperform all baselines by a margin of 4-10%. Initial real
robot experiments using a Franka Emika Panda robot and human tracking based on
HTC Vive VR gloves look promising.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoRAST: Towards Foundation Model-Powered Correlated Data Analysis in
  Resource-Constrained CPS and IoT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Hu, Jinhang Zuo, Alanis Zhao, Bob Iannucci, Carlee Joe-Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models (FMs) emerge as a promising solution to harness distributed
and diverse environmental data by leveraging prior knowledge to understand the
complicated temporal and spatial correlations within heterogeneous datasets.
Unlike distributed learning frameworks such as federated learning, which often
struggle with multimodal data, FMs can transform diverse inputs into
embeddings. This process facilitates the integration of information from
various modalities and the application of prior learning to new domains.
However, deploying FMs in resource-constrained edge systems poses significant
challenges. To this end, we introduce CoRAST, a novel learning framework that
utilizes FMs for enhanced analysis of distributed, correlated heterogeneous
data. Utilizing a server-based FM, CoRAST can exploit existing environment
information to extract temporal, spatial, and cross-modal correlations among
sensor data. This enables CoRAST to offer context-aware insights for localized
client tasks through FM-powered global representation learning. Our evaluation
on real-world weather dataset demonstrates CoRAST's ability to exploit
correlated heterogeneous data through environmental representation learning to
reduce the forecast errors by up to 50.3% compared to the baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted and to be published in 2024 IEEE International Workshop on
  Foundation Models for Cyber-Physical Systems & Internet of Things (FMSys)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ U-Sketch: An Efficient Approach for Sketch to Image Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilias Mitsouras, Eleftherios Tsonis, Paraskevi Tzouveli, Athanasios Voulodimos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated remarkable performance in text-to-image
synthesis, producing realistic and high resolution images that faithfully
adhere to the corresponding text-prompts. Despite their great success, they
still fall behind in sketch-to-image synthesis tasks, where in addition to
text-prompts, the spatial layout of the generated images has to closely follow
the outlines of certain reference sketches. Employing an MLP latent edge
predictor to guide the spatial layout of the synthesized image by predicting
edge maps at each denoising step has been recently proposed. Despite yielding
promising results, the pixel-wise operation of the MLP does not take into
account the spatial layout as a whole, and demands numerous denoising
iterations to produce satisfactory images, leading to time inefficiency. To
this end, we introduce U-Sketch, a framework featuring a U-Net type latent edge
predictor, which is capable of efficiently capturing both local and global
features, as well as spatial correlations between pixels. Moreover, we propose
the addition of a sketch simplification network that offers the user the choice
of preprocessing and simplifying input sketches for enhanced outputs. The
experimental results, corroborated by user feedback, demonstrate that our
proposed U-Net latent edge predictor leads to more realistic results, that are
better aligned with the spatial outlines of the reference sketches, while
drastically reducing the number of required denoising steps and, consequently,
the overall execution time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18421v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18421v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elliot Bolton, Abhinav Venigalla, Michihiro Yasunaga, David Hall, Betty Xiong, Tony Lee, Roxana Daneshjou, Jonathan Frankle, Percy Liang, Michael Carbin, Christopher D. Manning
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Models such as GPT-4 and Med-PaLM 2 have demonstrated impressive performance
on a wide variety of biomedical NLP tasks. However, these models have hundreds
of billions of parameters, are computationally expensive to run, require users
to send their input data over the internet, and are trained on unknown data
sources. Can smaller, more targeted models compete? To address this question,
we build and release BioMedLM, a 2.7 billion parameter GPT-style autoregressive
model trained exclusively on PubMed abstracts and full articles. When
fine-tuned, BioMedLM can produce strong multiple-choice biomedical
question-answering results competitive with much larger models, such as
achieving a score of 57.3% on MedMCQA (dev) and 69.0% on the MMLU Medical
Genetics exam. BioMedLM can also be fine-tuned to produce useful answers to
patient questions on medical topics. This demonstrates that smaller models can
potentially serve as transparent, privacy-preserving, economical and
environmentally friendly foundations for particular NLP applications, such as
in biomedicine. The model is available on the Hugging Face Hub:
https://huggingface.co/stanford-crfm/BioMedLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Channel-ensemble Approach: Unbiased and Low-variance Pseudo-labels is
  Critical for Semi-supervised Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Wu, Junbiao Pang, Baochang Zhang, Qingming Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning (SSL) is a practical challenge in computer vision.
Pseudo-label (PL) methods, e.g., FixMatch and FreeMatch, obtain the State Of
The Art (SOTA) performances in SSL. These approaches employ a
threshold-to-pseudo-label (T2L) process to generate PLs by truncating the
confidence scores of unlabeled data predicted by the self-training method.
However, self-trained models typically yield biased and high-variance
predictions, especially in the scenarios when a little labeled data are
supplied. To address this issue, we propose a lightweight channel-based
ensemble method to effectively consolidate multiple inferior PLs into the
theoretically guaranteed unbiased and low-variance one. Importantly, our
approach can be readily extended to any SSL framework, such as FixMatch or
FreeMatch. Experimental results demonstrate that our method significantly
outperforms state-of-the-art techniques on CIFAR10/100 in terms of
effectiveness and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering
  Using a VLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonkyun Kim, Changin Choi, Wonseok Lee, Wonjong Rhee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stimulated by the sophisticated reasoning capabilities of recent Large
Language Models (LLMs), a variety of strategies for bridging video modality
have been devised. A prominent strategy involves Video Language Models
(VideoLMs), which train a learnable interface with video data to connect
advanced vision encoders with LLMs. Recently, an alternative strategy has
surfaced, employing readily available foundation models, such as VideoLMs and
LLMs, across multiple stages for modality bridging. In this study, we introduce
a simple yet novel strategy where only a single Vision Language Model (VLM) is
utilized. Our starting point is the plain insight that a video comprises a
series of images, or frames, interwoven with temporal information. The essence
of video comprehension lies in adeptly managing the temporal aspects along with
the spatial details of each frame. Initially, we transform a video into a
single composite image by arranging multiple frames in a grid layout. The
resulting single image is termed as an image grid. This format, while
maintaining the appearance of a solitary image, effectively retains temporal
information within the grid structure. Therefore, the image grid approach
enables direct application of a single high-performance VLM without
necessitating any video-data training. Our extensive experimental analysis
across ten zero-shot video question answering benchmarks, including five
open-ended and five multiple-choice benchmarks, reveals that the proposed Image
Grid Vision Language Model (IG-VLM) surpasses the existing methods in nine out
of ten benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code is available at https://github.com/imagegridworth/IG-VLM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Large Language Models for Relevance Judgments in Legal Case
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengjie Ma, Chong Chen, Qi Chu, Jiaxin Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collecting relevant judgments for legal case retrieval is a challenging and
time-consuming task. Accurately judging the relevance between two legal cases
requires a considerable effort to read the lengthy text and a high level of
domain expertise to extract Legal Facts and make juridical judgments. With the
advent of advanced large language models, some recent studies have suggested
that it is promising to use LLMs for relevance judgment. Nonetheless, the
method of employing a general large language model for reliable relevance
judgments in legal case retrieval is yet to be thoroughly explored. To fill
this research gap, we devise a novel few-shot workflow tailored to the relevant
judgment of legal cases. The proposed workflow breaks down the annotation
process into a series of stages, imitating the process employed by human
annotators and enabling a flexible integration of expert reasoning to enhance
the accuracy of relevance judgments. By comparing the relevance judgments of
LLMs and human experts, we empirically show that we can obtain reliable
relevance judgments with the proposed workflow. Furthermore, we demonstrate the
capacity to augment existing legal case retrieval models through the synthesis
of data generated by the large language model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Colour and Brush Stroke Pattern Recognition in Abstract Art using
  Modified Deep Convolutional Generative Adversarial Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Srinitish Srinivasan, Varenya Pathak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Abstract Art is an immensely popular, discussed form of art that often has
the ability to depict the emotions of an artist. Many researchers have made
attempts to study abstract art in the form of edge detection, brush stroke and
emotion recognition algorithms using machine and deep learning. This papers
describes the study of a wide distribution of abstract paintings using
Generative Adversarial Neural Networks(GAN). GANs have the ability to learn and
reproduce a distribution enabling researchers and scientists to effectively
explore and study the generated image space. However, the challenge lies in
developing an efficient GAN architecture that overcomes common training
pitfalls. This paper addresses this challenge by introducing a modified-DCGAN
(mDCGAN) specifically designed for high-quality artwork generation. The
approach involves a thorough exploration of the modifications made, delving
into the intricate workings of DCGANs, optimisation techniques, and
regularisation methods aimed at improving stability and realism in art
generation enabling effective study of generated patterns. The proposed mDCGAN
incorporates meticulous adjustments in layer configurations and architectural
choices, offering tailored solutions to the unique demands of art generation
while effectively combating issues like mode collapse and gradient vanishing.
Further this paper explores the generated latent space by performing random
walks to understand vector relationships between brush strokes and colours in
the abstract art space and a statistical analysis of unstable outputs after a
certain period of GAN training and compare its significant difference. These
findings validate the effectiveness of the proposed approach, emphasising its
potential to revolutionise the field of digital art generation and digital art
ecosystem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 5 tables, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FTBC: Forward Temporal Bias Correction for Optimizing ANN-SNN Conversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofeng Wu, Velibor Bojkovic, Bin Gu, Kun Suo, Kai Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking Neural Networks (SNNs) offer a promising avenue for energy-efficient
computing compared with Artificial Neural Networks (ANNs), closely mirroring
biological neural processes. However, this potential comes with inherent
challenges in directly training SNNs through spatio-temporal backpropagation --
stemming from the temporal dynamics of spiking neurons and their discrete
signal processing -- which necessitates alternative ways of training, most
notably through ANN-SNN conversion. In this work, we introduce a lightweight
Forward Temporal Bias Correction (FTBC) technique, aimed at enhancing
conversion accuracy without the computational overhead. We ground our method on
provided theoretical findings that through proper temporal bias calibration the
expected error of ANN-SNN conversion can be reduced to be zero after each time
step. We further propose a heuristic algorithm for finding the temporal bias
only in the forward pass, thus eliminating the computational burden of
backpropagation and we evaluate our method on CIFAR-10/100 and ImageNet
datasets, achieving a notable increase in accuracy on all datasets. Codes are
released at a GitHub repository.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Multi-modal Models are Good Class-Incremental Learners <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xusheng Cao, Haori Lu, Linlan Huang, Xialei Liu, Ming-Ming Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In class-incremental learning (CIL) scenarios, the phenomenon of catastrophic
forgetting caused by the classifier's bias towards the current task has long
posed a significant challenge. It is mainly caused by the characteristic of
discriminative models. With the growing popularity of the generative
multi-modal models, we would explore replacing discriminative models with
generative ones for CIL. However, transitioning from discriminative to
generative models requires addressing two key challenges. The primary challenge
lies in transferring the generated textual information into the classification
of distinct categories. Additionally, it requires formulating the task of CIL
within a generative framework. To this end, we propose a novel generative
multi-modal model (GMM) framework for class-incremental learning. Our approach
directly generates labels for images using an adapted generative model. After
obtaining the detailed text, we use a text encoder to extract text features and
employ feature matching to determine the most similar label as the
classification prediction. In the conventional CIL settings, we achieve
significantly better results in long-sequence task scenarios. Under the
Few-shot CIL setting, we have improved by at least 14\% accuracy over all the
current state-of-the-art methods with significantly less forgetting. Our code
is available at \url{https://github.com/DoubleClass/GMM}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Improving Attributed Text Generation of Large Language Models via
  Preference Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongfang Li, Zetian Sun, Baotian Hu, Zhenyu Liu, Xinshuo Hu, Xuebo Liu, <span class="highlight-author">Min Zhang</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have been widely adopted in natural language
processing, yet they face the challenge of generating unreliable content.
Recent works aim to reduce misinformation and hallucinations by resorting to
attribution as a means to provide evidence (i.e., citations). However, current
attribution methods usually focus on the retrieval stage and automatic
evaluation that neglect mirroring the citation mechanisms in human scholarly
writing to bolster credibility. In this paper, we address these challenges by
modelling the attribution task as preference learning and introducing an
Automatic Preference Optimization (APO) framework. First, we create a curated
collection for post-training with 6,330 examples by collecting and filtering
from existing datasets. Second, considering the high cost of labelling
preference data, we further propose an automatic method to synthesize
attribution preference data resulting in 95,263 pairs. Moreover, inspired by
the human citation process, we further propose a progressive preference
optimization method by leveraging fine-grained information. Extensive
experiments on three datasets (i.e., ASQA, StrategyQA, and ELI5) demonstrate
that APO achieves state-of-the-art citation F1 with higher answer quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 15 tables, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IIP-Mixer:Intra-Inter Patch Mixing Architecture for Battery Remaining
  Useful Life <span class="highlight-title">Predict</span>ion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangzai Ye, Li Feng, Jianlan Guo, Yuqiang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately estimating the Remaining Useful Life (RUL) of lithium-ion
batteries is crucial for maintaining the safe and stable operation of
rechargeable battery management systems. However, this task is often
challenging due to the complex temporal dynamics involved. Recently,
attention-based networks, such as Transformers and Informer, have been the
popular architecture in time series forecasting. Despite their effectiveness,
these models with abundant parameters necessitate substantial training time to
unravel temporal patterns. To tackle these challenges, we propose a simple
MLP-Mixer-based architecture named 'Intra-Inter Patch Mixer' (IIP-Mixer), which
is an architecture based exclusively on multi-layer perceptrons (MLPs),
extracting information by mixing operations along both intra-patch and
inter-patch dimensions for battery RUL prediction. The proposed IIP-Mixer
comprises parallel dual-head mixer layers: the intra-patch mixing MLP,
capturing local temporal patterns in the short-term period, and the inter-patch
mixing MLP, capturing global temporal patterns in the long-term period.
Notably, to address the varying importance of features in RUL prediction, we
introduce a weighted loss function in the MLP-Mixer-based architecture, marking
the first time such an approach has been employed. Our experiments demonstrate
that IIP-Mixer achieves competitive performance in battery RUL prediction,
outperforming other popular time-series frameworks
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intent-Aware DRL-Based Uplink Dynamic Scheduler for 5G-NR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salwa Mostafa, Mateus P. Mota, Alvaro Valcarce, Mehdi Bennis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the problem of supporting Industrial Internet of Things user
equipment (IIoT UEs) with intent (i.e., requested quality of service (QoS)) and
random traffic arrival. A deep reinforcement learning (DRL) based centralized
dynamic scheduler for time-frequency resources is proposed to learn how to
schedule the available communication resources among the IIoT UEs. The proposed
scheduler leverages an RL framework to adapt to the dynamic changes in the
wireless communication system and traffic arrivals. Moreover, a graph-based
reduction scheme is proposed to reduce the state and action space of the RL
framework to allow fast convergence and a better learning strategy. Simulation
results demonstrate the effectiveness of the proposed intelligent scheduler in
guaranteeing the expressed intent of IIoT UEs compared to several traditional
scheduling schemes, such as round-robin, semi-static, and heuristic approaches.
The proposed scheduler also outperforms the contention-free and
contention-based schemes in maximizing the number of successfully computed
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Diverse Agricultural Data for Vision-Based Farming
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikolaj Cieslak, Umabharathi Govindarajan, Alejandro Garcia, Anuradha Chandrashekar, Torsten Hädrich, Aleksander Mendoza-Drosik, Dominik L. Michels, Sören Pirk, Chia-Chun Fu, Wojciech Pałubicki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a specialized procedural model for generating synthetic
agricultural scenes, focusing on soybean crops, along with various weeds. This
model is capable of simulating distinct growth stages of these plants, diverse
soil conditions, and randomized field arrangements under varying lighting
conditions. The integration of real-world textures and environmental factors
into the procedural generation process enhances the photorealism and
applicability of the synthetic data. Our dataset includes 12,000 images with
semantic labels, offering a comprehensive resource for computer vision tasks in
precision agriculture, such as semantic segmentation for autonomous weed
control. We validate our model's effectiveness by comparing the synthetic data
against real agricultural images, demonstrating its potential to significantly
augment training data for machine learning models in agriculture. This approach
not only provides a cost-effective solution for generating high-quality,
diverse data but also addresses specific needs in agricultural vision tasks
that are not fully covered by general-purpose models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Quantum Fuzzy-based Approach for Real-Time Detection of Solar Coronal
  Holes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanmoy Bandyopadhyay, Suman Kundu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The detection and analysis of the solar coronal holes (CHs) is an important
field of study in the domain of solar physics. Mainly, it is required for the
proper prediction of the geomagnetic storms which directly or indirectly affect
various space and ground-based systems. For the detection of CHs till date, the
solar scientist depends on manual hand-drawn approaches. However, with the
advancement of image processing technologies, some automated image segmentation
methods have been used for the detection of CHs. In-spite of this, fast and
accurate detection of CHs are till a major issues. Here in this work, a novel
quantum computing-based fast fuzzy c-mean technique has been developed for fast
detection of the CHs region. The task has been carried out in two stages, in
first stage the solar image has been segmented using a quantum computing based
fast fuzzy c-mean (QCFFCM) and in the later stage the CHs has been extracted
out from the segmented image based on image morphological operation. In the
work, quantum computing has been used to optimize the cost function of the fast
fuzzy c-mean (FFCM) algorithm, where quantum approximate optimization algorithm
(QAOA) has been used to optimize the quadratic part of the cost function. The
proposed method has been tested for 193 \AA{} SDO/AIA full-disk solar image
datasets and has been compared with the existing techniques. The outcome shows
the comparable performance of the proposed method with the existing one within
a very lesser time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LC-LLM: Explainable Lane-Change Intention and Trajectory <span class="highlight-title">Predict</span>ions
  with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingxing Peng, Xusen Guo, Xianda Chen, Meixin Zhu, Kehua Chen,  Hao,  Yang, Xuesong Wang, Yinhai Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To ensure safe driving in dynamic environments, autonomous vehicles should
possess the capability to accurately predict the lane change intentions of
surrounding vehicles in advance and forecast their future trajectories.
Existing motion prediction approaches have ample room for improvement,
particularly in terms of long-term prediction accuracy and interpretability. In
this paper, we address these challenges by proposing LC-LLM, an explainable
lane change prediction model that leverages the strong reasoning capabilities
and self-explanation abilities of Large Language Models (LLMs). Essentially, we
reformulate the lane change prediction task as a language modeling problem,
processing heterogeneous driving scenario information in natural language as
prompts for input into the LLM and employing a supervised fine-tuning technique
to tailor the LLM specifically for our lane change prediction task. This allows
us to utilize the LLM's powerful common sense reasoning abilities to understand
complex interactive information, thereby improving the accuracy of long-term
predictions. Furthermore, we incorporate explanatory requirements into the
prompts in the inference stage. Therefore, our LC-LLM model not only can
predict lane change intentions and trajectories but also provides explanations
for its predictions, enhancing the interpretability. Extensive experiments on
the large-scale highD dataset demonstrate the superior performance and
interpretability of our LC-LLM in lane change prediction task. To the best of
our knowledge, this is the first attempt to utilize LLMs for predicting lane
change behavior. Our study shows that LLMs can encode comprehensive interaction
information for driving behavior understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ mALBERT: Is a Compact Multilingual BERT Model Still Worth It? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christophe Servan, Sahar Ghannay, Sophie Rosset
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Within the current trend of Pretained Language Models (PLM), emerge more and
more criticisms about the ethical andecological impact of such models. In this
article, considering these critical remarks, we propose to focus on
smallermodels, such as compact models like ALBERT, which are more ecologically
virtuous than these PLM. However,PLMs enable huge breakthroughs in Natural
Language Processing tasks, such as Spoken and Natural LanguageUnderstanding,
classification, Question--Answering tasks. PLMs also have the advantage of
being multilingual, and,as far as we know, a multilingual version of compact
ALBERT models does not exist. Considering these facts, wepropose the free
release of the first version of a multilingual compact ALBERT model,
pre-trained using Wikipediadata, which complies with the ethical aspect of such
a language model. We also evaluate the model against classicalmultilingual PLMs
in classical NLP tasks. Finally, this paper proposes a rare study on the
subword tokenizationimpact on language performances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 2024 Joint International Conference on Computational Linguistics,
  Language Resources and Evaluation, May 2024, Torino, Italy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can LLMs Converse Formally? Automatically Assessing LLMs in Translating
  and Interpreting Formal Specifications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rushang Karia, Daksh Dobhal, Daniel Bramblett, Pulkit Verma, Siddharth Srivastava
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stakeholders often describe system requirements using natural language which
are then converted to formal syntax by a domain-expert leading to increased
design costs. This paper assesses the capabilities of Large Language Models
(LLMs) in converting between natural language descriptions and formal
specifications. Existing work has evaluated the capabilities of LLMs in
generating formal syntax such as source code but such experiments are typically
hand-crafted and use problems that are likely to be in the training set of
LLMs, and often require human-annotated datasets. We propose an approach that
can use two copies of an LLM in conjunction with an off-the-shelf verifier to
automatically evaluate its translation abilities without any additional human
input. Our approach generates formal syntax using language grammars to
automatically generate a dataset. We conduct an empirical evaluation to measure
the accuracy of this translation task and show that SOTA LLMs cannot adequately
solve this task, limiting their current utility in the design of complex
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chinese Offensive Language Detection:Current Status and Future
  Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunze Xiao, Houda Bouamor, Wajdi Zaghouani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the considerable efforts being made to monitor and regulate
user-generated content on social media platforms, the pervasiveness of
offensive language, such as hate speech or cyberbullying, in the digital space
remains a significant challenge. Given the importance of maintaining a
civilized and respectful online environment, there is an urgent and growing
need for automatic systems capable of detecting offensive speech in real time.
However, developing effective systems for processing languages such as Chinese
presents a significant challenge, owing to the language's complex and nuanced
nature, which makes it difficult to process automatically. This paper provides
a comprehensive overview of offensive language detection in Chinese, examining
current benchmarks and approaches and highlighting specific models and tools
for addressing the unique challenges of detecting offensive language in this
complex language. The primary objective of this survey is to explore the
existing techniques and identify potential avenues for further research that
can address the cultural and linguistic complexities of Chinese.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A thermodynamically consistent physics-informed deep learning material
  model for short fiber/polymer nanocomposites 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Betim Bahtiri, Behrouz Arash, Sven Scheffler, Maximilian Jux, Raimund Rolfes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work proposes a physics-informed deep learning (PIDL)-based constitutive
model for investigating the viscoelastic-viscoplastic behavior of short
fiber-reinforced nanoparticle-filled epoxies under various ambient conditions.
The deep-learning model is trained to enforce thermodynamic principles, leading
to a thermodynamically consistent constitutive model. To accomplish this, a
long short-term memory network is combined with a feed-forward neural network
to predict internal variables required for characterizing the internal
dissipation of the nanocomposite materials. In addition, another feed-forward
neural network is used to indicate the free-energy function, which enables
defining the thermodynamic state of the entire system. The PIDL model is
initially developed for the three-dimensional case by generating synthetic data
from a classical constitutive model. The model is then trained by extracting
the data directly from cyclic loading-unloading experimental tests. Numerical
examples show that the PIDL model can accurately predict the mechanical
behavior of epoxy-based nanocomposites for different volume fractions of fibers
and nanoparticles under various hygrothermal conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2305.08102</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Recommender System for NFT Collectibles with Item Feature <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minjoo Choi, Seonmi Kim, Yejin Kim, Youngbin Lee, Joohwan Hong, Yongjae Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems have been actively studied and applied in various domains
to deal with information overload. Although there are numerous studies on
recommender systems for movies, music, and e-commerce, comparatively less
attention has been paid to the recommender system for NFTs despite the
continuous growth of the NFT market. This paper presents a recommender system
for NFTs that utilizes a variety of data sources, from NFT transaction records
to external item features, to generate precise recommendations that cater to
individual preferences. We develop a data-efficient graph-based recommender
system to efficiently capture the complex relationship between each item and
users and generate node(item) embeddings which incorporate both node feature
information and graph structure. Furthermore, we exploit inputs beyond
user-item interactions, such as image feature, text feature, and price feature.
Numerical experiments verify the performance of the graph-based recommender
system improves significantly after utilizing all types of item features as
side information, thereby outperforming all other baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the AAAI 2023 Bridge on AI for Financial Services
  (https://sites.google.com/view/aaai-ai-fin/home)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shrinivas Ramasubramanian, Harsh Rangwani, Sho Takemori, Kunal Samanta, Yuhei Umeda, Venkatesh Babu Radhakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise in internet usage has led to the generation of massive amounts of
data, resulting in the adoption of various supervised and semi-supervised
machine learning algorithms, which can effectively utilize the colossal amount
of data to train models. However, before deploying these models in the real
world, these must be strictly evaluated on performance measures like worst-case
recall and satisfy constraints such as fairness. We find that current
state-of-the-art empirical techniques offer sub-optimal performance on these
practical, non-decomposable performance objectives. On the other hand, the
theoretical techniques necessitate training a new model from scratch for each
performance objective. To bridge the gap, we propose SelMix, a selective
mixup-based inexpensive fine-tuning technique for pre-trained models, to
optimize for the desired objective. The core idea of our framework is to
determine a sampling distribution to perform a mixup of features between
samples from particular classes such that it optimizes the given objective. We
comprehensively evaluate our technique against the existing empirical and
theoretically principled methods on standard benchmark datasets for imbalanced
classification. We find that proposed SelMix fine-tuning significantly improves
the performance for various practical non-decomposable objectives across
benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 SpotLight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GeNet: A <span class="highlight-title">Graph Neural Network</span>-based Anti-noise Task-Oriented Semantic
  <span class="highlight-title">Communication</span> Paradigm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunhang Zheng, Kechao Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional approaches to semantic communication tasks rely on the knowledge
of the signal-to-noise ratio (SNR) to mitigate channel noise. However, these
methods necessitate training under specific SNR conditions, entailing
considerable time and computational resources. In this paper, we propose GeNet,
a Graph Neural Network (GNN)-based paradigm for semantic communication aimed at
combating noise, thereby facilitating Task-Oriented Communication (TOC). We
propose a novel approach where we first transform the input data image into
graph structures. Then we leverage a GNN-based encoder to extract semantic
information from the source data. This extracted semantic information is then
transmitted through the channel. At the receiver's end, a GNN-based decoder is
utilized to reconstruct the relevant semantic information from the source data
for TOC. Through experimental evaluation, we show GeNet's effectiveness in
anti-noise TOC while decoupling the SNR dependency. We further evaluate GeNet's
performance by varying the number of nodes, revealing its versatility as a new
paradigm for semantic communication. Additionally, we show GeNet's robustness
to geometric transformations by testing it with different rotation angles,
without resorting to data augmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-Shot Recalibration of Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Lisa Li, Urvashi Khandelwal, Kelvin Guu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has uncovered promising ways to extract well-calibrated
confidence estimates from language models (LMs), where the model's confidence
score reflects how likely it is to be correct. However, while LMs may appear
well-calibrated over broad distributions, this often hides significant
miscalibration within narrower slices (e.g., systemic over-confidence in math
can balance out systemic under-confidence in history, yielding perfect
calibration in aggregate). To attain well-calibrated confidence estimates for
any slice of a distribution, we propose a new framework for few-shot
slice-specific recalibration. Specifically, we train a recalibration model that
takes in a few unlabeled examples from any given slice and predicts a curve
that remaps confidence scores to be more accurate for that slice. Our trained
model can recalibrate for arbitrary new slices, without using any labeled data
from that slice. This enables us to identify domain-specific confidence
thresholds above which the LM's predictions can be trusted, and below which it
should abstain. Experiments show that our few-shot recalibrator consistently
outperforms existing calibration methods, for instance improving calibration
error for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identification and Uses of Deep Learning Backbones via Pattern Mining <span class="chip">SDM24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Livanos, Ian Davidson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning is extensively used in many areas of data mining as a black-box
method with impressive results. However, understanding the core mechanism of
how deep learning makes predictions is a relatively understudied problem. Here
we explore the notion of identifying a backbone of deep learning for a given
group of instances. A group here can be instances of the same class or even
misclassified instances of the same class. We view each instance for a given
group as activating a subset of neurons and attempt to find a subgraph of
neurons associated with a given concept/group. We formulate this problem as a
set cover style problem and show it is intractable and presents a highly
constrained integer linear programming (ILP) formulation. As an alternative, we
explore a coverage-based heuristic approach related to pattern mining, and show
it converges to a Pareto equilibrium point of the ILP formulation.
Experimentally we explore these backbones to identify mistakes and improve
performance, explanation, and visualization. We demonstrate application-based
results using several challenging data sets, including Bird Audio Detection
(BAD) Challenge and Labeled Faces in the Wild (LFW), as well as the classic
MNIST data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures, published SIAM SDM24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DSF-GAN: Down<span class="highlight-title">Stream</span> Feedback Generative Adversarial Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oriel Perets, Nadav Rappoport
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Utility and privacy are two crucial measurements of the quality of synthetic
tabular data. While significant advancements have been made in privacy
measures, generating synthetic samples with high utility remains challenging.
To enhance the utility of synthetic samples, we propose a novel architecture
called the DownStream Feedback Generative Adversarial Network (DSF-GAN). This
approach incorporates feedback from a downstream prediction model during
training to augment the generator's loss function with valuable information.
Thus, DSF-GAN utilizes a downstream prediction task to enhance the utility of
synthetic samples. To evaluate our method, we tested it using two popular
datasets. Our experiments demonstrate improved model performance when training
on synthetic samples generated by DSF-GAN, compared to those generated by the
same GAN architecture without feedback. The evaluation was conducted on the
same validation set comprising real samples. All code and datasets used in this
research will be made openly available for ease of reproduction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Generative Class Incremental Learning Performance with Model
  Forgetting Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taro Togo, Ren Togo, Keisuke Maeda, Takahiro Ogawa, Miki Haseyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents a novel approach to Generative Class Incremental Learning
(GCIL) by introducing the forgetting mechanism, aimed at dynamically managing
class information for better adaptation to streaming data. GCIL is one of the
hot topics in the field of computer vision, and this is considered one of the
crucial tasks in society, specifically the continual learning of generative
models. The ability to forget is a crucial brain function that facilitates
continual learning by selectively discarding less relevant information for
humans. However, in the field of machine learning models, the concept of
intentionally forgetting has not been extensively investigated. In this study
we aim to bridge this gap by incorporating the forgetting mechanisms into GCIL,
thereby examining their impact on the models' ability to learn in continual
learning. Through our experiments, we have found that integrating the
forgetting mechanisms significantly enhances the models' performance in
acquiring new knowledge, underscoring the positive role that strategic
forgetting plays in the process of continual learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Manipulating Neural Path Planners via Slight Perturbations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zikang Xiong, Suresh Jagannathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-driven neural path planners are attracting increasing interest in the
robotics community. However, their neural network components typically come as
black boxes, obscuring their underlying decision-making processes. Their
black-box nature exposes them to the risk of being compromised via the
insertion of hidden malicious behaviors. For example, an attacker may hide
behaviors that, when triggered, hijack a delivery robot by guiding it to a
specific (albeit wrong) destination, trapping it in a predefined region, or
inducing unnecessary energy expenditure by causing the robot to repeatedly
circle a region. In this paper, we propose a novel approach to specify and
inject a range of hidden malicious behaviors, known as backdoors, into neural
path planners. Our approach provides a concise but flexible way to define these
behaviors, and we show that hidden behaviors can be triggered by slight
perturbations (e.g., inserting a tiny unnoticeable object), that can
nonetheless significantly compromise their integrity. We also discuss potential
techniques to identify these backdoors aimed at alleviating such risks. We
demonstrate our approach on both sampling-based and search-based neural path
planners.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond <span class="highlight-title">Embedding</span>s: The Promise of Visual Table in Multi-Modal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwu Zhong, Zi-Yuan Hu, Michael R. Lyu, Liwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual representation learning has been a cornerstone in computer vision,
evolving from supervised learning with human-annotated labels to aligning
image-text pairs from the Internet. Despite recent advancements in multi-modal
large language models (MLLMs), the visual representations they rely on, such as
CLIP embeddings, often lack access to external world knowledge critical for
real-world visual reasoning. In this work, we propose Visual Table, a novel
visual representation tailored for MLLMs. It provides hierarchical text
descriptions of holistic visual scenes, consisting of a scene description and
multiple object-centric descriptions that encompass categories, attributes, and
knowledge at instance level. We further develop a scalable generator for visual
table generation and train it on small-scale annotations from GPT4V. Extensive
evaluations demonstrate that, with generated visual tables as additional visual
representations, our model can consistently outperform the state-of-the-art
(SOTA) MLLMs across diverse benchmarks. When visual tables serve as standalone
visual representations, our model can closely match or even beat the SOTA MLLMs
that are built on CLIP visual embeddings. Our code is available at
https://github.com/LaVi-Lab/Visual-Table.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/LaVi-Lab/Visual-Table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Conversational Question Answering with Fine-Grained
  Retrieval-Augmentation and Self-Check 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linhao Ye, Zhikai Lei, Jianghao Yin, Qin Chen, Jie Zhou, Liang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) aims to generate more reliable and
accurate responses, by augmenting large language models (LLMs) with the
external vast and dynamic knowledge. Most previous work focuses on using RAG
for single-round question answering, while how to adapt RAG to the complex
conversational setting wherein the question is interdependent on the preceding
context is not well studied. In this paper, we propose a conversation-level RAG
approach, which incorporates fine-grained retrieval augmentation and self-check
for conversational question answering (CQA). In particular, our approach
consists of three components, namely conversational question refiner,
fine-grained retriever and self-check based response generator, which work
collaboratively for question understanding and relevant information acquisition
in conversational settings. Extensive experiments demonstrate the great
advantages of our approach over the state-of-the-art baselines. Moreover, we
also release a Chinese CQA dataset with new features including reformulated
question, extracted keyword, retrieved paragraphs and their helpfulness, which
facilitates further researches in RAG enhanced CQA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion,
  Reconstruction, and Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruikai Cui, Weizhe Liu, Weixuan Sun, Senbo Wang, Taizhang Shang, Yang Li, Xibin Song, Han Yan, Zhennan Wu, Shenzhou Chen, Hongdong Li, Pan Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D shape generation aims to produce innovative 3D content adhering to
specific conditions and constraints. Existing methods often decompose 3D shapes
into a sequence of localized components, treating each element in isolation
without considering spatial consistency. As a result, these approaches exhibit
limited versatility in 3D data representation and shape generation, hindering
their ability to generate highly diverse 3D shapes that comply with the
specified constraints. In this paper, we introduce a novel spatial-aware 3D
shape generation framework that leverages 2D plane representations for enhanced
3D shape modeling. To ensure spatial coherence and reduce memory usage, we
incorporate a hybrid shape representation technique that directly learns a
continuous signed distance field representation of the 3D shape using
orthogonal 2D planes. Additionally, we meticulously enforce spatial
correspondences across distinct planes using a transformer-based autoencoder
structure, promoting the preservation of spatial relationships in the generated
3D shapes. This yields an algorithm that consistently outperforms
state-of-the-art 3D shape generation methods on various tasks, including
unconditional shape generation, multi-modal shape completion, single-view
reconstruction, and text-to-shape synthesis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models Need Consultants for Reasoning: Becoming an Expert
  in a Complex Human System Through Behavior Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18230v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18230v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuwen Wang, Shirong Zeng, Cheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs), in conjunction with various reasoning
reinforcement methodologies, have demonstrated remarkable capabilities
comparable to humans in fields such as mathematics, law, coding, common sense,
and world knowledge. In this paper, we delve into the reasoning abilities of
LLMs within complex human systems. We propose a novel reasoning framework,
termed ``Mosaic Expert Observation Wall'' (MEOW) exploiting
generative-agents-based simulation technique. In the MEOW framework, simulated
data are utilized to train an expert model concentrating ``experience'' about a
specific task in each independent time of simulation. It is the accumulated
``experience'' through the simulation that makes for an expert on a task in a
complex human system. We conduct the experiments within a communication game
that mirrors real-world security scenarios. The results indicate that our
proposed methodology can cooperate with existing methodologies to enhance the
reasoning abilities of LLMs in complex human systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Transformer-Based Framework for Payload Malware Detection and
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyle Stein, Arash Mahyari, Guillermo Francia III, Eman El-Sheikh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As malicious cyber threats become more sophisticated in breaching computer
networks, the need for effective intrusion detection systems (IDSs) becomes
crucial. Techniques such as Deep Packet Inspection (DPI) have been introduced
to allow IDSs analyze the content of network packets, providing more context
for identifying potential threats. IDSs traditionally rely on using
anomaly-based and signature-based detection techniques to detect unrecognized
and suspicious activity. Deep learning techniques have shown great potential in
DPI for IDSs due to their efficiency in learning intricate patterns from the
packet content being transmitted through the network. In this paper, we propose
a revolutionary DPI algorithm based on transformers adapted for the purpose of
detecting malicious traffic with a classifier head. Transformers learn the
complex content of sequence data and generalize them well to similar scenarios
thanks to their self-attention mechanism. Our proposed method uses the raw
payload bytes that represent the packet contents and is deployed as
man-in-the-middle. The payload bytes are used to detect malicious packets and
classify their types. Experimental results on the UNSW-NB15 and CIC-IOT23
datasets demonstrate that our transformer-based model is effective in
distinguishing malicious from benign traffic in the test dataset, attaining an
average accuracy of 79\% using binary classification and 72\% on the
multi-classification experiment, both using solely payload bytes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Two-Dimensional to Three-Dimensional Environment with Q-Learning:
  Modeling Autonomous Navigation with Reinforcement Learning and no Libraries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ergon Cugler de Moraes Silva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) algorithms have become indispensable tools in
artificial intelligence, empowering agents to acquire optimal decision-making
policies through interactions with their environment and feedback mechanisms.
This study explores the performance of RL agents in both two-dimensional (2D)
and three-dimensional (3D) environments, aiming to research the dynamics of
learning across different spatial dimensions. A key aspect of this
investigation is the absence of pre-made libraries for learning, with the
algorithm developed exclusively through computational mathematics. The
methodological framework centers on RL principles, employing a Q-learning agent
class and distinct environment classes tailored to each spatial dimension. The
research aims to address the question: How do reinforcement learning agents
adapt and perform in environments of varying spatial dimensions, particularly
in 2D and 3D settings? Through empirical analysis, the study evaluates agents'
learning trajectories and adaptation processes, revealing insights into the
efficacy of RL algorithms in navigating complex, multi-dimensional spaces.
Reflections on the findings prompt considerations for future research,
particularly in understanding the dynamics of learning in higher-dimensional
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Large Language Models for Fuzzy String Matching in Political
  Science 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18218v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18218v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fuzzy string matching remains a key issue when political scientists combine
data from different sources. Existing matching methods invariably rely on
string distances, such as Levenshtein distance and cosine similarity. As such,
they are inherently incapable of matching strings that refer to the same entity
with different names such as ''JP Morgan'' and ''Chase Bank'', ''DPRK'' and
''North Korea'', ''Chuck Fleischmann (R)'' and ''Charles Fleischmann (R)''. In
this letter, we propose to use large language models to entirely sidestep this
problem in an easy and intuitive manner. Extensive experiments show that our
proposed methods can improve the state of the art by as much as 39% in terms of
average precision while being substantially easier and more intuitive to use by
political scientists. Moreover, our results are robust against various
temperatures. We further note that enhanced prompting can lead to additional
performance improvements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures, 1 table;</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preference-Based Planning in Stochastic Environments: From
  Partially-Ordered Temporal Goals to Most Preferred Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hazhar Rahmani, Abhishek N. Kulkarni, Jie Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human preferences are not always represented via complete linear orders: It
is natural to employ partially-ordered preferences for expressing incomparable
outcomes. In this work, we consider decision-making and probabilistic planning
in stochastic systems modeled as Markov decision processes (MDPs), given a
partially ordered preference over a set of temporally extended goals.
Specifically, each temporally extended goal is expressed using a formula in
Linear Temporal Logic on Finite Traces (LTL$_f$). To plan with the partially
ordered preference, we introduce order theory to map a preference over temporal
goals to a preference over policies for the MDP. Accordingly, a most preferred
policy under a stochastic ordering induces a stochastic nondominated
probability distribution over the finite paths in the MDP. To synthesize a most
preferred policy, our technical approach includes two key steps. In the first
step, we develop a procedure to transform a partially ordered preference over
temporal goals into a computational model, called preference automaton, which
is a semi-automaton with a partial order over acceptance conditions. In the
second step, we prove that finding a most preferred policy is equivalent to
computing a Pareto-optimal policy in a multi-objective MDP that is constructed
from the original MDP, the preference automaton, and the chosen stochastic
ordering relation. Throughout the paper, we employ running examples to
illustrate the proposed preference specification and solution approaches. We
demonstrate the efficacy of our algorithm using these examples, providing
detailed analysis, and then discuss several potential future directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2209.12267</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long and Short-Term Constraints Driven Safe Reinforcement Learning for
  Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuemin Hu, Pan Chen, Yijun Wen, Bo Tang, Long Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) has been widely used in decision-making tasks,
but it cannot guarantee the agent's safety in the training process due to the
requirements of interaction with the environment, which seriously limits its
industrial applications such as autonomous driving. Safe RL methods are
developed to handle this issue by constraining the expected safety violation
costs as a training objective, but they still permit unsafe state occurrence,
which is unacceptable in autonomous driving tasks. Moreover, these methods are
difficult to achieve a balance between the cost and return expectations, which
leads to learning performance degradation for the algorithms. In this paper, we
propose a novel algorithm based on the long and short-term constraints (LSTC)
for safe RL. The short-term constraint aims to guarantee the short-term state
safety that the vehicle explores, while the long-term constraint ensures the
overall safety of the vehicle throughout the decision-making process. In
addition, we develop a safe RL method with dual-constraint optimization based
on the Lagrange multiplier to optimize the training process for end-to-end
autonomous driving. Comprehensive experiments were conducted on the MetaDrive
simulator. Experimental results demonstrate that the proposed method achieves
higher safety in continuous state and action tasks, and exhibits higher
exploration performance in long-distance decision-making tasks compared with
state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Evolutionary Network Architecture Search Framework with Adaptive
  Multimodal Fusion for Hand Gesture Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhang Xia, Shihao Song, Zhanglu Hou, Junwen Xu, Juan Zou, Yuan Liu, Shengxiang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hand gesture recognition (HGR) based on multimodal data has attracted
considerable attention owing to its great potential in applications. Various
manually designed multimodal deep networks have performed well in multimodal
HGR (MHGR), but most of existing algorithms require a lot of expert experience
and time-consuming manual trials. To address these issues, we propose an
evolutionary network architecture search framework with the adaptive multimodel
fusion (AMF-ENAS). Specifically, we design an encoding space that
simultaneously considers fusion positions and ratios of the multimodal data,
allowing for the automatic construction of multimodal networks with different
architectures through decoding. Additionally, we consider three input streams
corresponding to intra-modal surface electromyography (sEMG), intra-modal
accelerometer (ACC), and inter-modal sEMG-ACC. To automatically adapt to
various datasets, the ENAS framework is designed to automatically search a MHGR
network with appropriate fusion positions and ratios. To the best of our
knowledge, this is the first time that ENAS has been utilized in MHGR to tackle
issues related to the fusion position and ratio of multimodal data.
Experimental results demonstrate that AMF-ENAS achieves state-of-the-art
performance on the Ninapro DB2, DB3, and DB7 datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Privacy Protection Capabilities of Chinese Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18205v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18205v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqi Yang, Xiaowen Huang, Jitao Sang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs), renowned for their impressive capabilities in
various tasks, have significantly advanced artificial intelligence. Yet, these
advancements have raised growing concerns about privacy and security
implications. To address these issues and explain the risks inherent in these
models, we have devised a three-tiered progressive framework tailored for
evaluating privacy in language systems. This framework consists of
progressively complex and in-depth privacy test tasks at each tier. Our primary
objective is to comprehensively evaluate the sensitivity of large language
models to private information, examining how effectively they discern, manage,
and safeguard sensitive data in diverse scenarios. This systematic evaluation
helps us understand the degree to which these models comply with privacy
protection guidelines and the effectiveness of their inherent safeguards
against privacy breaches. Our observations indicate that existing Chinese large
language models universally show privacy protection shortcomings. It seems that
at the moment this widespread issue is unavoidable and may pose corresponding
privacy risks in applications based on these models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EndToEndML: An Open-Source End-to-End Pipeline for Machine Learning
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nisha Pillai, Athish Ram Das, Moses Ayoola, Ganga Gireesan, Bindu Nanduri, Mahalingam Ramkumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI) techniques are widely applied in the life
sciences. However, applying innovative AI techniques to understand and
deconvolute biological complexity is hindered by the learning curve for life
science scientists to understand and use computing languages. An open-source,
user-friendly interface for AI models, that does not require programming skills
to analyze complex biological data will be extremely valuable to the
bioinformatics community. With easy access to different sequencing technologies
and increased interest in different 'omics' studies, the number of biological
datasets being generated has increased and analyzing these high-throughput
datasets is computationally demanding. The majority of AI libraries today
require advanced programming skills as well as machine learning, data
preprocessing, and visualization skills. In this research, we propose a
web-based end-to-end pipeline that is capable of preprocessing, training,
evaluating, and visualizing machine learning (ML) models without manual
intervention or coding expertise. By integrating traditional machine learning
and deep neural network models with visualizations, our library assists in
recognizing, classifying, clustering, and predicting a wide range of
multi-modal, multi-sensor datasets, including images, languages, and
one-dimensional numerical data, for drug discovery, pathogen classification,
and medical diagnostics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2024 7th International Conference on Information and Computer
  Technologies (ICICT)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Looking Beyond What You See: An Empirical Analysis on Subgroup
  Intersectional Fairness for Multi-label Chest X-ray Classification Using
  Social Determinants of Racial Health Inequities <span class="chip">ICCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18196v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18196v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dana Moukheiber, Saurabh Mahindre, Lama Moukheiber, Mira Moukheiber, Mingchen Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been significant progress in implementing deep learning models in
disease diagnosis using chest X- rays. Despite these advancements, inherent
biases in these models can lead to disparities in prediction accuracy across
protected groups. In this study, we propose a framework to achieve accurate
diagnostic outcomes and ensure fairness across intersectional groups in
high-dimensional chest X- ray multi-label classification. Transcending
traditional protected attributes, we consider complex interactions within
social determinants, enabling a more granular benchmark and evaluation of
fairness. We present a simple and robust method that involves retraining the
last classification layer of pre-trained models using a balanced dataset across
groups. Additionally, we account for fairness constraints and integrate
class-balanced fine-tuning for multi-label settings. The evaluation of our
method on the MIMIC-CXR dataset demonstrates that our framework achieves an
optimal tradeoff between accuracy and fairness compared to baseline methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV CVAMD 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SCANet: Correcting LEGO Assembly Errors with Self-Correct Assembly
  Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Wan, Kaichen Zhou, jinhong Chen, Hao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous assembly in robotics and 3D vision presents significant
challenges, particularly in ensuring assembly correctness. Presently,
predominant methods such as MEPNet focus on assembling components based on
manually provided images. However, these approaches often fall short in
achieving satisfactory results for tasks requiring long-term planning.
Concurrently, we observe that integrating a self-correction module can
partially alleviate such issues. Motivated by this concern, we introduce the
single-step assembly error correction task, which involves identifying and
rectifying misassembled components. To support research in this area, we
present the LEGO Error Correction Assembly Dataset (LEGO-ECA), comprising
manual images for assembly steps and instances of assembly failures.
Additionally, we propose the Self-Correct Assembly Network (SCANet), a novel
method to address this task. SCANet treats assembled components as queries,
determining their correctness in manual images and providing corrections when
necessary. Finally, we utilize SCANet to correct the assembly results of
MEPNet. Experimental results demonstrate that SCANet can identify and correct
MEPNet's misassembled results, significantly improving the correctness of
assembly. Our code and dataset are available at
https://github.com/Yaser-wyx/SCANet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can AI Models Appreciate Document Aesthetics? An Exploration of
  Legibility and Layout Quality in Relation to <span class="highlight-title">Predict</span>ion Confidence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hsiu-Wei Yang, Abhinav Agrawal, Pavlos Fragkogiannis, Shubham Nitin Mulay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A well-designed document communicates not only through its words but also
through its visual eloquence. Authors utilize aesthetic elements such as
colors, fonts, graphics, and layouts to shape the perception of information.
Thoughtful document design, informed by psychological insights, enhances both
the visual appeal and the comprehension of the content. While state-of-the-art
document AI models demonstrate the benefits of incorporating layout and image
data, it remains unclear whether the nuances of document aesthetics are
effectively captured. To bridge the gap between human cognition and AI
interpretation of aesthetic elements, we formulated hypotheses concerning AI
behavior in document understanding tasks, specifically anchored in document
design principles. With a focus on legibility and layout quality, we tested
four aspects of aesthetic effects: noise, font-size contrast, alignment, and
complexity, on model confidence using correlational analysis. The results and
observations highlight the value of model analysis rooted in document design
theories. Our work serves as a trailhead for further studies and we advocate
for continued research in this topic to deepen our understanding of how AI
interprets document aesthetics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mechanisms of non-factual hallucinations in language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Yu, Meng Cao, Jackie Chi Kit Cheung, Yue Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art language models (LMs) sometimes generate non-factual
hallucinations that misalign with world knowledge. Despite extensive efforts to
detect and mitigate hallucinations, understanding their internal mechanisms
remains elusive. Our study investigates the mechanistic causes of
hallucination, specifically non-factual ones where the LM incorrectly predicts
object attributes in response to subject-relation queries. With causal
mediation analysis and embedding space projection, we identify two general
mechanistic causes of hallucinations shared across LMs of various scales and
designs: 1) insufficient subject attribute knowledge in lower layer MLPs, and
2) failing to select the correct object attribute in upper layer attention
heads and MLPs. These two mechanisms exhibit varying degrees of subject-object
association, predictive uncertainty and perturbation robustness. Additionally,
we scrutinize LM pre-training checkpoints, revealing distinct learning dynamics
for the two mechanistic causes of hallucinations. We also highlight how
attribution features from our causal analysis can effectively construct
hallucination detectors. Our work proposes a mechanistic understanding of LM
factual errors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A 4D Hybrid Algorithm to Scale Parallel Training to Thousands of GPUs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.13525v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.13525v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Singh, Prajwal Singhania, Aditya K. Ranjan, Zack Sating, Abhinav Bhatele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large communication costs are a critical bottleneck in training
state-of-the-art neural networks on distributed systems. This paper introduces
AxoNN, a novel four-dimensional (4D) parallelization approach, inspired by
Agarwal's algorithm for matrix multiplication, for parallelizing tensor
computations in deep learning, AxoNN employs two key strategies to minimize
communication overhead. First, we optimize communication by overlapping
expensive collective operations (reduce-scatter, all-gather, all-reduce) with
computations. Our experiments with a 20-billion parameter transformer model
demonstrate that these optimizations deliver nearly 53\% improvement. Second,
we present an analytical model to assist users in identifying
communication-minimizing configurations within the vast search space defined by
our 4D algorithm. This model empowers practitioners by simplifying the tuning
process for their specific training workloads. When training an 80-billion
parameter model on 1024 GPUs of Perlmutter, AxoNN surpasses Megatron-LM, a
state-of-the-art framework, by a significant 26%. Additionally, it achieves 57%
of the theoretical peak FLOP/s.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shifting to Machine Supervision: Annotation-Efficient Semi and
  Self-Supervised Learning for Automatic Medical Image Segmentation and
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10319v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10319v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranav Singh, Raviteja Chukkapalli, Shravan Chaudhari, Luoyao Chen, Mei Chen, Jinqian Pan, Craig Smuda, Jacopo Cirrone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in clinical treatment are increasingly constrained by the
limitations of supervised learning techniques, which depend heavily on large
volumes of annotated data. The annotation process is not only costly but also
demands substantial time from clinical specialists. Addressing this issue, we
introduce the S4MI (Self-Supervision and Semi-Supervision for Medical Imaging)
pipeline, a novel approach that leverages advancements in self-supervised and
semi-supervised learning. These techniques engage in auxiliary tasks that do
not require labeling, thus simplifying the scaling of machine supervision
compared to fully-supervised methods. Our study benchmarks these techniques on
three distinct medical imaging datasets to evaluate their effectiveness in
classification and segmentation tasks. Notably, we observed that self
supervised learning significantly surpassed the performance of supervised
methods in the classification of all evaluated datasets. Remarkably, the
semi-supervised approach demonstrated superior outcomes in segmentation,
outperforming fully-supervised methods while using 50% fewer labels across all
datasets. In line with our commitment to contributing to the scientific
community, we have made the S4MI code openly accessible, allowing for broader
application and further development of these methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Seventeen pages (incl. references), five figures, and one table.
  (Under Review)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Agent-Pro: Learning to Evolve via Policy-Level Reflection and
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17574v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17574v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang, Yongliang Shen, Guiyang Hou, Zeqi Tan, Peng Li, Yueting Zhuang, Weiming Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models exhibit robust problem-solving capabilities for diverse
tasks. However, most LLM-based agents are designed as specific task solvers
with sophisticated prompt engineering, rather than agents capable of learning
and evolving through interactions. These task solvers necessitate manually
crafted prompts to inform task rules and regulate LLM behaviors, inherently
incapacitating to address complex dynamic scenarios e.g., large interactive
games. In light of this, we propose Agent-Pro: an LLM-based Agent with
Policy-level Reflection and Optimization that can learn a wealth of expertise
from interactive experiences and progressively elevate its behavioral policy.
Specifically, it involves a dynamic belief generation and reflection process
for policy evolution. Rather than action-level reflection, Agent-Pro
iteratively reflects on past trajectories and beliefs, fine-tuning its
irrational beliefs for a better policy. Moreover, a depth-first search is
employed for policy optimization, ensuring continual enhancement in policy
payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em,
outperforming vanilla LLM and specialized models. Our results show Agent-Pro
can learn and evolve in complex and dynamic scenes, which also benefits
numerous LLM-based applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LLM-based Agent</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Contrast: Better Reflection Through Inconsistent Solving
  Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02009v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02009v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqi Zhang, Yongliang Shen, Linjuan Wu, Qiuying Peng, Jun Wang, Yueting Zhuang, Weiming Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The reflection capacity of Large Language Model (LLM) has garnered extensive
attention. A post-hoc prompting strategy, e.g., reflexion and self-refine,
refines LLM's response based on self-evaluated or external feedback. However,
recent research indicates without external feedback, LLM's intrinsic reflection
is unstable. Our investigation unveils that the key bottleneck is the quality
of the self-evaluated feedback. We find LLMs often exhibit overconfidence or
high randomness when self-evaluate, offering stubborn or inconsistent feedback,
which causes poor reflection. To remedy this, we advocate Self-Contrast: It
adaptively explores diverse solving perspectives tailored to the request,
contrasts the differences, and summarizes these discrepancies into a checklist
which could be used to re-examine and eliminate discrepancies. Our method
endows LLM with diverse perspectives to alleviate stubborn biases. Moreover,
their discrepancies indicate potential errors or inherent uncertainties that
LLM often overlooks. Reflecting upon these can catalyze more accurate and
stable reflection. Experiments conducted on a series of reasoning and
translation tasks with different LLMs serve to underscore the effectiveness and
generality of our strategy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalization Bounds: Perspectives from Information Theory and
  PAC-Bayes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.04381v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.04381v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fredrik Hellström, Giuseppe Durisi, Benjamin Guedj, Maxim Raginsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental question in theoretical machine learning is generalization.
Over the past decades, the PAC-Bayesian approach has been established as a
flexible framework to address the generalization capabilities of machine
learning algorithms, and design new ones. Recently, it has garnered increased
interest due to its potential applicability for a variety of learning
algorithms, including deep neural networks. In parallel, an
information-theoretic view of generalization has developed, wherein the
relation between generalization and various information measures has been
established. This framework is intimately connected to the PAC-Bayesian
approach, and a number of results have been independently discovered in both
strands. In this monograph, we highlight this strong connection and present a
unified treatment of PAC-Bayesian and information-theoretic generalization
bounds. We present techniques and results that the two perspectives have in
common, and discuss the approaches and interpretations that differ. In
particular, we demonstrate how many proofs in the area share a modular
structure, through which the underlying ideas can be intuited. We pay special
attention to the conditional mutual information (CMI) framework; analytical
studies of the information complexity of learning algorithms; and the
application of the proposed methods to deep learning. This monograph is
intended to provide a comprehensive introduction to information-theoretic
generalization bounds and their connection to PAC-Bayes, serving as a
foundation from which the most recent developments are accessible. It is aimed
broadly towards researchers with an interest in generalization and theoretical
machine learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>228 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoupled Data Consistency with Diffusion Purification for Image
  Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06054v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06054v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Soo Min Kwon, Ismail R. Alkhouri, Saiprasad Ravishankar, Qing Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have recently gained traction as a powerful class of deep
generative priors, excelling in a wide range of image restoration tasks due to
their exceptional ability to model data distributions. To solve image
restoration problems, many existing techniques achieve data consistency by
incorporating additional likelihood gradient steps into the reverse sampling
process of diffusion models. However, the additional gradient steps pose a
challenge for real-world practical applications as they incur a large
computational overhead, thereby increasing inference time. They also present
additional difficulties when using accelerated diffusion model samplers, as the
number of data consistency steps is limited by the number of reverse sampling
steps. In this work, we propose a novel diffusion-based image restoration
solver that addresses these issues by decoupling the reverse process from the
data consistency steps. Our method involves alternating between a
reconstruction phase to maintain data consistency and a refinement phase that
enforces the prior via diffusion purification. Our approach demonstrates
versatility, making it highly adaptable for efficient problem-solving in latent
space. Additionally, it reduces the necessity for numerous sampling steps
through the integration of consistency models. The efficacy of our approach is
validated through comprehensive experiments across various image restoration
tasks, including image denoising, deblurring, inpainting, and super-resolution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedSN: A Novel <span class="highlight-title">Federated</span> Learning Framework over LEO Satellite Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01483v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01483v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Lin, Zhe Chen, Zihan Fang, Xianhao Chen, Xiong Wang, Yue Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, a large number of Low Earth Orbit (LEO) satellites have been
launched and deployed successfully in space by commercial companies, such as
SpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve
not only for communication but also for various machine learning applications,
such as space modulation recognition, remote sensing image classification, etc.
However, the ground station (GS) may be incapable of downloading such a large
volume of raw sensing data for centralized model training due to the limited
contact time with LEO satellites (e.g. 5 minutes). Therefore, federated
learning (FL) has emerged as the promising solution to address this problem via
on-device training. Unfortunately, to enable FL on LEO satellites, we still
face three critical challenges that are i) heterogeneous computing and memory
capabilities, ii) limited uplink rate, and iii) model staleness. To this end,
we propose FedSN as a general FL framework to tackle the above challenges, and
fully explore data diversity on LEO satellites. Specifically, we first present
a novel sub-structure scheme to enable heterogeneous local model training
considering different computing, memory, and communication constraints on LEO
satellites. Additionally, we propose a pseudo-synchronous model aggregation
strategy to dynamically schedule model aggregation for compensating model
staleness. To further demonstrate the effectiveness of the FedSN, we evaluate
it using space modulation recognition and remote sensing image classification
tasks by leveraging the data from real-world satellite networks. Extensive
experimental results demonstrate that FedSN framework achieves higher accuracy,
lower computing, and communication overhead than the state-of-the-art
benchmarks and the effectiveness of each components in FedSN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nonlinear Control Allocation: A Learning Based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.06180v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.06180v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hafiz Zeeshan Iqbal Khan, Surrayya Mobeen, Jahanzeb Rajput, Jamshed Riaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern aircraft are designed with redundant control effectors to cater for
fault tolerance and maneuverability requirements. This leads to aircraft being
over-actuated and requires control allocation schemes to distribute the control
commands among control effectors. Traditionally, optimization-based control
allocation schemes are used; however, for nonlinear allocation problems, these
methods require large computational resources. In this work, an artificial
neural network (ANN) based nonlinear control allocation scheme is proposed. The
proposed scheme is composed of learning the inverse of the control
effectiveness map through ANN, and then implementing it as an allocator instead
of solving an online optimization problem. Stability conditions are presented
for closed-loop systems incorporating the allocator, and computational
challenges are explored with piece-wise linear effectiveness functions and
ANN-based allocators. To demonstrate the efficacy of the proposed scheme, it is
compared with a standard quadratic programming-based method for control
allocation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE Conference on Decision and Control (CDC), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03100v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03100v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeqian Ju, Yuancheng Wang, Kai Shen, Xu Tan, Detai Xin, Dongchao Yang, Yanqing Liu, Yichong Leng, Kaitao Song, Siliang Tang, Zhizheng Wu, Tao Qin, Xiang-Yang Li, Wei Ye, Shikun Zhang, Jiang Bian, Lei He, Jinyu Li, Sheng Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent large-scale text-to-speech (TTS) models have achieved
significant progress, they still fall short in speech quality, similarity, and
prosody. Considering speech intricately encompasses various attributes (e.g.,
content, prosody, timbre, and acoustic details) that pose significant
challenges for generation, a natural idea is to factorize speech into
individual subspaces representing different attributes and generate them
individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with
novel factorized diffusion models to generate natural speech in a zero-shot
way. Specifically, 1) we design a neural codec with factorized vector
quantization (FVQ) to disentangle speech waveform into subspaces of content,
prosody, timbre, and acoustic details; 2) we propose a factorized diffusion
model to generate attributes in each subspace following its corresponding
prompt. With this factorization design, NaturalSpeech 3 can effectively and
efficiently model intricate speech with disentangled subspaces in a
divide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the
state-of-the-art TTS systems on quality, similarity, prosody, and
intelligibility, and achieves on-par quality with human recordings.
Furthermore, we achieve better performance by scaling to 1B parameters and 200K
hours of training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Achieving human-level quality and naturalness on multi-speaker
  datasets (e.g., LibriSpeech) in a zero-shot way</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and
  Ethics) Evaluation: A Review 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03123v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03123v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunder Ali Khowaja, Parus Khuwaja, Kapal Dev, Weizheng Wang, Lewis Nkenyereye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT is another large language model (LLM) vastly available for the
consumers on their devices but due to its performance and ability to converse
effectively, it has gained a huge popularity amongst research as well as
industrial community. Recently, many studies have been published to show the
effectiveness, efficiency, integration, and sentiments of chatGPT and other
LLMs. In contrast, this study focuses on the important aspects that are mostly
overlooked, i.e. sustainability, privacy, digital divide, and ethics and
suggests that not only chatGPT but every subsequent entry in the category of
conversational bots should undergo Sustainability, PrivAcy, Digital divide, and
Ethics (SPADE) evaluation. This paper discusses in detail the issues and
concerns raised over chatGPT in line with aforementioned characteristics. We
also discuss the recent EU AI Act briefly in accordance with the SPADE
evaluation. We support our hypothesis by some preliminary data collection and
visualizations along with hypothesized facts. We also suggest mitigations and
recommendations for each of the concerns. Furthermore, we also suggest some
policies and recommendations for EU AI policy act concerning ethics, digital
divide, and sustainability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 8 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Incorporating simulated spatial context information improves the
  effectiveness of contrastive learning models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15120v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15120v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lizhen Zhu, James Z. Wang, Wonseuk Lee, Brad Wyble
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual learning often occurs in a specific context, where an agent acquires
skills through exploration and tracking of its location in a consistent
environment. The historical spatial context of the agent provides a similarity
signal for self-supervised contrastive learning. We present a unique approach,
termed Environmental Spatial Similarity (ESS), that complements existing
contrastive learning methods. Using images from simulated, photorealistic
environments as an experimental setting, we demonstrate that ESS outperforms
traditional instance discrimination approaches. Moreover, sampling additional
data from the same environment substantially improves accuracy and provides new
augmentations. ESS allows remarkable proficiency in room classification and
spatial prediction tasks, especially in unfamiliar environments. This learning
paradigm has the potential to enable rapid visual learning in agents operating
in new environments with unique visual characteristics. Potentially
transformative applications span from robotics to space exploration. Our proof
of concept demonstrates improved efficiency over methods that rely on
extensive, disconnected datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Stochastic Approximation with Delayed Updates: Finite-Time Rates under
  Markovian Sampling <span class="chip">AISTATS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11800v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11800v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arman Adibi, Nicolo Dal Fabbro, Luca Schenato, Sanjeev Kulkarni, <span class="highlight-author">H. Vincent Poor</span>, George J. Pappas, Hamed Hassani, Aritra Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by applications in large-scale and multi-agent reinforcement
learning, we study the non-asymptotic performance of stochastic approximation
(SA) schemes with delayed updates under Markovian sampling. While the effect of
delays has been extensively studied for optimization, the manner in which they
interact with the underlying Markov process to shape the finite-time
performance of SA remains poorly understood. In this context, our first main
contribution is to show that under time-varying bounded delays, the delayed SA
update rule guarantees exponentially fast convergence of the \emph{last
iterate} to a ball around the SA operator's fixed point. Notably, our bound is
\emph{tight} in its dependence on both the maximum delay $\tau_{max}$, and the
mixing time $\tau_{mix}$. To achieve this tight bound, we develop a novel
inductive proof technique that, unlike various existing delayed-optimization
analyses, relies on establishing uniform boundedness of the iterates. As such,
our proof may be of independent interest. Next, to mitigate the impact of the
maximum delay on the convergence rate, we provide the first finite-time
analysis of a delay-adaptive SA scheme under Markovian sampling. In particular,
we show that the exponent of convergence of this scheme gets scaled down by
$\tau_{avg}$, as opposed to $\tau_{max}$ for the vanilla delayed SA rule; here,
$\tau_{avg}$ denotes the average delay across all iterations. Moreover, the
adaptive scheme requires no prior knowledge of the delay sequence for step-size
tuning. Our theoretical findings shed light on the finite-time effects of
delays for a broad class of algorithms, including TD learning, Q-learning, and
stochastic gradient descent under Markovian sampling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 27th International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2024!</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Negative Evidential Deep Learning for Open-set Semi-supervised
  Learning <span class="chip">AAAI2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12091v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12091v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Yu, Danruo Deng, Furui Liu, Yueming Jin, Qi Dou, Guangyong Chen, Pheng-Ann Heng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning (SSL) methods assume that labeled data, unlabeled
data and test data are from the same distribution. Open-set semi-supervised
learning (Open-set SSL) considers a more practical scenario, where unlabeled
data and test data contain new categories (outliers) not observed in labeled
data (inliers). Most previous works focused on outlier detection via binary
classifiers, which suffer from insufficient scalability and inability to
distinguish different types of uncertainty. In this paper, we propose a novel
framework, Adaptive Negative Evidential Deep Learning (ANEDL) to tackle these
limitations. Concretely, we first introduce evidential deep learning (EDL) as
an outlier detector to quantify different types of uncertainty, and design
different uncertainty metrics for self-training and inference. Furthermore, we
propose a novel adaptive negative optimization strategy, making EDL more
tailored to the unlabeled dataset containing both inliers and outliers. As
demonstrated empirically, our proposed method outperforms existing
state-of-the-art methods across four datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental
  Health Sensing Studies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17219v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17219v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshat Choube, Vedant Das Swain, Varun Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in mobile and wearable technologies have enabled the potential to
passively monitor a person's mental, behavioral, and affective health. These
approaches typically rely on longitudinal collection of self-reported outcomes,
e.g., depression, stress, and anxiety, to train machine learning (ML) models.
However, the need to continuously self-report adds a significant burden on the
participants, often resulting in attrition, missing labels, or insincere
responses. In this work, we introduce the Scale Scores Simulation using Mental
Models (SeSaMe) framework to alleviate participants' burden in digital mental
health studies. By leveraging pre-trained large language models (LLMs), SeSaMe
enables the simulation of participants' responses on psychological scales. In
SeSaMe, researchers can prompt LLMs with information on participants' internal
behavioral dispositions, enabling LLMs to construct mental models of
participants to simulate their responses on psychological scales. We
demonstrate an application of SeSaMe, where we use GPT-4 to simulate responses
on one scale using responses from another as behavioral information. We also
evaluate the alignment between human and SeSaMe-simulated responses to
psychological scales. Then, we present experiments to inspect the utility of
SeSaMe-simulated responses as ground truth in training ML models by replicating
established depression and anxiety screening tasks from a previous study. Our
results indicate SeSaMe to be a promising approach, but its alignment may vary
across scales and specific prediction objectives. We also observed that model
performance with simulated data was on par with using the real data for
training in most evaluation scenarios. We conclude by discussing the potential
implications of SeSaMe in addressing some challenges researchers face with
ground-truth collection in passive sensing studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Byzantine-resilient <span class="highlight-title">Federated</span> Learning With Adaptivity to Data
  Heterogeneity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13374v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13374v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyuan Zuo, Xingrun Yan, Rongfei Fan, Han Hu, Hangguan Shan, Tony Q. S. Quek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper deals with federated learning (FL) in the presence of malicious
Byzantine attacks and data heterogeneity. A novel Robust Average Gradient
Algorithm (RAGA) is proposed, which leverages the geometric median for
aggregation and can freely select the round number for local updating.
Different from most existing resilient approaches, which perform convergence
analysis based on strongly-convex loss function or homogeneously distributed
dataset, we conduct convergence analysis for not only strongly-convex but also
non-convex loss function over heterogeneous dataset. According to our
theoretical analysis, as long as the fraction of dataset from malicious users
is less than half, RAGA can achieve convergence at rate
$\mathcal{O}({1}/{T^{2/3- \delta}})$ where $T$ is the iteration number and
$\delta \in (0, 2/3)$ for non-convex loss function, and at linear rate for
strongly-convex loss function. Moreover, stationary point or global optimal
solution is proved to obtainable as data heterogeneity vanishes. Experimental
results corroborate the robustness of RAGA to Byzantine attacks and verifies
the advantage of RAGA over baselines on convergence performance under various
intensity of Byzantine attacks, for heterogeneous dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Demystifying Misconceptions in Social Bots Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17251v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17251v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefano Cresci, Kai-Cheng Yang, Angelo Spognardi, Roberto Di Pietro, Filippo Menczer, Marinella Petrocchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research on social bots aims at advancing knowledge and providing solutions
to one of the most debated forms of online manipulation. Yet, social bot
research is plagued by widespread biases, hyped results, and misconceptions
that set the stage for ambiguities, unrealistic expectations, and seemingly
irreconcilable findings. Overcoming such issues is instrumental towards
ensuring reliable solutions and reaffirming the validity of the scientific
method. In this contribution, we review some recent results in social bots
research, highlighting and revising factual errors as well as methodological
and conceptual biases. More importantly, we demystify common misconceptions,
addressing fundamental points on how social bots research is discussed. Our
analysis surfaces the need to discuss research about online disinformation and
manipulation in a rigorous, unbiased, and responsible way. This article
bolsters such effort by identifying and refuting common fallacious arguments
used by both proponents and opponents of social bots research, as well as
providing directions toward sound methodologies for future research in the
field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepMachining: Online <span class="highlight-title">Predict</span>ion of Machining Errors of Lathe Machines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16451v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16451v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang-Li Lu, Hwai-Jung Hsu, Che-Wei Chou, H. T. Kung, Chen-Hsin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe DeepMachining, a deep learning-based AI system for online
prediction of machining errors of lathe machine operations. We have built and
evaluated DeepMachining based on manufacturing data from factories.
Specifically, we first pretrain a deep learning model for a given lathe
machine's operations to learn the salient features of machining states. Then,
we fine-tune the pretrained model to adapt to specific machining tasks. We
demonstrate that DeepMachining achieves high prediction accuracy for multiple
tasks that involve different workpieces and cutting tools. To the best of our
knowledge, this work is one of the first factory experiments using pre-trained
deep-learning models to predict machining errors of lathe machines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structure Guided Large Language Model for SQL Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13284v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13284v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinggang Zhang, Junnan Dong, Hao Chen, Wentao Li, Feiran Huang, Xiao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating accurate Structured Querying Language (SQL) is a long-standing
problem, especially in matching users' semantic queries with structured
databases and then generating structured SQL. Existing models typically input
queries and database schemas into the LLM and rely on the LLM to perform
semantic-structure matching and generate structured SQL. However, such
solutions overlook the structural information within user queries and
databases, which can be utilized to enhance the generation of structured SQL.
This oversight can lead to inaccurate or unexecutable SQL generation. To fully
exploit the structure, we propose a structure-to-SQL framework, which leverages
the inherent structure information to improve the SQL generation of LLMs.
Specifically, we introduce our Structure Guided SQL~(SGU-SQL) generation model.
SGU-SQL first links user queries and databases in a structure-enhanced manner.
It then decomposes complicated linked structures with grammar trees to guide
the LLM to generate the SQL step by step. Extensive experiments on two
benchmark datasets illustrate that SGU-SQL can outperform sixteen SQL
generation baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recurrent Action Transformer with Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.09459v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.09459v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexey Staroverov, Egor Cherepanov, Dmitry Yudin, Alexey K. Kovalev, Aleksandr I. Panov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the use of transformers in offline reinforcement learning has
become a rapidly developing area. This is due to their ability to treat the
agent's trajectory in the environment as a sequence, thereby reducing the
policy learning problem to sequence modeling. In environments where the agent's
decisions depend on past events, it is essential to capture both the event
itself and the decision point in the context of the model. However, the
quadratic complexity of the attention mechanism limits the potential for
context expansion. One solution to this problem is to enhance transformers with
memory mechanisms. In this paper, we propose the Recurrent Action Transformer
with Memory (RATE) - a model that incorporates recurrent memory. To evaluate
our model, we conducted extensive experiments on both memory-intensive
environments (VizDoom-Two-Color, T-Maze) and classic Atari games and MuJoCo
control environments. The results show that the use of memory can significantly
improve performance in memory-intensive environments while maintaining or
improving results in classic environments. We hope that our findings will
stimulate research on memory mechanisms for transformers applicable to offline
reinforcement learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Pre-Training of Time-Series Data for Unsupervised Fault
  Detection in Semiconductor Manufacturing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sewoong Lee, JinKyou Choi, Min Su Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces TRACE-GPT, which stands for Time-seRies
Anomaly-detection with Convolutional Embedding and Generative Pre-trained
Transformers. TRACE-GPT is designed to pre-train univariate time-series sensor
data and detect faults on unlabeled datasets in semiconductor manufacturing. In
semiconductor industry, classifying abnormal time-series sensor data from
normal data is important because it is directly related to wafer defect.
However, small, unlabeled, and even mixed training data without enough
anomalies make classification tasks difficult. In this research, we capture
features of time-series data with temporal convolutional embedding and
Generative Pre-trained Transformer (GPT) to classify abnormal sequences from
normal sequences using cross entropy loss. We prove that our model shows better
performance than previous unsupervised models with both an open dataset, the
University of California Riverside (UCR) time-series classification archive,
and the process log of our Chemical Vapor Deposition (CVD) equipment. Our model
has the highest F1 score at Equal Error Rate (EER) across all datasets and is
only 0.026 below the supervised state-of-the-art baseline on the open dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attacks, Defenses and Evaluations for LLM Conversation Safety: A <span class="highlight-title">Survey</span> <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09283v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09283v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichen Dong, Zhanhui Zhou, Chao Yang, Jing Shao, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are now commonplace in conversation
applications. However, their risks of misuse for generating harmful responses
have raised serious societal concerns and spurred recent research on LLM
conversation safety. Therefore, in this survey, we provide a comprehensive
overview of recent studies, covering three critical aspects of LLM conversation
safety: attacks, defenses, and evaluations. Our goal is to provide a structured
summary that enhances understanding of LLM conversation safety and encourages
further investigation into this important subject. For easy reference, we have
categorized all the studies mentioned in this survey according to our taxonomy,
available at: https://github.com/niconi19/LLM-conversation-safety.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shapley Values-Powered Framework for Fair Reward Split in Content
  Produced by GenAI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09700v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09700v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Glinsky, Alexey Sokolsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is evident that, currently, generative models are surpassed in quality by
human professionals. However, with the advancements in Artificial Intelligence,
this gap will narrow, leading to scenarios where individuals who have dedicated
years of their lives to mastering a skill become obsolete due to their high
costs, which are inherently linked to the time they require to complete a task
-- a task that AI could accomplish in minutes or seconds. To avoid future
social upheavals, we must, even now, contemplate how to fairly assess the
contributions of such individuals in training generative models and how to
compensate them for the reduction or complete loss of their incomes. In this
work, we propose a method to structure collaboration between model developers
and data providers. To achieve this, we employ Shapley Values to quantify the
contribution of artist(s) in an image generated by the Stable Diffusion-v1.5
model and to equitably allocate the reward among them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 32 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ABScribe: Rapid Exploration & Organization of Multiple Writing
  Variations in Human-AI Co-Writing Tasks using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00117v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00117v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohi Reza, Nathan Laundry, Ilya Musabirov, Peter Dushniku, Zhi Yuan "Michael" Yu, Kashish Mittal, Tovi Grossman, Michael Liut, Anastasia Kuzminykh, Joseph Jay Williams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploring alternative ideas by rewriting text is integral to the writing
process. State-of-the-art Large Language Models (LLMs) can simplify writing
variation generation. However, current interfaces pose challenges for
simultaneous consideration of multiple variations: creating new variations
without overwriting text can be difficult, and pasting them sequentially can
clutter documents, increasing workload and disrupting writers' flow. To tackle
this, we present ABScribe, an interface that supports rapid, yet visually
structured, exploration and organization of writing variations in human-AI
co-writing tasks. With ABScribe, users can swiftly modify variations using LLM
prompts, which are auto-converted into reusable buttons. Variations are stored
adjacently within text fields for rapid in-place comparisons using mouse-over
interactions on a popup toolbar. Our user study with 12 writers shows that
ABScribe significantly reduces task workload (d = 1.20, p < 0.001), enhances
user perceptions of the revision process (d = 2.41, p < 0.001) compared to a
popular baseline workflow, and provides insights into how writers explore
variations using LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CHI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label
  Learning <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10365v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10365v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyu Tian, Hongxin Wei, Yiqun Wang, Lei Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partial-label learning (PLL) is an important weakly supervised learning
problem, which allows each training example to have a candidate label set
instead of a single ground-truth label. Identification-based methods have been
widely explored to tackle label ambiguity issues in PLL, which regard the true
label as a latent variable to be identified. However, identifying the true
labels accurately and completely remains challenging, causing noise in pseudo
labels during model training. In this paper, we propose a new method called
CroSel, which leverages historical predictions from the model to identify true
labels for most training examples. First, we introduce a cross selection
strategy, which enables two deep models to select true labels of partially
labeled data for each other. Besides, we propose a novel consistency
regularization term called co-mix to avoid sample waste and tiny noise caused
by false selection. In this way, CroSel can pick out the true labels of most
examples with high precision. Extensive experiments demonstrate the superiority
of CroSel, which consistently outperforms previous state-of-the-art methods on
benchmark datasets. Additionally, our method achieves over 90\% accuracy and
quantity for selecting true labels on CIFAR-type datasets under various
settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Challenging Common Paradigms in Multi-Task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.04698v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.04698v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cathrin Elich, Lukas Kirchdorfer, Jan M. Köhler, Lukas Schott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While multi-task learning (MTL) has gained significant attention in recent
years, its underlying mechanisms remain poorly understood. Recent methods did
not yield consistent performance improvements over single task learning (STL)
baselines, underscoring the importance of gaining more profound insights about
challenges specific to MTL. In our study, we challenge paradigms in MTL in the
context of STL: First, the impact of the choice of optimizer has only been
mildly investigated in MTL. We show the pivotal role of common STL tools such
as the Adam optimizer in MTL empirically in various experiments. To further
investigate Adam's effectiveness, we theoretical derive a partial loss-scale
invariance under mild assumptions. Second, the notion of gradient conflicts has
often been phrased as a specific problem in MTL. We delve into the role of
gradient conflicts in MTL and compare it to STL. For angular gradient alignment
we find no evidence that this is a unique problem in MTL. We emphasize
differences in gradient magnitude as the main distinguishing factor. Lastly, we
compare the transferability of features learned through MTL and STL on common
image corruptions, and find light evidence that MTL can lead to superior
transferability. Overall, we find surprising similarities between STL and MTL
suggesting to consider methods from both fields in a broader context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>-</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Solving a Real-World Package Delivery Routing Problem Using Quantum
  Annealers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15114v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15114v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eneko Osaba, Esther Villar-Rodriguez, Antón Asla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research focused on the conjunction between quantum computing and routing
problems has been very prolific in recent years. Most of the works revolve
around classical problems such as the Traveling Salesman Problem or the Vehicle
Routing Problem. Even though working on these problems is valuable, it is also
undeniable that their academic-oriented nature falls short of real-world
requirements. The main objective of this research is to present a solving
method for realistic instances, avoiding problem relaxations or technical
shortcuts. Instead, a quantum-classical hybrid solver has been developed,
coined Q4RPD, that considers a set of real constraints such as a heterogeneous
fleet of vehicles, priority deliveries, and capacities characterized by two
values: weight and dimensions of the packages. Q4RPD resorts to the Leap
Constrained Quadratic Model Hybrid Solver of D-Wave. To demonstrate the
application of Q4RPD, an experimentation composed of six different instances
has been conducted, aiming to serve as illustrative examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures and 4 tables. Paper submitted for review in
  Scientific Reports</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose
  Estimation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12028v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12028v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Li, Mengyuan Liu, Hong Liu, Pichao Wang, Jialun Cai, Nicu Sebe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have been successfully applied in the field of video-based 3D
human pose estimation. However, the high computational costs of these video
pose transformers (VPTs) make them impractical on resource-constrained devices.
In this paper, we present a plug-and-play pruning-and-recovering framework,
called Hourglass Tokenizer (HoT), for efficient transformer-based 3D human pose
estimation from videos. Our HoT begins with pruning pose tokens of redundant
frames and ends with recovering full-length tokens, resulting in a few pose
tokens in the intermediate transformer blocks and thus improving the model
efficiency. To effectively achieve this, we propose a token pruning cluster
(TPC) that dynamically selects a few representative tokens with high semantic
diversity while eliminating the redundancy of video frames. In addition, we
develop a token recovering attention (TRA) to restore the detailed
spatio-temporal information based on the selected tokens, thereby expanding the
network output to the original full-length temporal resolution for fast
inference. Extensive experiments on two benchmark datasets (i.e., Human3.6M and
MPI-INF-3DHP) demonstrate that our method can achieve both high efficiency and
estimation accuracy compared to the original VPT models. For instance, applying
to MotionBERT and MixSTE on Human3.6M, our HoT can save nearly 50% FLOPs
without sacrificing accuracy and nearly 40% FLOPs with only 0.2% accuracy drop,
respectively. Code and models are available at
https://github.com/NationalGAILab/HoT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024, Open Sourced</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on
  Prompt-based Language Models <span class="chip">NAACL2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16432v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16432v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Xu, Wenjie Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt-based learning is a new language model training paradigm that adapts
the Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes
the performance benchmarks across various natural language processing (NLP)
tasks. Instead of using a fixed prompt template to fine-tune the model, some
research demonstrates the effectiveness of searching for the prompt via
optimization. Such prompt optimization process of prompt-based learning on PLMs
also gives insight into generating adversarial prompts to mislead the model,
raising concerns about the adversarial vulnerability of this paradigm. Recent
studies have shown that universal adversarial triggers (UATs) can be generated
to alter not only the predictions of the target PLMs but also the prediction of
corresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based
learning paradigm. However, UATs found in previous works are often unreadable
tokens or characters and can be easily distinguished from natural texts with
adaptive defenses. In this work, we consider the naturalness of the UATs and
develop $\textit{LinkPrompt}$, an adversarial attack algorithm to generate UATs
by a gradient-based beam search algorithm that not only effectively attacks the
target PLMs and PFMs but also maintains the naturalness among the trigger
tokens. Extensive results demonstrate the effectiveness of
$\textit{LinkPrompt}$, as well as the transferability of UATs generated by
$\textit{LinkPrompt}$ to open-sourced Large Language Model (LLM) Llama2 and
API-accessed LLM GPT-3.5-turbo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the main conference of NAACL2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLatrieval: LLM-Verified Retrieval for Verifiable Generation <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07838v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07838v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaonan Li, Changtai Zhu, Linyang Li, Zhangyue Yin, Tianxiang Sun, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Verifiable generation aims to let the large language model (LLM) generate
text with supporting documents, which enables the user to flexibly verify the
answer and makes the LLM's output more reliable. Retrieval plays a crucial role
in verifiable generation. Specifically, the retrieved documents not only
supplement knowledge to help the LLM generate correct answers, but also serve
as supporting evidence for the user to verify the LLM's output. However, the
widely used retrievers become the bottleneck of the entire pipeline and limit
the overall performance. Their capabilities are usually inferior to LLMs since
they often have much fewer parameters than the large language model and have
not been demonstrated to scale well to the size of LLMs. If the retriever does
not correctly find the supporting documents, the LLM can not generate the
correct and verifiable answer, which overshadows the LLM's remarkable
abilities. To address these limitations, we propose \LLatrieval (Large Language
Model Verified Retrieval), where the LLM updates the retrieval result until it
verifies that the retrieved documents can sufficiently support answering the
question. Thus, the LLM can iteratively provide feedback to retrieval and
facilitate the retrieval result to fully support verifiable generation.
Experiments show that LLatrieval significantly outperforms extensive baselines
and achieves state-of-the-art results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NAACL 2024 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Object Coherence in Layout-to-Image Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10522v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10522v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibin Wang, Weizhong Zhang, Jianwei Zheng, Cheng Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Layout-to-image synthesis is an emerging technique in conditional image
generation. It aims to generate complex scenes, where users require fine
control over the layout of the objects in a scene. However, it remains
challenging to control the object coherence, including semantic coherence
(e.g., the cat looks at the flowers or not) and physical coherence (e.g., the
hand and the racket should not be misaligned). In this paper, we propose a
novel diffusion model with effective global semantic fusion (GSF) and
self-similarity feature enhancement modules to guide the object coherence for
this task. For semantic coherence, we argue that the image caption contains
rich information for defining the semantic relationship within the objects in
the images. Instead of simply employing cross-attention between captions and
generated images, which addresses the highly relevant layout restriction and
semantic coherence separately and thus leads to unsatisfying results shown in
our experiments, we develop GSF to fuse the supervision from the layout
restriction and semantic coherence requirement and exploit it to guide the
image synthesis process. Moreover, to improve the physical coherence, we
develop a Self-similarity Coherence Attention (SCA) module to explicitly
integrate local contextual physical coherence into each pixel's generation
process. Specifically, we adopt a self-similarity map to encode the coherence
restrictions and employ it to extract coherent features from text embedding.
Through visualization of our self-similarity map, we explore the essence of
SCA, revealing that its effectiveness is not only in capturing reliable
physical coherence patterns but also in enhancing complex texture generation.
Extensive experiments demonstrate the superiority of our proposed method in
both image generation quality and controllability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01739v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01739v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, Yang You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To help the open-source community have a better understanding of
Mixture-of-Experts (MoE) based large language models (LLMs), we train and
release OpenMoE, a series of fully open-sourced and reproducible decoder-only
MoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T
tokens. Our investigation confirms that MoE-based LLMs can offer a more
favorable cost-effectiveness trade-off than dense LLMs, highlighting the
potential effectiveness for future LLM development.
  One more important contribution of this study is an in-depth analysis of the
routing mechanisms within our OpenMoE models, leading to three significant
findings: Context-Independent Specialization, Early Routing Learning, and
Drop-towards-the-End. We discovered that routing decisions in MoE models are
predominantly based on token IDs, with minimal context relevance. The
token-to-expert assignments are determined early in the pre-training phase and
remain largely unchanged. This imperfect routing can result in performance
degradation, particularly in sequential tasks like multi-turn conversations,
where tokens appearing later in a sequence are more likely to be dropped.
Finally, we rethink our design based on the above-mentioned observations and
analysis. To facilitate future MoE LLM development, we propose potential
strategies for mitigating the issues we found and further improving
off-the-shelf MoE LLM designs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VIGraph: Generative Self-supervised Learning for Class-Imbalanced Node
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01191v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01191v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulan Hu, Sheng Ouyang, Zhirui Yang, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class imbalance in graph data presents significant challenges for node
classification. While existing methods, such as SMOTE-based approaches,
partially mitigate this issue, they still exhibit limitations in constructing
imbalanced graphs. Generative self-supervised learning (SSL) methods,
exemplified by graph autoencoders (GAEs), offer a promising solution by
directly generating minority nodes from the data itself, yet their potential
remains underexplored. In this paper, we delve into the shortcomings of
SMOTE-based approaches in the construction of imbalanced graphs. Furthermore,
we introduce VIGraph, a simple yet effective generative SSL approach that
relies on the Variational GAE as the fundamental model. VIGraph strictly
adheres to the concept of imbalance when constructing imbalanced graphs and
innovatively leverages the variational inference (VI) ability of Variational
GAE to generate nodes for minority classes. VIGraph introduces comprehensive
training strategies, including cross-view contrastive learning at the decoding
phase to capture semantic knowledge, adjacency matrix reconstruction to
preserve graph structure, and alignment strategy to ensure stable training.
VIGraph can generate high-quality nodes directly usable for classification,
eliminating the need to integrate the generated nodes back to the graph as well
as additional retraining found in SMOTE-based methods. We conduct extensive
experiments, results from which demonstrate the superiority and generality of
our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Quadruped Locomotion Using Differentiable Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14864v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14864v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunlong Song, Sangbae Kim, Davide Scaramuzza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While most recent advancements in legged robot control have been driven by
model-free reinforcement learning, we explore the potential of differentiable
simulation. Differentiable simulation promises faster convergence and more
stable training by computing low-variant first-order gradients using the robot
model, but so far, its use for legged robot control has remained limited to
simulation. The main challenge with differentiable simulation lies in the
complex optimization landscape of robotic tasks due to discontinuities in
contact-rich environments, e.g., quadruped locomotion. This work proposes a
new, differentiable simulation framework to overcome these challenges. The key
idea involves decoupling the complex whole-body simulation, which may exhibit
discontinuities due to contact, into two separate continuous domains.
Subsequently, we align the robot state resulting from the simplified model with
a more precise, non-differentiable simulator to maintain sufficient simulation
accuracy. Our framework enables learning quadruped walking in minutes using a
single simulated robot without any parallelization. When augmented with GPU
parallelization, our approach allows the quadruped robot to master diverse
locomotion skills, including trot, pace, bound, and gallop, on challenging
terrains in minutes. Additionally, our policy achieves robust locomotion
performance in the real world zero-shot. To the best of our knowledge, this
work represents the first demonstration of using differentiable simulation for
controlling a real quadruped robot. This work provides several important
insights into using differentiable simulations for legged locomotion in the
real world.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieval-Augmented Generation for Large Language Models: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10997v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10997v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) showcase impressive capabilities but encounter
challenges like hallucination, outdated knowledge, and non-transparent,
untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has
emerged as a promising solution by incorporating knowledge from external
databases. This enhances the accuracy and credibility of the generation,
particularly for knowledge-intensive tasks, and allows for continuous knowledge
updates and integration of domain-specific information. RAG synergistically
merges LLMs' intrinsic knowledge with the vast, dynamic repositories of
external databases. This comprehensive review paper offers a detailed
examination of the progression of RAG paradigms, encompassing the Naive RAG,
the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the
tripartite foundation of RAG frameworks, which includes the retrieval, the
generation and the augmentation techniques. The paper highlights the
state-of-the-art technologies embedded in each of these critical components,
providing a profound understanding of the advancements in RAG systems.
Furthermore, this paper introduces up-to-date evaluation framework and
benchmark. At the end, this article delineates the challenges currently faced
and points out prospective avenues for research and development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing Work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ World Models via Policy-Guided Trajectory Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.08533v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.08533v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marc Rigter, Jun Yamada, Ingmar Posner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  World models are a powerful tool for developing intelligent agents. By
predicting the outcome of a sequence of actions, world models enable policies
to be optimised via on-policy reinforcement learning (RL) using synthetic data,
i.e. in "in imagination". Existing world models are autoregressive in that they
interleave predicting the next state with sampling the next action from the
policy. Prediction error inevitably compounds as the trajectory length grows.
In this work, we propose a novel world modelling approach that is not
autoregressive and generates entire on-policy trajectories in a single pass
through a diffusion model. Our approach, Policy-Guided Trajectory Diffusion
(PolyGRAD), leverages a denoising model in addition to the gradient of the
action distribution of the policy to diffuse a trajectory of initially random
states and actions into an on-policy synthetic trajectory. We analyse the
connections between PolyGRAD, score-based generative models, and
classifier-guided diffusion models. Our results demonstrate that PolyGRAD
outperforms state-of-the-art baselines in terms of trajectory prediction error
for short trajectories, with the exception of autoregressive diffusion. For
short trajectories, PolyGRAD obtains similar errors to autoregressive
diffusion, but with lower computational requirements. For long trajectories,
PolyGRAD obtains comparable performance to baselines. Our experiments
demonstrate that PolyGRAD enables performant policies to be trained via
on-policy RL in imagination for MuJoCo continuous control domains. Thus,
PolyGRAD introduces a new paradigm for accurate on-policy world modelling
without autoregressive sampling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in TMLR, March 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Functional Graph Convolutional Networks: A unified multi-task and
  multi-modal learning framework to facilitate health and social-care insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10158v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10158v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobia Boschi, Francesca Bonin, Rodrigo Ordonez-Hurtado, Cécile Rousseau, Alessandra Pascale, John Dinsmore
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel Functional Graph Convolutional Network (funGCN)
framework that combines Functional Data Analysis and Graph Convolutional
Networks to address the complexities of multi-task and multi-modal learning in
digital health and longitudinal studies. With the growing importance of health
solutions to improve health care and social support, ensure healthy lives, and
promote well-being at all ages, funGCN offers a unified approach to handle
multivariate longitudinal data for multiple entities and ensures
interpretability even with small sample sizes. Key innovations include
task-specific embedding components that manage different data types, the
ability to perform classification, regression, and forecasting, and the
creation of a knowledge graph for insightful data interpretation. The efficacy
of funGCN is validated through simulation experiments and a real-data
application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Concept-Based Causal Transition and Symbolic Reasoning for
  Visual Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03325v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03325v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilue Qian, Peiyu Yu, Ying Nian Wu, Yao Su, Wei Wang, Lifeng Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual planning simulates how humans make decisions to achieve desired goals
in the form of searching for visual causal transitions between an initial
visual state and a final visual goal state. It has become increasingly
important in egocentric vision with its advantages in guiding agents to perform
daily tasks in complex environments. In this paper, we propose an interpretable
and generalizable visual planning framework consisting of i) a novel
Substitution-based Concept Learner (SCL) that abstracts visual inputs into
disentangled concept representations, ii) symbol abstraction and reasoning that
performs task planning via the self-learned symbols, and iii) a Visual Causal
Transition model (ViCT) that grounds visual causal transitions to semantically
similar real-world actions. Given an initial state, we perform goal-conditioned
visual planning with a symbolic reasoning method fueled by the learned
representations and causal transitions to reach the goal state. To verify the
effectiveness of the proposed model, we collect a large-scale visual planning
dataset based on AI2-THOR, dubbed as CCTP. Extensive experiments on this
challenging dataset demonstrate the superior performance of our method in
visual task planning. Empirically, we show that our framework can generalize to
unseen task trajectories, unseen object categories, and real-world data.
Further details of this work are provided at
https://fqyqc.github.io/ConTranPlan/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identifying the Correlation Between Language Distance and Cross-Lingual
  <span class="highlight-title">Transfer</span> in a Multilingual Representation Space <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.02151v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.02151v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fred Philippy, Siwen Guo, Shohreh Haddadan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior research has investigated the impact of various linguistic features on
cross-lingual transfer performance. In this study, we investigate the manner in
which this effect can be mapped onto the representation space. While past
studies have focused on the impact on cross-lingual alignment in multilingual
language models during fine-tuning, this study examines the absolute evolution
of the respective language representation spaces produced by MLLMs. We place a
specific emphasis on the role of linguistic characteristics and investigate
their inter-correlation with the impact on representation spaces and
cross-lingual transfer performance. Additionally, this paper provides
preliminary evidence of how these findings can be leveraged to enhance transfer
to linguistically distant languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGTYP Workshop 2023 (co-located with EACL 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Effects of Mixed Sample Data Augmentation are Class Dependent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09136v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09136v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haeil Lee, Hansang Lee, Junmo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixed Sample Data Augmentation (MSDA) techniques, such as Mixup, CutMix, and
PuzzleMix, have been widely acknowledged for enhancing performance in a variety
of tasks. A previous study reported the class dependency of traditional data
augmentation (DA), where certain classes benefit disproportionately compared to
others. This paper reveals a class dependent effect of MSDA, where some classes
experience improved performance while others experience degraded performance.
This research addresses the issue of class dependency in MSDA and proposes an
algorithm to mitigate it. The approach involves training on a mixture of MSDA
and non-MSDA data, which not only mitigates the negative impact on the affected
classes, but also improves overall accuracy. Furthermore, we provide in-depth
analysis and discussion of why MSDA introduced class dependencies and which
classes are most likely to have them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 18 figures, Overall Revision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18920v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18920v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongliang Cao, Marvin Eisenberger, Nafie El Amrani, Daniel Cremers, Florian Bernard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although 3D shape matching and interpolation are highly interrelated, they
are often studied separately and applied sequentially to relate different 3D
shapes, thus resulting in sub-optimal performance. In this work we present a
unified framework to predict both point-wise correspondences and shape
interpolation between 3D shapes. To this end, we combine the deep functional
map framework with classical surface deformation models to map shapes in both
spectral and spatial domains. On the one hand, by incorporating spatial maps,
our method obtains more accurate and smooth point-wise correspondences compared
to previous functional map methods for shape matching. On the other hand, by
introducing spectral maps, our method gets rid of commonly used but
computationally expensive geodesic distance constraints that are only valid for
near-isometric shape deformations. Furthermore, we propose a novel test-time
adaptation scheme to capture both pose-dominant and shape-dominant
deformations. Using different challenging datasets, we demonstrate that our
method outperforms previous state-of-the-art methods for both shape matching
and interpolation, even compared to supervised approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Regulatable AI Systems: Technical Gaps and Policy Opportunities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.12609v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.12609v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xudong Shen, Hannah Brown, Jiashu Tao, Martin Strobel, Yao Tong, Akshay Narayan, Harold Soh, Finale Doshi-Velez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is increasing attention being given to how to regulate AI systems. As
governing bodies grapple with what values to encapsulate into regulation, we
consider the technical half of the question: To what extent can AI experts vet
an AI system for adherence to regulatory requirements? We investigate this
question through the lens of two public sector procurement checklists,
identifying what we can do now, what should be possible with technical
innovation, and what requirements need a more interdisciplinary approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>scheduled for publication in the Communications of the ACM, titled
  "Directions of Technical Innovation for Regulatable AI Systems"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMP++: Motion Manifold Primitives with Parametric Curve Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17072v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17072v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonghyeon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion Manifold Primitives (MMP), a manifold-based approach for encoding
basic motion skills, can produce diverse trajectories, enabling the system to
adapt to unseen constraints. Nonetheless, we argue that current MMP models lack
crucial functionalities of movement primitives, such as temporal and via-points
modulation, found in traditional approaches. This shortfall primarily stems
from MMP's reliance on discrete-time trajectories. To overcome these
limitations, we introduce Motion Manifold Primitives++ (MMP++), a new model
that integrates the strengths of both MMP and traditional methods by
incorporating parametric curve representations into the MMP framework.
Furthermore, we identify a significant challenge with MMP++: performance
degradation due to geometric distortions in the latent space, meaning that
similar motions are not closely positioned. To address this, Isometric Motion
Manifold Primitives++ (IMMP++) is proposed to ensure the latent space
accurately preserves the manifold's geometry. Our experimental results across
various applications, including 2-DoF planar motions, 7-DoF robot arm motions,
and SE(3) trajectory planning, show that MMP++ and IMMP++ outperform existing
methods in trajectory generation tasks, achieving substantial improvements in
some cases. Moreover, they enable the modulation of latent coordinates and
via-points, thereby allowing efficient online adaptation to dynamic
environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages. This work has been submitted to the IEEE for possible
  publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regret-Based Defense in Adversarial Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.06912v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.06912v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roman Belaire, Pradeep Varakantham, Thanh Nguyen, David Lo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Reinforcement Learning (DRL) policies have been shown to be vulnerable
to small adversarial noise in observations. Such adversarial noise can have
disastrous consequences in safety-critical environments. For instance, a
self-driving car receiving adversarially perturbed sensory observations about
nearby signs (e.g., a stop sign physically altered to be perceived as a speed
limit sign) or objects (e.g., cars altered to be recognized as trees) can be
fatal. Existing approaches for making RL algorithms robust to an
observation-perturbing adversary have focused on reactive approaches that
iteratively improve against adversarial examples generated at each iteration.
While such approaches have been shown to provide improvements over regular RL
methods, they are reactive and can fare significantly worse if certain
categories of adversarial examples are not generated during training. To that
end, we pursue a more proactive approach that relies on directly optimizing a
well-studied robustness measure, regret instead of expected value. We provide a
principled approach that minimizes maximum regret over a "neighborhood" of
observations to the received "observation". Our regret criterion can be used to
modify existing value- and policy-based Deep RL methods. We demonstrate that
our approaches provide a significant improvement in performance across a wide
variety of benchmarks against leading approaches for robust Deep RL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAMAS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MA4DIV: Multi-Agent Reinforcement Learning for Search Result
  Diversification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17421v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17421v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqun Chen, Jiaxin Mao, Yi Zhang, Dehong Ma, Long Xia, Jun Fan, Daiting Shi, Zhicong Cheng, Simiu Gu, Dawei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The objective of search result diversification (SRD) is to ensure that
selected documents cover as many different subtopics as possible. Existing
methods primarily utilize a paradigm of "greedy selection", i.e., selecting one
document with the highest diversity score at a time. These approaches tend to
be inefficient and are easily trapped in a suboptimal state. In addition, some
other methods aim to approximately optimize the diversity metric, such as
$\alpha$-NDCG, but the results still remain suboptimal. To address these
challenges, we introduce Multi-Agent reinforcement learning (MARL) for search
result DIVersity, which called MA4DIV. In this approach, each document is an
agent and the search result diversification is modeled as a cooperative task
among multiple agents. This approach allows for directly optimizing the
diversity metrics, such as $\alpha$-NDCG, while achieving high training
efficiency. We conducted preliminary experiments on public TREC datasets to
demonstrate the effectiveness and potential of MA4DIV. Considering the limited
number of queries in public TREC datasets, we construct a large-scale dataset
from industry sources and show that MA4DIV achieves substantial improvements in
both effectiveness and efficiency than existing baselines on a industrial scale
dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs Are Few-Shot In-Context Low-Resource Language Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16512v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16512v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Cahyawijaya, Holy Lovenia, Pascale Fung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) empowers large language models (LLMs) to perform
diverse tasks in underrepresented languages using only short in-context
information, offering a crucial avenue for narrowing the gap between
high-resource and low-resource languages. Nonetheless, there is only a handful
of works explored ICL for low-resource languages with most of them focusing on
relatively high-resource languages, such as French and Spanish. In this work,
we extensively study ICL and its cross-lingual variation (X-ICL) on 25
low-resource and 7 relatively higher-resource languages. Our study not only
assesses the effectiveness of ICL with LLMs in low-resource languages but also
identifies the shortcomings of in-context label alignment, and introduces a
more effective alternative: query alignment. Moreover, we provide valuable
insights into various facets of ICL for low-resource languages. Our study
concludes the significance of few-shot in-context information on enhancing the
low-resource understanding quality of LLMs through semantically relevant
information by closing the language gap in the target language and aligning the
semantics between the targeted low-resource and the high-resource language that
the model is proficient in. Our work highlights the importance of advancing ICL
research, particularly for low-resource languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Programming Education with ChatGPT: A Case Study on Student
  Perceptions and Interactions in a Python Course 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15472v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15472v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boxaun Ma, Li Chen, Shin'ichi Konomi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of ChatGPT as a supportive tool in education, notably in
programming courses, addresses the unique challenges of programming education
by providing assistance with debugging, code generation, and explanations.
Despite existing research validating ChatGPT's effectiveness, its application
in university-level programming education and a detailed understanding of
student interactions and perspectives remain limited. This paper explores
ChatGPT's impact on learning in a Python programming course tailored for
first-year students over eight weeks. By analyzing responses from surveys,
open-ended questions, and student-ChatGPT dialog data, we aim to provide a
comprehensive view of ChatGPT's utility and identify both its advantages and
limitations as perceived by students. Our study uncovers a generally positive
reception toward ChatGPT and offers insights into its role in enhancing the
programming education experience. These findings contribute to the broader
discourse on AI's potential in education, suggesting paths for future research
and application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SSM Meets Video Diffusion Models: Efficient Video Generation with
  Structured State Spaces <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07711v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07711v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuta Oshima, Shohei Taniguchi, Masahiro Suzuki, Yutaka Matsuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the remarkable achievements in image generation through diffusion
models, the research community has shown increasing interest in extending these
models to video generation. Recent diffusion models for video generation have
predominantly utilized attention layers to extract temporal features. However,
attention layers are limited by their memory consumption, which increases
quadratically with the length of the sequence. This limitation presents
significant challenges when attempting to generate longer video sequences using
diffusion models. To overcome this challenge, we propose leveraging state-space
models (SSMs). SSMs have recently gained attention as viable alternatives due
to their linear memory consumption relative to sequence length. In the
experiments, we first evaluate our SSM-based model with UCF101, a standard
benchmark of video generation. In addition, to investigate the potential of
SSMs for longer video generation, we perform an experiment using the MineRL
Navigate dataset, varying the number of frames to 64, 200, and 400. In these
settings, our SSM-based model can considerably save memory consumption for
longer sequences, while maintaining competitive FVD scores to the
attention-based models. Our codes are available at
https://github.com/shim0114/SSM-Meets-Video-Diffusion-Models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as workshop paper at ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Weakly Supervised AUC Optimization: A Unified Partial AUC Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14258v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14258v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Xie, Yu Liu, Hao-Yuan He, Ming Li, Zhi-Hua Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since acquiring perfect supervision is usually difficult, real-world machine
learning tasks often confront inaccurate, incomplete, or inexact supervision,
collectively referred to as weak supervision. In this work, we present WSAUC, a
unified framework for weakly supervised AUC optimization problems, which covers
noisy label learning, positive-unlabeled learning, multi-instance learning, and
semi-supervised learning scenarios. Within the WSAUC framework, we first frame
the AUC optimization problems in various weakly supervised scenarios as a
common formulation of minimizing the AUC risk on contaminated sets, and
demonstrate that the empirical risk minimization problems are consistent with
the true AUC. Then, we introduce a new type of partial AUC, specifically, the
reversed partial AUC (rpAUC), which serves as a robust training objective for
AUC maximization in the presence of contaminated labels. WSAUC offers a
universal solution for AUC optimization in various weakly supervised scenarios
by maximizing the empirical rpAUC. Theoretical and experimental results under
multiple settings support the effectiveness of WSAUC on a range of weakly
supervised AUC optimization tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TPAMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate
  Professional and Non-Professional Styled Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09131v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09131v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Zong, Yuyan Chen, Weiming Lu, Jian Shao, Yueting Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated efficacy in various linguistic
applications, including text summarization and controlled text generation.
However, studies into their capacity of switching between styles via
fine-tuning remain underexplored. This study concentrates on textual
professionalism and introduces a novel methodology, named ProSwitch, which
equips a language model with the ability to produce both professional and
non-professional responses through knowledge-guided instruction tuning.
ProSwitch unfolds across three phases: data preparation for gathering domain
knowledge and training corpus; instruction tuning for optimizing language
models with multiple levels of instruction formats; and comprehensive
evaluation for assessing the professionalism discrimination and reference-based
quality of generated text. Comparative analysis of ProSwitch against both
general and specialized language models reveals that our approach outperforms
baselines in switching between professional and non-professional text
generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Re2LLM: Reflective Reinforcement Large Language Model for Session-based
  <span class="highlight-title">Recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16427v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16427v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyan Wang, Yingpeng Du, Zhu Sun, Haoyan Chua, Kaidong Feng, Wenya Wang, Jie Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are emerging as promising approaches to enhance
session-based recommendation (SBR), where both prompt-based and
fine-tuning-based methods have been widely investigated to align LLMs with SBR.
However, the former methods struggle with optimal prompts to elicit the correct
reasoning of LLMs due to the lack of task-specific feedback, leading to
unsatisfactory recommendations. Although the latter methods attempt to
fine-tune LLMs with domain-specific knowledge, they face limitations such as
high computational costs and reliance on open-source backbones. To address such
issues, we propose a Reflective Reinforcement Large Language Model (Re2LLM) for
SBR, guiding LLMs to focus on specialized knowledge essential for more accurate
recommendations effectively and efficiently. In particular, we first design the
Reflective Exploration Module to effectively extract knowledge that is readily
understandable and digestible by LLMs. To be specific, we direct LLMs to
examine recommendation errors through self-reflection and construct a knowledge
base (KB) comprising hints capable of rectifying these errors. To efficiently
elicit the correct reasoning of LLMs, we further devise the Reinforcement
Utilization Module to train a lightweight retrieval agent. It learns to select
hints from the constructed KB based on the task-specific feedback, where the
hints can serve as guidance to help correct LLMs reasoning for better
recommendations. Extensive experiments on multiple real-world datasets
demonstrate that our method consistently outperforms state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dial-MAE: ConTextual Masked Auto-Encoder for Retrieval-based Dialogue
  Systems <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04357v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04357v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenpeng Su, Xing Wu, Wei Zhou, Guangyuan Ma, Songlin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialogue response selection aims to select an appropriate response from
several candidates based on a given user and system utterance history. Most
existing works primarily focus on post-training and fine-tuning tailored for
cross-encoders. However, there are no post-training methods tailored for dense
encoders in dialogue response selection. We argue that when the current
language model, based on dense dialogue systems (such as BERT), is employed as
a dense encoder, it separately encodes dialogue context and response, leading
to a struggle to achieve the alignment of both representations. Thus, we
propose Dial-MAE (Dialogue Contextual Masking Auto-Encoder), a straightforward
yet effective post-training technique tailored for dense encoders in dialogue
response selection. Dial-MAE uses an asymmetric encoder-decoder architecture to
compress the dialogue semantics into dense vectors, which achieves better
alignment between the features of the dialogue context and response. Our
experiments have demonstrated that Dial-MAE is highly effective, achieving
state-of-the-art performance on two commonly evaluated benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SoftTiger: A Clinical Foundation Model for Healthcare Workflows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00868v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00868v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye Chen, Igor Couto, Wei Cai, Cong Fu, Bruno Dorneles
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SoftTiger, a clinical large language model (CLaM) designed as a
foundation model for healthcare workflows. The narrative and unstructured
nature of clinical notes is a major obstacle for healthcare intelligentization.
We address a critical problem of structuring clinical notes into clinical data,
according to international interoperability standards. We collect and annotate
data for three subtasks, namely, international patient summary, clinical
impression and medical encounter. We then supervised fine-tuned a
state-of-the-art LLM using public and credentialed clinical data. The training
is orchestrated in a way that the target model can first support basic clinical
tasks such as abbreviation expansion and temporal information extraction, and
then learn to perform more complex downstream clinical tasks. Moreover, we
address several modeling challenges in the healthcare context, e.g., extra long
context window. Our blind pairwise evaluation shows that SoftTiger outperforms
other popular open-source models and GPT-3.5, comparable to Gemini-pro, with a
mild gap from GPT-4. We believe that LLMs may become a step-stone towards
healthcare digitalization and democratization. Therefore, we publicly release
SoftTiger models at scales of 13 billion and 70 billion parameters, as well as
datasets and code for our innovative scalable evaluation, hopefully, making a
significant contribution to the healthcare industry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probing Multimodal Large Language Models for Global and Local Semantic
  Representations <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17304v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17304v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingxu Tao, Quzhe Huang, Kun Xu, Liwei Chen, Yansong Feng, Dongyan Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancement of Multimodal Large Language Models (MLLMs) has greatly
accelerated the development of applications in understanding integrated texts
and images. Recent works leverage image-caption datasets to train MLLMs,
achieving state-of-the-art performance on image-to-text tasks. However, there
are few studies exploring which layers of MLLMs make the most effort to the
global image information, which plays vital roles in multimodal comprehension
and generation. In this study, we find that the intermediate layers of models
can encode more global semantic information, whose representation vectors
perform better on visual-language entailment tasks, rather than the topmost
layers. We further probe models regarding local semantic representations
through object recognition tasks. We find that the topmost layers may
excessively focus on local information, leading to a diminished ability to
encode global information. Our code and data are released via
https://github.com/kobayashikanna01/probing_MLLM_rep.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024 as a short paper (Camera Ready)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-Distribution and Out-of-Distribution Self-supervised ECG
  Representation Learning for Arrhythmia Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.06427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.06427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sahar Soltanieh, Javad Hashemi, Ali Etemad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a systematic investigation into the effectiveness of
Self-Supervised Learning (SSL) methods for Electrocardiogram (ECG) arrhythmia
detection. We begin by conducting a novel analysis of the data distributions on
three popular ECG-based arrhythmia datasets: PTB-XL, Chapman, and Ribeiro. To
the best of our knowledge, our study is the first to quantitatively explore and
characterize these distributions in the area. We then perform a comprehensive
set of experiments using different augmentations and parameters to evaluate the
effectiveness of various SSL methods, namely SimCRL, BYOL, and SwAV, for ECG
representation learning, where we observe the best performance achieved by
SwAV. Furthermore, our analysis shows that SSL methods achieve highly
competitive results to those achieved by supervised state-of-the-art methods.
To further assess the performance of these methods on both In-Distribution (ID)
and Out-of-Distribution (OOD) ECG data, we conduct cross-dataset training and
testing experiments. Our comprehensive experiments show almost identical
results when comparing ID and OOD schemes, indicating that SSL techniques can
learn highly effective representations that generalize well across different
OOD datasets. This finding can have major implications for ECG-based arrhythmia
detection. Lastly, to further analyze our results, we perform detailed
per-disease studies on the performance of the SSL methods on the three
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been published in the IEEE Journal of Biomedical and
  Health Informatics (JBHI). Copyright IEEE. Please cite as: S. Soltanieh, J.
  Hashemi and A. Etemad, "In-Distribution and Out-of-Distribution
  Self-Supervised ECG Representation Learning for Arrhythmia Detection," in
  IEEE Journal of Biomedical and Health Informatics, vol. 28, no. 2, pp.
  789-800, Feb. 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Imitating Cost-Constrained Behaviors in Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17456v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17456v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Shao, Pradeep Varakantham, Shih-Fen Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Complex planning and scheduling problems have long been solved using various
optimization or heuristic approaches. In recent years, imitation learning that
aims to learn from expert demonstrations has been proposed as a viable
alternative to solving these problems. Generally speaking, imitation learning
is designed to learn either the reward (or preference) model or directly the
behavioral policy by observing the behavior of an expert. Existing work in
imitation learning and inverse reinforcement learning has focused on imitation
primarily in unconstrained settings (e.g., no limit on fuel consumed by the
vehicle). However, in many real-world domains, the behavior of an expert is
governed not only by reward (or preference) but also by constraints. For
instance, decisions on self-driving delivery vehicles are dependent not only on
the route preferences/rewards (depending on past demand data) but also on the
fuel in the vehicle and the time available. In such problems, imitation
learning is challenging as decisions are not only dictated by the reward model
but are also dependent on a cost-constrained model. In this paper, we provide
multiple methods that match expert distributions in the presence of trajectory
cost constraints through (a) Lagrangian-based method; (b) Meta-gradients to
find a good trade-off between expected return and minimizing constraint
violation; and (c) Cost-violation-based alternating gradient. We empirically
show that leading imitation learning approaches imitate cost-constrained
behaviors poorly and our meta-gradient-based approach achieves the best
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 34th International Conference on Automated Planning
  and Scheduling (ICAPS-24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Innovation Paradox: Concept Space Expansion with Diminishing
  Originality and the Promise of Creative AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13300v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13300v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Serhad Sarica, Jianxi Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Innovation, typically spurred by reusing, recombining, and synthesizing
existing concepts, is expected to result in an exponential growth of the
concept space over time. However, our statistical analysis of TechNet, which is
a comprehensive technology semantic network encompassing over four million
concepts derived from patent texts, reveals a linear rather than exponential
expansion of the overall technological concept space. Moreover, there is a
notable decline in the originality of newly created concepts. These trends can
be attributed to the constraints of human cognitive abilities to innovate
beyond an ever-growing space of prior art, among other factors. Integrating
creative artificial intelligence (CAI) into the innovation process holds the
potential to overcome these limitations and alter the observed trends in the
future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Forthcoming on the Design Science</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs in Political Science: Heralding a New Era of Visual Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00154v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00154v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interest is increasing among political scientists in leveraging the extensive
information available in images. However, the challenge of interpreting these
images lies in the need for specialized knowledge in computer vision and access
to specialized hardware. As a result, image analysis has been limited to a
relatively small group within the political science community. This landscape
could potentially change thanks to the rise of large language models (LLMs).
This paper aims to raise awareness of the feasibility of using Gemini for image
content analysis. A retrospective analysis was conducted on a corpus of 688
images. Content reports were elicited from Gemini for each image and then
manually evaluated by the authors. We find that Gemini is highly accurate in
performing object detection, which is arguably the most common and fundamental
task in image analysis for political scientists. Equally important, we show
that it is easy to implement as the entire command consists of a single prompt
in natural language; it is fast to run and should meet the time budget of most
researchers; and it is free to use and does not require any specialized
hardware. In addition, we illustrate how political scientists can leverage
Gemini for other image understanding tasks, including face identification,
sentiment analysis, and caption generation. Our findings suggest that Gemini
and other similar LLMs have the potential to drastically stimulate and
accelerate image research in political science and social sciences more
broadly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language
  Models <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16915v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16915v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atsushi Keyaki, Ribeka Keyaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning in information retrieval systems using pre-trained language
models (PLM-based IR) requires learning query representations and
query-document relations, in addition to downstream task-specific learning.
This study introduces coarse-tuning as an intermediate learning stage that
bridges pre-training and fine-tuning. By learning query representations and
query-document relations in coarse-tuning, we aim to reduce the load of
fine-tuning and improve the learning effect of downstream IR tasks. We propose
Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the
appropriateness of query-document pairs. Evaluation experiments show that the
proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc
document retrieval datasets. Furthermore, the results of the query prediction
task suggested that coarse-tuning facilitated learning of query representation
and query-document relations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Look Before You Leap: Problem Elaboration Prompting Improves
  Mathematical Reasoning in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15764v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15764v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Liao, Jidong Tian, Shaohua Hu, Hao He, Yaohui Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) still grapple with complex tasks like
mathematical reasoning. Despite significant efforts invested in improving
prefix prompts or reasoning process, the crucial role of problem context might
have been neglected. Accurate recognition of inputs is fundamental for solving
mathematical tasks, as ill-formed problems could potentially mislead LLM's
reasoning. In this study, we propose a new approach named Problem Elaboration
Prompting (PEP) to enhance the mathematical capacities of LLMs. Specifically,
PEP decomposes and elucidates the problem context before reasoning, therefore
enhancing the context modeling and parsing efficiency. Experiments across
datasets and models demonstrate promising performances: (1) PEP demonstrates an
overall enhancement in various mathematical tasks. For instance, with the
GPT-3.5 model, PEP exhibits improvements of 9.93% and 8.80% on GSM8k through
greedy decoding and self-consistency, respectively. (2) PEP can be easily
implemented and integrated with other prompting methods. (3) PEP shows
particular strength in handling distraction problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Follower Agnostic Methods for Stackelberg Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01421v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01421v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chinmay Maheshwari, James Cheng, S. Shankar Sasty, Lillian Ratliff, Eric Mazumdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present an efficient algorithm to solve online Stackelberg
games, featuring multiple followers, in a follower-agnostic manner. Unlike
previous works, our approach works even when leader has no knowledge about the
followers' utility functions or strategy space. Our algorithm introduces a
unique gradient estimator, leveraging specially designed strategies to probe
followers. In a departure from traditional assumptions of optimal play, we
model followers' responses using a convergent adaptation rule, allowing for
realistic and dynamic interactions. The leader constructs the gradient
estimator solely based on observations of followers' actions. We provide both
non-asymptotic convergence rates to stationary points of the leader's objective
and demonstrate asymptotic convergence to a \emph{local Stackelberg
equilibrium}. To validate the effectiveness of our algorithm, we use this
algorithm to solve the problem of incentive design on a large-scale
transportation network, showcasing its robustness even when the leader lacks
access to followers' demand.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Act without Actions <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10812v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10812v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Schmidt, Minqi Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training large models on vast amounts of web data has proven to be an
effective approach for obtaining powerful, general models in domains such as
language and vision. However, this paradigm has not yet taken hold in
reinforcement learning. This is because videos, the most abundant form of
embodied behavioral data on the web, lack the action labels required by
existing methods for imitating behavior from demonstrations. We introduce
Latent Action Policies (LAPO), a method for recovering latent action
information, and thereby latent-action policies, world models, and inverse
dynamics models, purely from videos. LAPO is the first method able to recover
the structure of the true action space just from observed dynamics, even in
challenging procedurally-generated environments. LAPO enables training
latent-action policies that can be rapidly fine-tuned into expert-level
policies, either offline using a small action-labeled dataset, or online with
rewards. LAPO takes a first step towards pre-training powerful, generalist
policies and world models on the vast amounts of videos readily available on
the web.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2024 (spotlight). The code can be found at
  http://github.com/schmidtdominik/LAPO</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth
  Estimation <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suraj Patni, Aradhye Agarwal, Chetan Arora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the absence of parallax cues, a learning-based single image depth
estimation (SIDE) model relies heavily on shading and contextual cues in the
image. While this simplicity is attractive, it is necessary to train such
models on large and varied datasets, which are difficult to capture. It has
been shown that using embeddings from pre-trained foundational models, such as
CLIP, improves zero shot transfer in several applications. Taking inspiration
from this, in our paper we explore the use of global image priors generated
from a pre-trained ViT model to provide more detailed contextual information.
We argue that the embedding vector from a ViT model, pre-trained on a large
dataset, captures greater relevant information for SIDE than the usual route of
generating pseudo image captions, followed by CLIP based text embeddings. Based
on this idea, we propose a new SIDE model using a diffusion backbone which is
conditioned on ViT embeddings. Our proposed design establishes a new
state-of-the-art (SOTA) for SIDE on NYUv2 dataset, achieving Abs Rel error of
0.059(14% improvement) compared to 0.069 by the current SOTA (VPD). And on
KITTI dataset, achieving Sq Rel error of 0.139 (2% improvement) compared to
0.142 by the current SOTA (GEDepth). For zero-shot transfer with a model
trained on NYUv2, we report mean relative improvement of (20%, 23%, 81%, 25%)
over NeWCRFs on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%,
18%, 45%, 9%) by ZoeDepth. The code is available at
https://github.com/Aradhye2002/EcoDepth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-form factuality in large language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, Quoc V. Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often generate content that contains factual
errors when responding to fact-seeking prompts on open-ended topics. To
benchmark a model's long-form factuality in open domains, we first use GPT-4 to
generate LongFact, a prompt set comprising thousands of questions spanning 38
topics. We then propose that LLM agents can be used as automated evaluators for
long-form factuality through a method which we call Search-Augmented Factuality
Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into
a set of individual facts and to evaluate the accuracy of each fact using a
multi-step reasoning process comprising sending search queries to Google Search
and determining whether a fact is supported by the search results. Furthermore,
we propose extending F1 score as an aggregated metric for long-form factuality.
To do so, we balance the percentage of supported facts in a response
(precision) with the percentage of provided facts relative to a hyperparameter
representing a user's preferred response length (recall).
  Empirically, we demonstrate that LLM agents can achieve superhuman rating
performance - on a set of ~16k individual facts, SAFE agrees with crowdsourced
human annotators 72% of the time, and on a random subset of 100 disagreement
cases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times
cheaper than human annotators. We also benchmark thirteen language models on
LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding
that larger language models generally achieve better long-form factuality.
LongFact, SAFE, and all experimental code are available at
https://github.com/google-deepmind/long-form-factuality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ImageNet-D: <span class="highlight-title">Benchmark</span>ing Neural Network Robustness on Diffusion
  Synthetic Object <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenshuang Zhang, Fei Pan, Junmo Kim, In So Kweon, Chengzhi Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We establish rigorous benchmarks for visual perception robustness. Synthetic
images such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific
type of evaluation over synthetic corruptions, backgrounds, and textures, yet
those robustness benchmarks are restricted in specified variations and have low
synthetic quality. In this work, we introduce generative model as a data source
for synthesizing hard images that benchmark deep models' robustness. Leveraging
diffusion models, we are able to generate images with more diversified
backgrounds, textures, and materials than any prior work, where we term this
benchmark as ImageNet-D. Experimental results show that ImageNet-D results in a
significant accuracy drop to a range of vision models, from the standard ResNet
visual classifier to the latest foundation models like CLIP and MiniGPT-4,
significantly reducing their accuracy by up to 60\%. Our work suggests that
diffusion models can be an effective source to test vision models. The code and
dataset are available at https://github.com/chenshuang-zhang/imagenet_d.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Superior Parallel Big Data Clustering through Competitive Stochastic
  Sample Size Optimization in Big-means 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rustam Mussabayev, Ravil Mussabayev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel K-means clustering algorithm, an advancement on
the conventional Big-means methodology. The proposed method efficiently
integrates parallel processing, stochastic sampling, and competitive
optimization to create a scalable variant designed for big data applications.
It addresses scalability and computation time challenges typically faced with
traditional techniques. The algorithm adjusts sample sizes dynamically for each
worker during execution, optimizing performance. Data from these sample sizes
are continually analyzed, facilitating the identification of the most efficient
configuration. By incorporating a competitive element among workers using
different sample sizes, efficiency within the Big-means algorithm is further
stimulated. In essence, the algorithm balances computational time and
clustering quality by employing a stochastic, competitive sampling strategy in
a parallel computing setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CaT: Constraints as Terminations for Legged Locomotion Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elliot Chane-Sane, Pierre-Alexandre Leziart, Thomas Flayols, Olivier Stasse, Philippe Souères, Nicolas Mansard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Reinforcement Learning (RL) has demonstrated impressive results in
solving complex robotic tasks such as quadruped locomotion. Yet, current
solvers fail to produce efficient policies respecting hard constraints. In this
work, we advocate for integrating constraints into robot learning and present
Constraints as Terminations (CaT), a novel constrained RL algorithm. Departing
from classical constrained RL formulations, we reformulate constraints through
stochastic terminations during policy learning: any violation of a constraint
triggers a probability of terminating potential future rewards the RL agent
could attain. We propose an algorithmic approach to this formulation, by
minimally modifying widely used off-the-shelf RL algorithms in robot learning
(such as Proximal Policy Optimization). Our approach leads to excellent
constraint adherence without introducing undue complexity and computational
overhead, thus mitigating barriers to broader adoption. Through empirical
evaluation on the real quadruped robot Solo crossing challenging obstacles, we
demonstrate that CaT provides a compelling solution for incorporating
constraints into RL frameworks. Videos and code are available at
https://constraints-as-terminations.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project webpage: https://constraints-as-terminations.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detection of subclinical atherosclerosis by image-based deep learning on
  chest x-ray 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guglielmo Gallone, Francesco Iodice, Alberto Presta, Davide Tore, Ovidio de Filippo, Michele Visciano, Carlo Alberto Barbano, Alessandro Serafini, Paola Gorrini, Alessandro Bruno, Walter Grosso Marra, James Hughes, Mario Iannaccone, Paolo Fonio, Attilio Fiandrotti, Alessandro Depaoli, Marco Grangetto, Gaetano Maria de Ferrari, Fabrizio D'Ascenzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aims. To develop a deep-learning based system for recognition of subclinical
atherosclerosis on a plain frontal chest x-ray. Methods and Results. A
deep-learning algorithm to predict coronary artery calcium (CAC) score (the
AI-CAC model) was developed on 460 chest x-ray (80% training cohort, 20%
internal validation cohort) of primary prevention patients (58.4% male, median
age 63 [51-74] years) with available paired chest x-ray and chest computed
tomography (CT) indicated for any clinical reason and performed within 3
months. The CAC score calculated on chest CT was used as ground truth. The
model was validated on an temporally-independent cohort of 90 patients from the
same institution (external validation). The diagnostic accuracy of the AI-CAC
model assessed by the area under the curve (AUC) was the primary outcome.
Overall, median AI-CAC score was 35 (0-388) and 28.9% patients had no AI-CAC.
AUC of the AI-CAC model to identify a CAC>0 was 0.90 in the internal validation
cohort and 0.77 in the external validation cohort. Sensitivity was consistently
above 92% in both cohorts. In the overall cohort (n=540), among patients with
AI-CAC=0, a single ASCVD event occurred, after 4.3 years. Patients with
AI-CAC>0 had significantly higher Kaplan Meier estimates for ASCVD events
(13.5% vs. 3.4%, log-rank=0.013). Conclusion. The AI-CAC model seems to
accurately detect subclinical atherosclerosis on chest x-ray with elevated
sensitivity, and to predict ASCVD events with elevated negative predictive
value. Adoption of the AI-CAC model to refine CV risk stratification or as an
opportunistic screening tool requires prospective evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to European Heart Journal - Cardiovascular Imaging Added
  also the additional material 44 pages (30 main paper, 14 additional
  material), 14 figures (5 main manuscript, 9 additional material)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding the Learning Dynamics of Alignment with Human Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shawn Im, Yixuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning large language models (LLMs) with human intentions has become a
critical task for safely deploying models in real-world systems. While existing
alignment approaches have seen empirical success, theoretically understanding
how these methods affect model behavior remains an open question. Our work
provides an initial attempt to theoretically analyze the learning dynamics of
human preference alignment. We formally show how the distribution of preference
datasets influences the rate of model updates and provide rigorous guarantees
on the training accuracy. Our theory also reveals an intricate phenomenon where
the optimization is prone to prioritizing certain behaviors with higher
preference distinguishability. We empirically validate our findings on
contemporary LLMs and alignment tasks, reinforcing our theoretical insights and
shedding light on considerations for future alignment approaches. Disclaimer:
This paper contains potentially offensive text; reader discretion is advised.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Usage-Specific Survival Modeling Based on Operational Data and Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olov Holmer, Mattias Krysander, Erik Frisk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate predictions of when a component will fail are crucial when planning
maintenance, and by modeling the distribution of these failure times, survival
models have shown to be particularly useful in this context. The presented
methodology is based on conventional neural network-based survival models that
are trained using data that is continuously gathered and stored at specific
times, called snapshots. An important property of this type of training data is
that it can contain more than one snapshot from a specific individual which
results in that standard maximum likelihood training can not be directly
applied since the data is not independent. However, the papers show that if the
data is in a specific format where all snapshot times are the same for all
individuals, called homogeneously sampled, maximum likelihood training can be
applied and produce desirable results. In many cases, the data is not
homogeneously sampled and in this case, it is proposed to resample the data to
make it homogeneously sampled. How densely the dataset is sampled turns out to
be an important parameter; it should be chosen large enough to produce good
results, but this also increases the size of the dataset which makes training
slow. To reduce the number of samples needed during training, the paper also
proposes a technique to, instead of resampling the dataset once before the
training starts, randomly resample the dataset at the start of each epoch
during the training. The proposed methodology is evaluated on both a simulated
dataset and an experimental dataset of starter battery failures. The results
show that if the data is homogeneously sampled the methodology works as
intended and produces accurate survival models. The results also show that
randomly resampling the dataset on each epoch is an effective way to reduce the
size of the training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nonlinear model reduction for operator learning <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamidreza Eivazi, Stefan Wittek, Andreas Rausch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Operator learning provides methods to approximate mappings between
infinite-dimensional function spaces. Deep operator networks (DeepONets) are a
notable architecture in this field. Recently, an extension of DeepONet based on
model reduction and neural networks, proper orthogonal decomposition
(POD)-DeepONet, has been able to outperform other architectures in terms of
accuracy for several benchmark tests. We extend this idea towards nonlinear
model order reduction by proposing an efficient framework that combines neural
networks with kernel principal component analysis (KPCA) for operator learning.
Our results demonstrate the superior performance of KPCA-DeepONet over
POD-DeepONet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a Tiny Paper at ICLR 2024 (Notable)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Manufacturing Quality <span class="highlight-title">Predict</span>ion Models through the
  Integration of Explainability Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Gross, Helge Spieker, Arnaud Gotlieb, Ricardo Knoblauch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research presents a method that utilizes explainability techniques to
amplify the performance of machine learning (ML) models in forecasting the
quality of milling processes, as demonstrated in this paper through a
manufacturing use case. The methodology entails the initial training of ML
models, followed by a fine-tuning phase where irrelevant features identified
through explainability methods are eliminated. This procedural refinement
results in performance enhancements, paving the way for potential reductions in
manufacturing costs and a better understanding of the trained ML models. This
study highlights the usefulness of explainability techniques in both explaining
and optimizing predictive models in the manufacturing realm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-Supervised Learning for Deep Causal Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasin Ibrahim, Hermione Warr, Konstantinos Kamnitsas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing models that can answer questions of the form "How would $x$ change
if $y$ had been $z$?" is fundamental for advancing medical image analysis.
Training causal generative models that address such counterfactual questions,
though, currently requires that all relevant variables have been observed and
that corresponding labels are available in training data. However, clinical
data may not have complete records for all patients and state of the art causal
generative models are unable to take full advantage of this. We thus develop,
for the first time, a semi-supervised deep causal generative model that
exploits the causal relationships between variables to maximise the use of all
available data. We explore this in the setting where each sample is either
fully labelled or fully unlabelled, as well as the more clinically realistic
case of having different labels missing for each sample. We leverage techniques
from causal inference to infer missing values and subsequently generate
realistic counterfactuals, even for samples with incomplete labels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning for Traffic Flow <span class="highlight-title">Predict</span>ion using Cellular Automata-based
  Model and CNN-LSTM architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaohui Yang, Kshitij Jerath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have attempted to use deep learning to predict future states of
traffic flow, but have met with mixed results. These approaches face two key
challenges. First, training deep learning neural networks requires large
amounts of training data which are not yet easily available for traffic flow
systems. Second, even when data is available, the neural networks require
access to historical data that covers most possible traffic flow dynamics to
successfully predict future traffic states. Specifically, these deep learning
approaches do not fully leverage domain-knowledge about traffic flow dynamics,
despite a significant existing knowledge-base. In this work, we propose to
solve both issues using a Convolutional Neural Network (CNNs) with Long Short
Term Memory (LSTM) deep learning architecture to successfully predict traffic
flow, while leveraging a cellular automata-based statistical mechanics model of
traffic flow to generate training and test data. Another major contribution of
this paper is the insight that training data for a large traffic system can
actually be sampled from the simulations of a much smaller traffic system. This
is achieved through observing that the normalized energy distribution of the
statistical mechanics model is scale invariant, which significantly eases the
burden of data generation for large scale traffic systems. The resulting
simulations indicate good agreement between the predicted and the true traffic
flow dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conditional Wasserstein Distances with Applications in Bayesian OT Flow
  Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jannis Chemseddine, Paul Hagemann, Christian Wald, Gabriele Steidl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In inverse problems, many conditional generative models approximate the
posterior measure by minimizing a distance between the joint measure and its
learned approximation. While this approach also controls the distance between
the posterior measures in the case of the Kullback--Leibler divergence, this is
in general not hold true for the Wasserstein distance. In this paper, we
introduce a conditional Wasserstein distance via a set of restricted couplings
that equals the expected Wasserstein distance of the posteriors. Interestingly,
the dual formulation of the conditional Wasserstein-1 flow resembles losses in
the conditional Wasserstein GAN literature in a quite natural way. We derive
theoretical properties of the conditional Wasserstein distance, characterize
the corresponding geodesics and velocity fields as well as the flow ODEs.
Subsequently, we propose to approximate the velocity fields by relaxing the
conditional Wasserstein distance. Based on this, we propose an extension of OT
Flow Matching for solving Bayesian inverse problems and demonstrate its
numerical advantages on an inverse problem and class-conditional image
generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper supersedes arXiv:2310.13433</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fpga-Based Neural Thrust Controller for UAVs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18703v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18703v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sharif Azem, David Scheunert, Mengguang Li, Jonas Gehrunger, Kai Cui, Christian Hochberger, Heinz Koepp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of unmanned aerial vehicles (UAVs) has improved a variety of
fields by providing a versatile, cost-effective and accessible platform for
implementing state-of-the-art algorithms. To accomplish a broader range of
tasks, there is a growing need for enhanced on-board computing to cope with
increasing complexity and dynamic environmental conditions. Recent advances
have seen the application of Deep Neural Networks (DNNs), particularly in
combination with Reinforcement Learning (RL), to improve the adaptability and
performance of UAVs, especially in unknown environments. However, the
computational requirements of DNNs pose a challenge to the limited computing
resources available on many UAVs. This work explores the use of Field
Programmable Gate Arrays (FPGAs) as a viable solution to this challenge,
offering flexibility, high performance, energy and time efficiency. We propose
a novel hardware board equipped with an Artix-7 FPGA for a popular open-source
micro-UAV platform. We successfully validate its functionality by implementing
an RL-based low-level controller using real-world experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contrastive Learning with Orthonormal Anchors (CLOA) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanran Li, Daniel Pimentel-Alarcón
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study focuses on addressing the instability issues prevalent in
contrastive learning, specifically examining the InfoNCE loss function and its
derivatives. We reveal a critical observation that these loss functions exhibit
a restrictive behavior, leading to a convergence phenomenon where embeddings
tend to merge into a singular point. This "over-fusion" effect detrimentally
affects classification accuracy in subsequent supervised-learning tasks.
Through theoretical analysis, we demonstrate that embeddings, when equalized or
confined to a rank-1 linear subspace, represent a local minimum for InfoNCE. In
response to this challenge, our research introduces an innovative strategy that
leverages the same or fewer labeled data than typically used in the fine-tuning
phase. The loss we proposed, Orthonormal Anchor Regression Loss, is designed to
disentangle embedding clusters, significantly enhancing the distinctiveness of
each embedding while simultaneously ensuring their aggregation into dense,
well-defined clusters. Our method demonstrates remarkable improvements with
just a fraction of the conventional label requirements, as evidenced by our
results on CIFAR10 and CIFAR100 datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InceptionTime vs. Wavelet -- A comparison for <span class="highlight-title">time series</span> classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Klenkert, Daniel Schaeffer, Julian Stauch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks were used to classify infrasound data. Two different
approaches were compared. One based on the direct classification of time series
data, using a custom implementation of the InceptionTime network. For the other
approach, we generated 2D images of the wavelet transformation of the signals,
which were subsequently classified using a ResNet implementation. Choosing
appropriate hyperparameter settings, both achieve a classification accuracy of
above 90 %, with the direct approach reaching 95.2 %.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Representatividad Muestral en la Incertidumbre Simétrica Multivariada
  para la Selección de Atributos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18685v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18685v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gustavo Sosa-Cabrera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we analyze the behavior of the multivariate symmetric
uncertainty (MSU) measure through the use of statistical simulation techniques
under various mixes of informative and non-informative randomly generated
features. Experiments show how the number of attributes, their cardinalities,
and the sample size affect the MSU. In this thesis, through observation of
results, it is proposed an heuristic condition that preserves good quality in
the MSU under different combinations of these three factors, providing a new
useful criterion to help drive the process of dimension reduction.
  --
  En el presente trabajo hemos analizado el comportamiento de una versi\'on
multivariada de la incertidumbre sim\'etrica a trav\'es de t\'ecnicas de
simulaci\'on estad\'isticas sobre varias combinaciones de atributos
informativos y no-informativos generados de forma aleatoria. Los experimentos
muestran como el n\'umero de atributos, sus cardinalidades y el tama\~no
muestral afectan al MSU como medida. En esta tesis, mediante la observaci\'on
de resultados hemos propuesto una condici\'on que preserva una buena calidad en
el MSU bajo diferentes combinaciones de los tres factores mencionados, lo cual
provee un nuevo y valioso criterio para llevar a cabo el proceso de reducci\'on
de dimensionalidad.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>52 pages, in Spanish. Advisors: Miguel Garc\'ia-Torres, Santiago
  G\'omez-Guerrero, Christian E. Schaerer Serra</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TransFusion: Contrastive Learning with Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanran Li, Daniel Pimentel-Alarcón
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel framework, TransFusion, designed to make the
process of contrastive learning more analytical and explainable. TransFusion
consists of attention blocks whose softmax being replaced by ReLU, and its
final block's weighted-sum operation is truncated to leave the adjacency matrix
as the output. The model is trained by minimizing the Jensen-Shannon Divergence
between its output and the target affinity matrix, which indicates whether each
pair of samples belongs to the same or different classes. The main contribution
of TransFusion lies in defining a theoretical limit for answering two
fundamental questions in the field: the maximum level of data augmentation and
the minimum batch size required for effective contrastive learning.
Furthermore, experimental results indicate that TransFusion successfully
extracts features that isolate clusters from complex real-world data, leading
to improved classification accuracy in downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 4 figures,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NL-ITI: Optimizing Probing and Intervention for Improvement of ITI
  Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakub Hoscilowicz, Adam Wiacek, Jan Chojnacki, Adam Cieslak, Leszek Michon, Vitalii Urbanevych, Artur Janicki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLM) are prone to returning false information. It
constitutes one of major challenges in the AI field. In our work, we explore
paradigm introduced by Inference-Time-Intervention (ITI). In first stage, it
identifies attention heads, which contain the highest amount of desired type of
knowledge (e.g., truthful). Afterwards, during inference, LLM activations are
shifted for chosen subset of attention heads. We further improved the ITI
framework by introducing a nonlinear probing and multi-token intervention -
Non-Linear ITI (NL-ITI). NL-ITI is tested on diverse multiple-choice
benchmarks, including TruthfulQA, on which we report around 14% MC1 metric
improvement with respect to the baseline ITI results. NL-ITI achieves also
encouraging results on other testsets - on Business Ethics subdomain of MMLU,
around 18% MC1 improvement over baseline LLaMA2-7B. Additionally, NL-ITI
performs better while being less invasive in the behavior of LLM at the same
time (as measured by Kullback-Leibler divergence).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/Samsung/NL-ITI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fact Checking Beyond Training Set <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Payam Karisani, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating the veracity of everyday claims is time consuming and in some
cases requires domain expertise. We empirically demonstrate that the commonly
used fact checking pipeline, known as the retriever-reader, suffers from
performance deterioration when it is trained on the labeled data from one
domain and used in another domain. Afterwards, we delve into each component of
the pipeline and propose novel algorithms to address this problem. We propose
an adversarial algorithm to make the retriever component robust against
distribution shift. Our core idea is to initially train a bi-encoder on the
labeled source data, and then, to adversarially train two separate document and
claim encoders using unlabeled target data. We then focus on the reader
component and propose to train it such that it is insensitive towards the order
of claims and evidence documents. Our empirical evaluations support the
hypothesis that such a reader shows a higher robustness against distribution
shift. To our knowledge, there is no publicly available multi-topic fact
checking dataset. Thus, we propose a simple automatic method to re-purpose two
well-known fact checking datasets. We then construct eight fact checking
scenarios from these datasets, and compare our model to a set of strong
baseline models, including recent domain adaptation models that use GPT4 for
generating synthetic data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aiming for Relevance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bar Eini Porat, Danny Eytan, Uri Shalit
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vital signs are crucial in intensive care units (ICUs). They are used to
track the patient's state and to identify clinically significant changes.
Predicting vital sign trajectories is valuable for early detection of adverse
events. However, conventional machine learning metrics like RMSE often fail to
capture the true clinical relevance of such predictions. We introduce novel
vital sign prediction performance metrics that align with clinical contexts,
focusing on deviations from clinical norms, overall trends, and trend
deviations. These metrics are derived from empirical utility curves obtained in
a previous study through interviews with ICU clinicians. We validate the
metrics' usefulness using simulated and real clinical datasets (MIMIC and
eICU). Furthermore, we employ these metrics as loss functions for neural
networks, resulting in models that excel in predicting clinically significant
events. This research paves the way for clinically relevant machine learning
model evaluation and optimization, promising to improve ICU patient care. 10
pages, 9 figures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures, AMIA Informatics 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Network-Based Piecewise Survival Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olov Holmer, Erik Frisk, Mattias Krysander
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, a family of neural network-based survival models is presented.
The models are specified based on piecewise definitions of the hazard function
and the density function on a partitioning of the time; both constant and
linear piecewise definitions are presented, resulting in a family of four
models. The models can be seen as an extension of the commonly used
discrete-time and piecewise exponential models and thereby add flexibility to
this set of standard models. Using a simulated dataset the models are shown to
perform well compared to the highly expressive, state-of-the-art energy-based
model, while only requiring a fraction of the computation time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transformers-based architectures for stroke segmentation: A review 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yalda Zafari-Ghadim, Essam A. Rashed, Mohamed Mabrok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stroke remains a significant global health concern, necessitating precise and
efficient diagnostic tools for timely intervention and improved patient
outcomes. The emergence of deep learning methodologies has transformed the
landscape of medical image analysis. Recently, Transformers, initially designed
for natural language processing, have exhibited remarkable capabilities in
various computer vision applications, including medical image analysis. This
comprehensive review aims to provide an in-depth exploration of the
cutting-edge Transformer-based architectures applied in the context of stroke
segmentation. It commences with an exploration of stroke pathology, imaging
modalities, and the challenges associated with accurate diagnosis and
segmentation. Subsequently, the review delves into the fundamental ideas of
Transformers, offering detailed insights into their architectural intricacies
and the underlying mechanisms that empower them to effectively capture complex
spatial information within medical images. The existing literature is
systematically categorized and analyzed, discussing various approaches that
leverage Transformers for stroke segmentation. A critical assessment is
provided, highlighting the strengths and limitations of these methods,
including considerations of performance and computational efficiency.
Additionally, this review explores potential avenues for future research and
development
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fusion approaches for emotion recognition from speech using acoustic and
  text-based features <span class="chip">ICASSP 2020</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo Pepino, Pablo Riera, Luciana Ferrer, Agustin Gravano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study different approaches for classifying emotions from
speech using acoustic and text-based features. We propose to obtain
contextualized word embeddings with BERT to represent the information contained
in speech transcriptions and show that this results in better performance than
using Glove embeddings. We also propose and compare different strategies to
combine the audio and text modalities, evaluating them on IEMOCAP and
MSP-PODCAST datasets. We find that fusing acoustic and text-based systems is
beneficial on both datasets, though only subtle differences are observed across
the evaluated fusion approaches. Finally, for IEMOCAP, we show the large effect
that the criteria used to define the cross-validation folds have on results. In
particular, the standard way of creating folds for this dataset results in a
highly optimistic estimation of performance for the text-based system,
suggesting that some previous works may overestimate the advantage of
incorporating transcriptions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages. Accepted in ICASSP 2020</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ First Experiences with the Identification of People at Risk for Diabetes
  in Argentina using Machine Learning Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enzo Rucci, Gonzalo Tittarelli, Franco Ronchetti, Jorge F. Elgart, Laura Lanzarini, Juan José Gagliardino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting Type 2 Diabetes (T2D) and Prediabetes (PD) is a real challenge for
medicine due to the absence of pathogenic symptoms and the lack of known
associated risk factors. Even though some proposals for machine learning models
enable the identification of people at risk, the nature of the condition makes
it so that a model suitable for one population may not necessarily be suitable
for another. In this article, the development and assessment of predictive
models to identify people at risk for T2D and PD specifically in Argentina are
discussed. First, the database was thoroughly preprocessed and three specific
datasets were generated considering a compromise between the number of records
and the amount of available variables. After applying 5 different
classification models, the results obtained show that a very good performance
was observed for two datasets with some of these models. In particular, RF, DT,
and ANN demonstrated great classification power, with good values for the
metrics under consideration. Given the lack of this type of tool in Argentina,
this work represents the first step towards the development of more
sophisticated models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Computer Science - CACIC 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Lipschitz Estimation for CNNs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuf Sulehman, Tingting Mu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating the Lipschitz constant of deep neural networks is of growing
interest as it is useful for informing on generalisability and adversarial
robustness. Convolutional neural networks (CNNs) in particular, underpin much
of the recent success in computer vision related applications. However,
although existing methods for estimating the Lipschitz constant can be tight,
they have limited scalability when applied to CNNs. To tackle this, we propose
a novel method to accelerate Lipschitz constant estimation for CNNs. The core
idea is to divide a large convolutional block via a joint layer and width-wise
partition, into a collection of smaller blocks. We prove an upper-bound on the
Lipschitz constant of the larger block in terms of the Lipschitz constants of
the smaller blocks. Through varying the partition factor, the resulting method
can be adjusted to prioritise either accuracy or scalability and permits
parallelisation. We demonstrate an enhanced scalability and comparable accuracy
to existing baselines through a range of experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Heterogeneous Peridynamic Neural Operators: Discover Biotissue
  Constitutive Law and Microstructure From Digital Image Correlation
  Measurements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siavash Jafarzadeh, Stewart Silling, Lu Zhang, Colton Ross, Chung-Hao Lee, S. M. Rakibur Rahman, Shuodao Wang, Yue Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human tissues are highly organized structures with specific collagen fiber
arrangements varying from point to point. The effects of such heterogeneity
play an important role for tissue function, and hence it is of critical to
discover and understand the distribution of such fiber orientations from
experimental measurements, such as the digital image correlation data. To this
end, we introduce the heterogeneous peridynamic neural operator (HeteroPNO)
approach, for data-driven constitutive modeling of heterogeneous anisotropic
materials. The goal is to learn both a nonlocal constitutive law together with
the material microstructure, in the form of a heterogeneous fiber orientation
field, from loading field-displacement field measurements. To this end, we
propose a two-phase learning approach. Firstly, we learn a homogeneous
constitutive law in the form of a neural network-based kernel function and a
nonlocal bond force, to capture complex homogeneous material responses from
data. Then, in the second phase we reinitialize the learnt bond force and the
kernel function, and training them together with a fiber orientation field for
each material point. Owing to the state-based peridynamic skeleton, our
HeteroPNO-learned material models are objective and have the balance of linear
and angular momentum guaranteed. Moreover, the effects from heterogeneity and
nonlinear constitutive relationship are captured by the kernel function and the
bond force respectively, enabling physical interpretability. As a result, our
HeteroPNO architecture can learn a constitutive model for a biological tissue
with anisotropic heterogeneous response undergoing large deformation regime.
Moreover, the framework is capable to provide displacement and stress field
predictions for new and unseen loading instances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Impact of Uniform Inputs on Activation Sparsity and Energy-Latency
  Attacks in Computer Vision <span class="chip">SP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Müller, Erwin Quiring
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Resource efficiency plays an important role for machine learning nowadays.
The energy and decision latency are two critical aspects to ensure a
sustainable and practical application. Unfortunately, the energy consumption
and decision latency are not robust against adversaries. Researchers have
recently demonstrated that attackers can compute and submit so-called sponge
examples at inference time to increase the energy consumption and decision
latency of neural networks. In computer vision, the proposed strategy crafts
inputs with less activation sparsity which could otherwise be used to
accelerate the computation. In this paper, we analyze the mechanism how these
energy-latency attacks reduce activation sparsity. In particular, we find that
input uniformity is a key enabler. A uniform image, that is, an image with
mostly flat, uniformly colored surfaces, triggers more activations due to a
specific interplay of convolution, batch normalization, and ReLU activation.
Based on these insights, we propose two new simple, yet effective strategies
for crafting sponge examples: sampling images from a probability distribution
and identifying dense, yet inconspicuous inputs in natural datasets. We
empirically examine our findings in a comprehensive evaluation with multiple
image classification models and show that our attack achieves the same sparsity
effect as prior sponge-example methods, but at a fraction of computation
effort. We also show that our sponge examples transfer between different neural
networks. Finally, we discuss applications of our findings for the good by
improving efficiency by increasing sparsity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the DLSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One flow to correct them all: improving simulations in high-energy
  physics with a single normalising flow and a switch 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caio Cesar Daumann, Mauro Donega, Johannes Erdmann, Massimiliano Galli, Jan Lukas Späh, Davide Valsecchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulated events are key ingredients in almost all high-energy physics
analyses. However, imperfections in the simulation can lead to sizeable
differences between the observed data and simulated events. The effects of such
mismodelling on relevant observables must be corrected either effectively via
scale factors, with weights or by modifying the distributions of the
observables and their correlations. We introduce a correction method that
transforms one multidimensional distribution (simulation) into another one
(data) using a simple architecture based on a single normalising flow with a
boolean condition. We demonstrate the effectiveness of the method on a
physics-inspired toy dataset with non-trivial mismodelling of several
observables and their correlations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Optimizing Hyperparameters for Quantum Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sabrina Herbst, Vincenzo De Maio, Ivona Brandic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing capabilities of Machine Learning (ML) models go hand in hand
with an immense amount of data and computational power required for training.
Therefore, training is usually outsourced into HPC facilities, where we have
started to experience limits in scaling conventional HPC hardware, as theorized
by Moore's law. Despite heavy parallelization and optimization efforts, current
state-of-the-art ML models require weeks for training, which is associated with
an enormous $CO_2$ footprint. Quantum Computing, and specifically Quantum
Machine Learning (QML), can offer significant theoretical speed-ups and
enhanced expressive power. However, training QML models requires tuning various
hyperparameters, which is a nontrivial task and suboptimal choices can highly
affect the trainability and performance of the models. In this study, we
identify the most impactful hyperparameters and collect data about the
performance of QML models. We compare different configurations and provide
researchers with performance data and concrete suggestions for hyperparameter
selection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SteinGen: Generating Fidelitous and Diverse Graph Samples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gesine Reinert, Wenkai Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating graphs that preserve characteristic structures while promoting
sample diversity can be challenging, especially when the number of graph
observations is small. Here, we tackle the problem of graph generation from
only one observed graph. The classical approach of graph generation from
parametric models relies on the estimation of parameters, which can be
inconsistent or expensive to compute due to intractable normalisation
constants. Generative modelling based on machine learning techniques to
generate high-quality graph samples avoids parameter estimation but usually
requires abundant training samples. Our proposed generating procedure,
SteinGen, which is phrased in the setting of graphs as realisations of
exponential random graph models, combines ideas from Stein's method and MCMC by
employing Markovian dynamics which are based on a Stein operator for the target
model. SteinGen uses the Glauber dynamics associated with an estimated Stein
operator to generate a sample, and re-estimates the Stein operator from the
sample after every sampling step. We show that on a class of exponential random
graph models this novel "estimation and re-estimation" generation strategy
yields high distributional similarity (high fidelity) to the original data,
combined with high sample diversity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-Informed <span class="highlight-title">Graph Neural Network</span>s for Water Distribution Systems <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inaam Ashraf, Janine Strotherm, Luca Hermes, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Water distribution systems (WDS) are an integral part of critical
infrastructure which is pivotal to urban development. As 70% of the world's
population will likely live in urban environments in 2050, efficient simulation
and planning tools for WDS play a crucial role in reaching UN's sustainable
developmental goal (SDG) 6 - "Clean water and sanitation for all". In this
realm, we propose a novel and efficient machine learning emulator, more
precisely, a physics-informed deep learning (DL) model, for hydraulic state
estimation in WDS. Using a recursive approach, our model only needs a few graph
convolutional neural network (GCN) layers and employs an innovative algorithm
based on message passing. Unlike conventional machine learning tasks, the model
uses hydraulic principles to infer two additional hydraulic state features in
the process of reconstructing the available ground truth feature in an
unsupervised manner. To the best of our knowledge, this is the first DL
approach to emulate the popular hydraulic simulator EPANET, utilizing no
additional information. Like most DL models and unlike the hydraulic simulator,
our model demonstrates vastly faster emulation times that do not increase
drastically with the size of the WDS. Moreover, we achieve high accuracy on the
ground truth and very similar results compared to the hydraulic simulator as
demonstrated through experiments on five real-world WDS datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of the paper with the same title published at
  Proceedings of the AAAI Conference on Artificial Intelligence 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PDNNet: PDN-Aware GNN-CNN Heterogeneous Network for Dynamic IR Drop
  <span class="highlight-title">Predict</span>ion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Zhao, Zhuomin Chai, Xun Jiang, Yibo Lin, Runsheng Wang, Ru Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  IR drop on the power delivery network (PDN) is closely related to PDN's
configuration and cell current consumption. As the integrated circuit (IC)
design is growing larger, dynamic IR drop simulation becomes computationally
unaffordable and machine learning based IR drop prediction has been explored as
a promising solution. Although CNN-based methods have been adapted to IR drop
prediction task in several works, the shortcomings of overlooking PDN
configuration is non-negligible. In this paper, we consider not only how to
properly represent cell-PDN relation, but also how to model IR drop following
its physical nature in the feature aggregation procedure. Thus, we propose a
novel graph structure, PDNGraph, to unify the representations of the PDN
structure and the fine-grained cell-PDN relation. We further propose a
dual-branch heterogeneous network, PDNNet, incorporating two parallel GNN-CNN
branches to favorably capture the above features during the learning process.
Several key designs are presented to make the dynamic IR drop prediction highly
effective and interpretable. We are the first work to apply graph structure to
deep-learning based dynamic IR drop prediction method. Experiments show that
PDNNet outperforms the state-of-the-art CNN-based methods by up to 39.3%
reduction in prediction error and achieves 545x speedup compared to the
commercial tool, which demonstrates the superiority of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Noise-Robust Keyword Spotting through Self-supervised Pretraining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob Mørk, Holger Severin Bovbjerg, Gergely Kiss, Zheng-Hua Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Voice assistants are now widely available, and to activate them a keyword
spotting (KWS) algorithm is used. Modern KWS systems are mainly trained using
supervised learning methods and require a large amount of labelled data to
achieve a good performance. Leveraging unlabelled data through self-supervised
learning (SSL) has been shown to increase the accuracy in clean conditions.
This paper explores how SSL pretraining such as Data2Vec can be used to enhance
the robustness of KWS models in noisy conditions, which is under-explored.
  Models of three different sizes are pretrained using different pretraining
approaches and then fine-tuned for KWS. These models are then tested and
compared to models trained using two baseline supervised learning methods, one
being standard training using clean data and the other one being multi-style
training (MTR). The results show that pretraining and fine-tuning on clean data
is superior to supervised learning on clean data across all testing conditions,
and superior to supervised MTR for testing conditions of SNR above 5 dB. This
indicates that pretraining alone can increase the model's robustness. Finally,
it is found that using noisy data for pretraining models, especially with the
Data2Vec-denoising approach, significantly enhances the robustness of KWS
models in noisy conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention-aware semantic relevance <span class="highlight-title">predict</span>ing Chinese sentence reading 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, several influential computational models and metrics have
been proposed to predict how humans comprehend and process sentence. One
particularly promising approach is contextual semantic similarity. Inspired by
the attention algorithm in Transformer and human memory mechanisms, this study
proposes an ``attention-aware'' approach for computing contextual semantic
relevance. This new approach takes into account the different contributions of
contextual parts and the expectation effect, allowing it to incorporate
contextual information fully. The attention-aware approach also facilitates the
simulation of existing reading models and evaluate them. The resulting
``attention-aware'' metrics of semantic relevance can more accurately predict
fixation durations in Chinese reading tasks recorded in an eye-tracking corpus
than those calculated by existing approaches. The study's findings further
provide strong support for the presence of semantic preview benefits in Chinese
naturalistic reading. Furthermore, the attention-aware metrics of semantic
relevance, being memory-based, possess high interpretability from both
linguistic and cognitive standpoints, making them a valuable computational tool
for modeling eye-movements in reading and further gaining insight into the
process of language comprehension. Our approach underscores the potential of
these metrics to advance our comprehension of how humans understand and process
language, ultimately leading to a better understanding of language
comprehension and processing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ skscope: Fast Sparsity-Constrained Optimization in Python 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zezhi Wang, Jin Zhu, Peng Chen, Huiyang Peng, Xiaoke Zhang, Anran Wang, Yu Zheng, Junxian Zhu, Xueqin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Applying iterative solvers on sparsity-constrained optimization (SCO)
requires tedious mathematical deduction and careful programming/debugging that
hinders these solvers' broad impact. In the paper, the library skscope is
introduced to overcome such an obstacle. With skscope, users can solve the SCO
by just programming the objective function. The convenience of skscope is
demonstrated through two examples in the paper, where sparse linear regression
and trend filtering are addressed with just four lines of code. More
importantly, skscope's efficient implementation allows state-of-the-art solvers
to quickly attain the sparse solution regardless of the high dimensionality of
parameter space. Numerical experiments reveal the available solvers in skscope
can achieve up to 80x speedup on the competing relaxation solutions obtained
via the benchmarked convex solver. skscope is published on the Python Package
Index (PyPI) and Conda, and its source code is available at:
https://github.com/abess-team/skscope.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safe and Robust Reinforcement-Learning: Principles and Practice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taku Yamagata, Raul Santos-Rodriguez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) has shown remarkable success in solving
relatively complex tasks, yet the deployment of RL systems in real-world
scenarios poses significant challenges related to safety and robustness. This
paper aims to identify and further understand those challenges thorough the
exploration of the main dimensions of the safe and robust RL landscape,
encompassing algorithmic, ethical, and practical considerations. We conduct a
comprehensive review of methodologies and open problems that summarizes the
efforts in recent years to address the inherent risks associated with RL
applications.
  After discussing and proposing definitions for both safe and robust RL, the
paper categorizes existing research works into different algorithmic approaches
that enhance the safety and robustness of RL agents. We examine techniques such
as uncertainty estimation, optimisation methodologies, exploration-exploitation
trade-offs, and adversarial training. Environmental factors, including
sim-to-real transfer and domain adaptation, are also scrutinized to understand
how RL systems can adapt to diverse and dynamic surroundings. Moreover, human
involvement is an integral ingredient of the analysis, acknowledging the broad
set of roles that humans can take in this context.
  Importantly, to aid practitioners in navigating the complexities of safe and
robust RL implementation, this paper introduces a practical checklist derived
from the synthesized literature. The checklist encompasses critical aspects of
algorithm design, training environment considerations, and ethical guidelines.
It will serve as a resource for developers and policymakers alike to ensure the
responsible deployment of RL systems in many application domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Theoretical Bound-Guided Hierarchical VAE for Neural Image Codecs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichi Zhang, Zhihao Duan, Yuning Huang, Fengqing Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies reveal a significant theoretical link between variational
autoencoders (VAEs) and rate-distortion theory, notably in utilizing VAEs to
estimate the theoretical upper bound of the information rate-distortion
function of images. Such estimated theoretical bounds substantially exceed the
performance of existing neural image codecs (NICs). To narrow this gap, we
propose a theoretical bound-guided hierarchical VAE (BG-VAE) for NIC. The
proposed BG-VAE leverages the theoretical bound to guide the NIC model towards
enhanced performance. We implement the BG-VAE using Hierarchical VAEs and
demonstrate its effectiveness through extensive experiments. Along with
advanced neural network blocks, we provide a versatile, variable-rate NIC that
outperforms existing methods when considering both rate-distortion performance
and computational complexity. The code is available at BG-VAE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2024 IEEE International Conference on Multimedia and Expo (ICME2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Plays a Pivotal Role in the Object-Attribute Compositional
  Generalization of CLIP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reza Abbasi, Mohammad Samiei, Mohammad Hossein Rohban, Mahdieh Soleymani Baghshah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models, such as CLIP, have shown promising
Out-of-Distribution (OoD) generalization under various types of distribution
shifts. Recent studies attempted to investigate the leading cause of this
capability. In this work, we follow the same path, but focus on a specific type
of OoD data - images with novel compositions of attribute-object pairs - and
study whether such models can successfully classify those images into
composition classes. We carefully designed an authentic image test dataset
called ImageNet-AO, consisting of attributes for objects that are unlikely
encountered in the CLIP training sets. We found that CLIPs trained with large
datasets such as OpenAI CLIP, LAION-400M, and LAION-2B show orders-of-magnitude
improvement in effective compositional OoD generalization compared to both
supervised models and CLIPs trained with smaller datasets, such as CC-12M and
YFCC-15M. Our results provide evidence that the scale and diversity of training
data and language supervision play a key role in unlocking the compositional
generalization abilities of vision-language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Oral accepted at OODCV 2023(http://www.ood-cv.org)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Line Search Methods for Large Scale Neural Network Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18519v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18519v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Kenneweg, Tristan Kenneweg, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent studies, line search methods have shown significant improvements in
the performance of traditional stochastic gradient descent techniques,
eliminating the need for a specific learning rate schedule. In this paper, we
identify existing issues in state-of-the-art line search methods, propose
enhancements, and rigorously evaluate their effectiveness. We test these
methods on larger datasets and more complex data domains than before.
Specifically, we improve the Armijo line search by integrating the momentum
term from ADAM in its search direction, enabling efficient large-scale
training, a task that was previously prone to failure using Armijo line search
methods. Our optimization approach outperforms both the previous Armijo
implementation and tuned learning rate schedules for Adam. Our evaluation
focuses on Transformers and CNNs in the domains of NLP and image data. Our work
is publicly available as a Python package, which provides a hyperparameter free
Pytorch optimizer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Algorithms for Regularized Nonnegative Scale-invariant
  Low-rank Approximation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18517v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18517v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeremy E. Cohen, Valentin Leplat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regularized nonnegative low-rank approximations such as sparse Nonnegative
Matrix Factorization or sparse Nonnegative Tucker Decomposition are an
important branch of dimensionality reduction models with enhanced
interpretability. However, from a practical perspective, the choice of
regularizers and regularization coefficients, as well as the design of
efficient algorithms, is challenging because of the multifactor nature of these
models and the lack of theory to back these choices. This paper aims at
improving upon these issues. By studying a more general model called the
Homogeneous Regularized Scale-Invariant, we prove that the scale-invariance
inherent to low-rank approximation models causes an implicit regularization
with both unexpected beneficial and detrimental effects. This observation
allows to better understand the effect of regularization functions in low-rank
approximation models, to guide the choice of the regularization
hyperparameters, and to design balancing strategies to enhance the convergence
speed of dedicated optimization algorithms. Some of these results were already
known but restricted to specific instances of regularized low-rank
approximations. We also derive a generic Majorization Minimization algorithm
that handles many regularized nonnegative low-rank approximations, with
convergence guarantees. We showcase our contributions on sparse Nonnegative
Matrix Factorization, ridge-regularized Canonical Polyadic decomposition and
sparse Nonnegative Tucker Decomposition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CT-3DFlow : Leveraging 3D Normalizing Flows for Unsupervised Detection
  of Pathological Pulmonary CT scans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aissam Djahnine, Alexandre Popoff, Emilien Jupin-Delevaux, Vincent Cottin, Olivier Nempont, Loic Boussel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised pathology detection can be implemented by training a model on
healthy data only and measuring the deviation from the training set upon
inference, for example with CNN-based feature extraction and one-class
classifiers, or reconstruction-score-based methods such as AEs, GANs and
Diffusion models. Normalizing Flows (NF) have the ability to directly learn the
probability distribution of training examples through an invertible
architecture. We leverage this property in a novel 3D NF-based model named
CT-3DFlow, specifically tailored for patient-level pulmonary pathology
detection in chest CT data. Our model is trained unsupervised on healthy 3D
pulmonary CT patches, and detects deviations from its log-likelihood
distribution as anomalies. We aggregate patches-level likelihood values from a
patient's CT scan to provide a patient-level 'normal'/'abnormal' prediction.
Out-of-distribution detection performance is evaluated using expert annotations
on a separate chest CT test dataset, outperforming other state-of-the-art
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed Maximum Consensus over Noisy Links 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18509v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18509v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ehsan Lari, Reza Arablouei, Naveen K. D. Venkategowda, Stefan Werner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a distributed algorithm, termed noise-robust distributed maximum
consensus (RD-MC), for estimating the maximum value within a multi-agent
network in the presence of noisy communication links. Our approach entails
redefining the maximum consensus problem as a distributed optimization problem,
allowing a solution using the alternating direction method of multipliers.
Unlike existing algorithms that rely on multiple sets of noise-corrupted
estimates, RD-MC employs a single set, enhancing both robustness and
efficiency. To further mitigate the effects of link noise and improve
robustness, we apply moving averaging to the local estimates. Through extensive
simulations, we demonstrate that RD-MC is significantly more robust to
communication link noise compared to existing maximum-consensus algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 7 figures, submitted to EUSIPCO 2024 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Faster Convergence for Transformer Fine-tuning with Line Search Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Kenneweg, Leonardo Galli, Tristan Kenneweg, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have shown that line search methods greatly increase performance
of traditional stochastic gradient descent methods on a variety of datasets and
architectures [1], [2]. In this work we succeed in extending line search
methods to the novel and highly popular Transformer architecture and dataset
domains in natural language processing. More specifically, we combine the
Armijo line search with the Adam optimizer and extend it by subdividing the
networks architecture into sensible units and perform the line search
separately on these local units. Our optimization method outperforms the
traditional Adam optimizer and achieves significant performance improvements
for small data sets or small training budgets, while performing equal or better
for other tested cases. Our work is publicly available as a python package,
which provides a hyperparameter-free pytorch optimizer that is compatible with
arbitrary network architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Direct mineral content <span class="highlight-title">predict</span>ion from drill core images via <span class="highlight-title">transfer</span>
  learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Romana Boiger, Sergey V. Churakov, Ignacio Ballester Llagaria, Georg Kosakowski, Raphael Wüst, Nikolaos I. Prasianakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep subsurface exploration is important for mining, oil and gas industries,
as well as in the assessment of geological units for the disposal of chemical
or nuclear waste, or the viability of geothermal energy systems. Typically,
detailed examinations of subsurface formations or units are performed on
cuttings or core materials extracted during drilling campaigns, as well as on
geophysical borehole data, which provide detailed information about the
petrophysical properties of the rocks. Depending on the volume of rock samples
and the analytical program, the laboratory analysis and diagnostics can be very
time-consuming. This study investigates the potential of utilizing machine
learning, specifically convolutional neural networks (CNN), to assess the
lithology and mineral content solely from analysis of drill core images, aiming
to support and expedite the subsurface geological exploration. The paper
outlines a comprehensive methodology, encompassing data preprocessing, machine
learning methods, and transfer learning techniques. The outcome reveals a
remarkable 96.7% accuracy in the classification of drill core segments into
distinct formation classes. Furthermore, a CNN model was trained for the
evaluation of mineral content using a learning data set from multidimensional
log analysis data (silicate, total clay, carbonate). When benchmarked against
laboratory XRD measurements on samples from the cores, both the advanced
multidimensional log analysis model and the neural network approach developed
here provide equally good performance. This work demonstrates that deep
learning and particularly transfer learning can support extracting
petrophysical properties, including mineral content and formation
classification, from drill core images, thus offering a road map for enhancing
model performance and data set quality in image-based analysis of drill cores.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning in PINNs: Phase transition, total diffusion, and generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18494v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18494v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sokratis J. Anagnostopoulos, Juan Diego Toscano, Nikolaos Stergiopulos, George Em Karniadakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the learning dynamics of fully-connected neural networks
through the lens of gradient signal-to-noise ratio (SNR), examining the
behavior of first-order optimizers like Adam in non-convex objectives. By
interpreting the drift/diffusion phases in the information bottleneck theory,
focusing on gradient homogeneity, we identify a third phase termed ``total
diffusion", characterized by equilibrium in the learning rates and homogeneous
gradients. This phase is marked by an abrupt SNR increase, uniform residuals
across the sample space and the most rapid training convergence. We propose a
residual-based re-weighting scheme to accelerate this diffusion in quadratic
loss functions, enhancing generalization. We also explore the information
compression phenomenon, pinpointing a significant saturation-induced
compression of activations at the total diffusion phase, with deeper layers
experiencing negligible information loss. Supported by experimental data on
physics-informed neural networks (PINNs), which underscore the importance of
gradient homogeneity due to their PDE-based sample inter-dependence, our
findings suggest that recognizing phase transitions could refine ML
optimization strategies for improved generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Impact of Employing Weather <span class="highlight-title">Forecast</span> Data as Input to the Estimation of
  Evapotranspiration by Deep Neural Network Models <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro J. Vaz, Gabriela Schütz, Carlos Guerrero, Pedro J. S. Cardoso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reference Evapotranspiration (ET0) is a key parameter for designing smart
irrigation scheduling, since it is related by a coefficient to the water needs
of a crop. The United Nations Food and Agriculture Organization, proposed a
standard method for ET0 computation (FAO56PM), based on the parameterization of
the Penman-Monteith equation, that is widely adopted in the literature. To
compute ET0 using the FAO56-PM method, four main weather parameters are needed:
temperature, humidity, wind, and solar radiation (SR). One way to make daily
ET0 estimations for future days is to use freely available weather forecast
services (WFSs), where many meteorological parameters are estimated up to the
next 15 days. A problem with this method is that currently, SR is not provided
as a free forecast parameter on most of those online services or, normally,
such forecasts present a financial cost penalty. For this reason, several ET0
estimation models using machine and deep learning were developed and presented
in the literature, that use as input features a reduced set of carefully
selected weather parameters, that are compatible with common freely available
WFSs. However, most studies on this topic have only evaluated model performance
using data from weather stations (WSs), without considering the effect of using
weather forecast data. In this study, the performance of authors' previous
models is evaluated when using weather forecast data from two online WFSs, in
the following scenarios: (i) direct ET0 estimation by an ANN model, and (ii)
estimate SR by ANN model, and then use that estimation for ET0 computation,
using the FAO56-PM method. Employing data collected from two WFSs and a WS
located in Vale do Lobo, Portugal, the latter approach achieved the best
result, with a coefficient of determination (R2) ranging between 0.893 and
0.667, when considering forecasts up to 15 days.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A partial version of the work submitted to ESRE/INTERNATIONAL
  CONFERENCE ON ENVIRONMENTAL SCIENCES AND RENEWABLE ENERGY</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthesizing EEG Signals from Event-Related Potential Paradigms with
  Conditional Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guido Klein, Pierre Guetschel, Gianluigi Silvestri, Michael Tangermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data scarcity in the brain-computer interface field can be alleviated through
the use of generative models, specifically diffusion models. While diffusion
models have previously been successfully applied to electroencephalogram (EEG)
data, existing models lack flexibility w.r.t.~sampling or require alternative
representations of the EEG data. To overcome these limitations, we introduce a
novel approach to conditional diffusion models that utilizes classifier-free
guidance to directly generate subject-, session-, and class-specific EEG data.
In addition to commonly used metrics, domain-specific metrics are employed to
evaluate the specificity of the generated samples. The results indicate that
the proposed model can generate EEG data that resembles real data for each
subject, session, and class.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to 9th Graz BCI conference, 6 pages, 3 figures, first
  figure is split into two subfigures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SingularTrajectory: Universal Trajectory <span class="highlight-title">Predict</span>or Using Diffusion Model <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18452v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18452v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inhwan Bae, Young-Jae Park, Hae-Gon Jeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are five types of trajectory prediction tasks: deterministic,
stochastic, domain adaptation, momentary observation, and few-shot. These
associated tasks are defined by various factors, such as the length of input
paths, data split and pre-processing methods. Interestingly, even though they
commonly take sequential coordinates of observations as input and infer future
paths in the same coordinates as output, designing specialized architectures
for each task is still necessary. For the other task, generality issues can
lead to sub-optimal performances. In this paper, we propose SingularTrajectory,
a diffusion-based universal trajectory prediction framework to reduce the
performance gap across the five tasks. The core of SingularTrajectory is to
unify a variety of human dynamics representations on the associated tasks. To
do this, we first build a Singular space to project all types of motion
patterns from each task into one embedding space. We next propose an adaptive
anchor working in the Singular space. Unlike traditional fixed anchor methods
that sometimes yield unacceptable paths, our adaptive anchor enables correct
anchors, which are put into a wrong location, based on a traversability map.
Finally, we adopt a diffusion-based predictor to further enhance the prototype
paths using a cascaded denoising process. Our unified framework ensures the
generality across various benchmark settings such as input modality, and
trajectory lengths. Extensive experiments on five public benchmarks demonstrate
that SingularTrajectory substantially outperforms existing models, highlighting
its effectiveness in estimating general dynamics of human movements. Code is
publicly available at https://github.com/inhwanbae/SingularTrajectory .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoRAST: Towards Foundation Model-Powered Correlated Data Analysis in
  Resource-Constrained CPS and IoT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Hu, Jinhang Zuo, Alanis Zhao, Bob Iannucci, Carlee Joe-Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models (FMs) emerge as a promising solution to harness distributed
and diverse environmental data by leveraging prior knowledge to understand the
complicated temporal and spatial correlations within heterogeneous datasets.
Unlike distributed learning frameworks such as federated learning, which often
struggle with multimodal data, FMs can transform diverse inputs into
embeddings. This process facilitates the integration of information from
various modalities and the application of prior learning to new domains.
However, deploying FMs in resource-constrained edge systems poses significant
challenges. To this end, we introduce CoRAST, a novel learning framework that
utilizes FMs for enhanced analysis of distributed, correlated heterogeneous
data. Utilizing a server-based FM, CoRAST can exploit existing environment
information to extract temporal, spatial, and cross-modal correlations among
sensor data. This enables CoRAST to offer context-aware insights for localized
client tasks through FM-powered global representation learning. Our evaluation
on real-world weather dataset demonstrates CoRAST's ability to exploit
correlated heterogeneous data through environmental representation learning to
reduce the forecast errors by up to 50.3% compared to the baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted and to be published in 2024 IEEE International Workshop on
  Foundation Models for Cyber-Physical Systems & Internet of Things (FMSys)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Language Beat Numerical Regression? Language-Based Multimodal
  Trajectory <span class="highlight-title">Predict</span>ion <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inhwan Bae, Junoh Lee, Hae-Gon Jeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models have demonstrated impressive ability in context understanding
and generative performance. Inspired by the recent success of language
foundation models, in this paper, we propose LMTraj (Language-based Multimodal
Trajectory predictor), which recasts the trajectory prediction task into a sort
of question-answering problem. Departing from traditional numerical regression
models, which treat the trajectory coordinate sequence as continuous signals,
we consider them as discrete signals like text prompts. Specially, we first
transform an input space for the trajectory coordinate into the natural
language space. Here, the entire time-series trajectories of pedestrians are
converted into a text prompt, and scene images are described as text
information through image captioning. The transformed numerical and image data
are then wrapped into the question-answering template for use in a language
model. Next, to guide the language model in understanding and reasoning
high-level knowledge, such as scene context and social relationships between
pedestrians, we introduce an auxiliary multi-task question and answering. We
then train a numerical tokenizer with the prompt data. We encourage the
tokenizer to separate the integer and decimal parts well, and leverage it to
capture correlations between the consecutive numbers in the language model.
Lastly, we train the language model using the numerical tokenizer and all of
the question-answer prompts. Here, we propose a beam-search-based most-likely
prediction and a temperature-based multimodal prediction to implement both
deterministic and stochastic inferences. Applying our LMTraj, we show that the
language-based model can be a powerful pedestrian trajectory predictor, and
outperforms existing numerical-based predictor methods. Code is publicly
available at https://github.com/inhwanbae/LMTrajectory .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FRESCO: <span class="highlight-title">Federated</span> Reinforcement Energy System for Cooperative
  Optimization <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Mauricio Cuadrado, Roberto Alejandro Gutierrez, Martin Takáč
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise in renewable energy is creating new dynamics in the energy grid that
promise to create a cleaner and more participative energy grid, where
technology plays a crucial part in making the required flexibility to achieve
the vision of the next-generation grid. This work presents FRESCO, a framework
that aims to ease the implementation of energy markets using a hierarchical
control architecture of reinforcement learning agents trained using federated
learning. The core concept we are proving is that having greedy agents subject
to changing conditions from a higher level agent creates a cooperative setup
that will allow for fulfilling all the individual objectives. This paper
presents a general overview of the framework, the current progress, and some
insights we obtained from the recent results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tiny Paper at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalized Policy Learning for Smart Grids: FL TRPO Approach <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunxiang Li, Nicolas Mauricio Cuadrado, Samuel Horváth, Martin Takáč
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The smart grid domain requires bolstering the capabilities of existing energy
management systems; Federated Learning (FL) aligns with this goal as it
demonstrates a remarkable ability to train models on heterogeneous datasets
while maintaining data privacy, making it suitable for smart grid applications,
which often involve disparate data distributions and interdependencies among
features that hinder the suitability of linear models. This paper introduces a
framework that combines FL with a Trust Region Policy Optimization (FL TRPO)
aiming to reduce energy-associated emissions and costs. Our approach reveals
latent interconnections and employs personalized encoding methods to capture
unique insights, understanding the relationships between features and optimal
strategies, allowing our model to generalize to previously unseen data.
Experimental results validate the robustness of our approach, affirming its
proficiency in effectively learning policy models for smart grid challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 Workshop: Tackling Climate Change with Machine Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Global Vegetation Modeling with Pre-Trained Weather Transformers <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18438v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18438v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pascal Janetzky, Florian Gallusser, Simon Hentschel, Andreas Hotho, Anna Krause
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate vegetation models can produce further insights into the complex
interaction between vegetation activity and ecosystem processes. Previous
research has established that long-term trends and short-term variability of
temperature and precipitation affect vegetation activity. Motivated by the
recent success of Transformer-based Deep Learning models for medium-range
weather forecasting, we adapt the publicly available pre-trained FourCastNet to
model vegetation activity while accounting for the short-term dynamics of
climate variability. We investigate how the learned global representation of
the atmosphere's state can be transferred to model the normalized difference
vegetation index (NDVI). Our model globally estimates vegetation activity at a
resolution of \SI{0.25}{\degree} while relying only on meteorological data. We
demonstrate that leveraging pre-trained weather models improves the NDVI
estimates compared to learning an NDVI model from scratch. Additionally, we
compare our results to other recent data-driven NDVI modeling approaches from
machine learning and ecology literature. We further provide experimental
evidence on how much data and training time is necessary to turn FourCastNet
into an effective vegetation model. Code and models will be made available upon
publication.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tackling Climate Change with Machine Learning Workshop @ ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collaborative Active Learning in Conditional Trust Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18436v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18436v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zan-Kai Chong, Hiroyuki Ohsaki, Bryan Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate collaborative active learning, a paradigm in
which multiple collaborators explore a new domain by leveraging their combined
machine learning capabilities without disclosing their existing data and
models. Instead, the collaborators share prediction results from the new domain
and newly acquired labels. This collaboration offers several advantages: (a) it
addresses privacy and security concerns by eliminating the need for direct
model and data disclosure; (b) it enables the use of different data sources and
insights without direct data exchange; and (c) it promotes cost-effectiveness
and resource efficiency through shared labeling costs. To realize these
benefits, we introduce a collaborative active learning framework designed to
fulfill the aforementioned objectives. We validate the effectiveness of the
proposed framework through simulations. The results demonstrate that
collaboration leads to higher AUC scores compared to independent efforts,
highlighting the framework's ability to overcome the limitations of individual
models. These findings support the use of collaborative approaches in active
learning, emphasizing their potential to enhance outcomes through collective
expertise and shared resources. Our work provides a foundation for further
research on collaborative active learning and its practical applications in
various domains where data privacy, cost efficiency, and model performance are
critical considerations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 9 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ U-Sketch: An Efficient Approach for Sketch to Image Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilias Mitsouras, Eleftherios Tsonis, Paraskevi Tzouveli, Athanasios Voulodimos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated remarkable performance in text-to-image
synthesis, producing realistic and high resolution images that faithfully
adhere to the corresponding text-prompts. Despite their great success, they
still fall behind in sketch-to-image synthesis tasks, where in addition to
text-prompts, the spatial layout of the generated images has to closely follow
the outlines of certain reference sketches. Employing an MLP latent edge
predictor to guide the spatial layout of the synthesized image by predicting
edge maps at each denoising step has been recently proposed. Despite yielding
promising results, the pixel-wise operation of the MLP does not take into
account the spatial layout as a whole, and demands numerous denoising
iterations to produce satisfactory images, leading to time inefficiency. To
this end, we introduce U-Sketch, a framework featuring a U-Net type latent edge
predictor, which is capable of efficiently capturing both local and global
features, as well as spatial correlations between pixels. Moreover, we propose
the addition of a sketch simplification network that offers the user the choice
of preprocessing and simplifying input sketches for enhanced outputs. The
experimental results, corroborated by user feedback, demonstrate that our
proposed U-Net latent edge predictor leads to more realistic results, that are
better aligned with the spatial outlines of the reference sketches, while
drastically reducing the number of required denoising steps and, consequently,
the overall execution time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SemRoDe: Macro Adversarial Training to Learn Representations That are
  Robust to Word-Level Attacks <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18423v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18423v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian Formento, Wenjie Feng, Chuan Sheng Foo, Luu Anh Tuan, See-Kiong Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs) are indispensable tools for natural language processing
tasks, but their vulnerability to adversarial attacks remains a concern. While
current research has explored adversarial training techniques, their
improvements to defend against word-level attacks have been limited. In this
work, we propose a novel approach called Semantic Robust Defence (SemRoDe), a
Macro Adversarial Training strategy to enhance the robustness of LMs. Drawing
inspiration from recent studies in the image domain, we investigate and later
confirm that in a discrete data setting such as language, adversarial samples
generated via word substitutions do indeed belong to an adversarial domain
exhibiting a high Wasserstein distance from the base domain. Our method learns
a robust representation that bridges these two domains. We hypothesize that if
samples were not projected into an adversarial domain, but instead to a domain
with minimal shift, it would improve attack robustness. We align the domains by
incorporating a new distance-based objective. With this, our model is able to
learn more generalized representations by aligning the model's high-level
output features and therefore better handling unseen adversarial samples. This
method can be generalized across word embeddings, even when they share minimal
overlap at both vocabulary and word-substitution levels. To evaluate the
effectiveness of our approach, we conduct experiments on BERT and RoBERTa
models on three datasets. The results demonstrate promising state-of-the-art
robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in NAACL 2024 (Main Track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Topos of Transformer Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18415v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18415v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mattia Jacopo Villani, Peter McBurney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The transformer neural network has significantly out-shined all other neural
network architectures as the engine behind large language models. We provide a
theoretical analysis of the expressivity of the transformer architecture
through the lens of topos theory. From this viewpoint, we show that many common
neural network architectures, such as the convolutional, recurrent and graph
convolutional networks, can be embedded in a pretopos of piecewise-linear
functions, but that the transformer necessarily lives in its topos completion.
In particular, this suggests that the two network families instantiate
different fragments of logic: the former are first order, whereas transformers
are higher-order reasoners. Furthermore, we draw parallels with architecture
search and gradient descent, integrating our analysis in the framework of
cybernetic agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering
  Using a VLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonkyun Kim, Changin Choi, Wonseok Lee, Wonjong Rhee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stimulated by the sophisticated reasoning capabilities of recent Large
Language Models (LLMs), a variety of strategies for bridging video modality
have been devised. A prominent strategy involves Video Language Models
(VideoLMs), which train a learnable interface with video data to connect
advanced vision encoders with LLMs. Recently, an alternative strategy has
surfaced, employing readily available foundation models, such as VideoLMs and
LLMs, across multiple stages for modality bridging. In this study, we introduce
a simple yet novel strategy where only a single Vision Language Model (VLM) is
utilized. Our starting point is the plain insight that a video comprises a
series of images, or frames, interwoven with temporal information. The essence
of video comprehension lies in adeptly managing the temporal aspects along with
the spatial details of each frame. Initially, we transform a video into a
single composite image by arranging multiple frames in a grid layout. The
resulting single image is termed as an image grid. This format, while
maintaining the appearance of a solitary image, effectively retains temporal
information within the grid structure. Therefore, the image grid approach
enables direct application of a single high-performance VLM without
necessitating any video-data training. Our extensive experimental analysis
across ten zero-shot video question answering benchmarks, including five
open-ended and five multiple-choice benchmarks, reveals that the proposed Image
Grid Vision Language Model (IG-VLM) surpasses the existing methods in nine out
of ten benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code is available at https://github.com/imagegridworth/IG-VLM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Spectrogram Analysis in a Multiple Classifier Fusion Framework for
  Power Grid Classification Using Electric Network Frequency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Tzolopoulos, Christos Korgialas, Constantine Kotropoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Electric Network Frequency (ENF) serves as a unique signature inherent to
power distribution systems. Here, a novel approach for power grid
classification is developed, leveraging ENF. Spectrograms are generated from
audio and power recordings across different grids, revealing distinctive ENF
patterns that aid in grid classification through a fusion of classifiers. Four
traditional machine learning classifiers plus a Convolutional Neural Network
(CNN), optimized using Neural Architecture Search, are developed for One-vs-All
classification. This process generates numerous predictions per sample, which
are then compiled and used to train a shallow multi-label neural network
specifically designed to model the fusion process, ultimately leading to the
conclusive class prediction for each sample. Experimental findings reveal that
both validation and testing accuracy outperform those of current
state-of-the-art classifiers, underlining the effectiveness and robustness of
the proposed methodology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13th International Conference on Pattern Recognition Applications and
  Methods (ICPRAM)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Colour and Brush Stroke Pattern Recognition in Abstract Art using
  Modified Deep Convolutional Generative Adversarial Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Srinitish Srinivasan, Varenya Pathak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Abstract Art is an immensely popular, discussed form of art that often has
the ability to depict the emotions of an artist. Many researchers have made
attempts to study abstract art in the form of edge detection, brush stroke and
emotion recognition algorithms using machine and deep learning. This papers
describes the study of a wide distribution of abstract paintings using
Generative Adversarial Neural Networks(GAN). GANs have the ability to learn and
reproduce a distribution enabling researchers and scientists to effectively
explore and study the generated image space. However, the challenge lies in
developing an efficient GAN architecture that overcomes common training
pitfalls. This paper addresses this challenge by introducing a modified-DCGAN
(mDCGAN) specifically designed for high-quality artwork generation. The
approach involves a thorough exploration of the modifications made, delving
into the intricate workings of DCGANs, optimisation techniques, and
regularisation methods aimed at improving stability and realism in art
generation enabling effective study of generated patterns. The proposed mDCGAN
incorporates meticulous adjustments in layer configurations and architectural
choices, offering tailored solutions to the unique demands of art generation
while effectively combating issues like mode collapse and gradient vanishing.
Further this paper explores the generated latent space by performing random
walks to understand vector relationships between brush strokes and colours in
the abstract art space and a statistical analysis of unstable outputs after a
certain period of GAN training and compare its significant difference. These
findings validate the effectiveness of the proposed approach, emphasising its
potential to revolutionise the field of digital art generation and digital art
ecosystem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 5 tables, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tensor-based Graph Learning with Consistency and Specificity for
  Multi-view Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18393v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18393v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Shi, Lei Cao, Yunshan Ye, Yu Zhao, Badong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph learning is widely recognized as a crucial technique in multi-view
clustering. Existing graph learning methods typically involve constructing an
adaptive neighbor graph based on probabilistic neighbors and then learning a
consensus graph to for clustering, however, they are confronted with two
limitations. Firstly, they often rely on Euclidean distance to measure
similarity when constructing the adaptive neighbor graph, which proves
inadequate in capturing the intrinsic structure among data points in many
real-world scenarios. Secondly, most of these methods focus solely on consensus
graph, ignoring view-specific graph information. In response to the
aforementioned drawbacks, we in this paper propose a novel tensor-based graph
learning framework that simultaneously considers consistency and specificity
for multi-view clustering. Specifically, we calculate the similarity distance
on the Stiefel manifold to preserve the intrinsic structure among data points.
By making an assumption that the learned neighbor graph of each view comprises
both a consistent graph and a view-specific graph, we formulate a new
tensor-based target graph learning paradigm. Owing to the benefits of tensor
singular value decomposition (t-SVD) in uncovering high-order correlations,
this model is capable of achieving a complete understanding of the target
graph. Furthermore, we develop an iterative algorithm to solve the proposed
objective optimization problem. Experiments conducted on real-world datasets
have demonstrated the superior performance of the proposed method over some
state-of-the-art multi-view clustering methods. The source code has been
released on https://github.com/lshi91/CSTGL-Code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Multi-modal Models are Good Class-Incremental Learners <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xusheng Cao, Haori Lu, Linlan Huang, Xialei Liu, Ming-Ming Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In class-incremental learning (CIL) scenarios, the phenomenon of catastrophic
forgetting caused by the classifier's bias towards the current task has long
posed a significant challenge. It is mainly caused by the characteristic of
discriminative models. With the growing popularity of the generative
multi-modal models, we would explore replacing discriminative models with
generative ones for CIL. However, transitioning from discriminative to
generative models requires addressing two key challenges. The primary challenge
lies in transferring the generated textual information into the classification
of distinct categories. Additionally, it requires formulating the task of CIL
within a generative framework. To this end, we propose a novel generative
multi-modal model (GMM) framework for class-incremental learning. Our approach
directly generates labels for images using an adapted generative model. After
obtaining the detailed text, we use a text encoder to extract text features and
employ feature matching to determine the most similar label as the
classification prediction. In the conventional CIL settings, we achieve
significantly better results in long-sequence task scenarios. Under the
Few-shot CIL setting, we have improved by at least 14\% accuracy over all the
current state-of-the-art methods with significantly less forgetting. Our code
is available at \url{https://github.com/DoubleClass/GMM}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IIP-Mixer:Intra-Inter Patch Mixing Architecture for Battery Remaining
  Useful Life <span class="highlight-title">Predict</span>ion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangzai Ye, Li Feng, Jianlan Guo, Yuqiang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately estimating the Remaining Useful Life (RUL) of lithium-ion
batteries is crucial for maintaining the safe and stable operation of
rechargeable battery management systems. However, this task is often
challenging due to the complex temporal dynamics involved. Recently,
attention-based networks, such as Transformers and Informer, have been the
popular architecture in time series forecasting. Despite their effectiveness,
these models with abundant parameters necessitate substantial training time to
unravel temporal patterns. To tackle these challenges, we propose a simple
MLP-Mixer-based architecture named 'Intra-Inter Patch Mixer' (IIP-Mixer), which
is an architecture based exclusively on multi-layer perceptrons (MLPs),
extracting information by mixing operations along both intra-patch and
inter-patch dimensions for battery RUL prediction. The proposed IIP-Mixer
comprises parallel dual-head mixer layers: the intra-patch mixing MLP,
capturing local temporal patterns in the short-term period, and the inter-patch
mixing MLP, capturing global temporal patterns in the long-term period.
Notably, to address the varying importance of features in RUL prediction, we
introduce a weighted loss function in the MLP-Mixer-based architecture, marking
the first time such an approach has been employed. Our experiments demonstrate
that IIP-Mixer achieves competitive performance in battery RUL prediction,
outperforming other popular time-series frameworks
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stragglers-Aware Low-Latency Synchronous <span class="highlight-title">Federated</span> Learning via
  Layer-Wise Model Updates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Natalie Lang, Alejandro Cohen, Nir Shlezinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synchronous federated learning (FL) is a popular paradigm for collaborative
edge learning. It typically involves a set of heterogeneous devices locally
training neural network (NN) models in parallel with periodic centralized
aggregations. As some of the devices may have limited computational resources
and varying availability, FL latency is highly sensitive to stragglers.
Conventional approaches discard incomplete intra-model updates done by
stragglers, alter the amount of local workload and architecture, or resort to
asynchronous settings; which all affect the trained model performance under
tight training latency constraints. In this work, we propose straggler-aware
layer-wise federated learning (SALF) that leverages the optimization procedure
of NNs via backpropagation to update the global model in a layer-wise fashion.
SALF allows stragglers to synchronously convey partial gradients, having each
layer of the global model be updated independently with a different
contributing set of users. We provide a theoretical analysis, establishing
convergence guarantees for the global model under mild assumptions on the
distribution of the participating devices, revealing that SALF converges at the
same asymptotic rate as FL with no timing limitations. This insight is matched
with empirical observations, demonstrating the performance gains of SALF
compared to alternative mechanisms mitigating the device heterogeneity gap in
FL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ship in Sight: Diffusion Models for Ship-Image Super Resolution <span class="chip">IJCNN</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luigi Sigillo, Riccardo Fosco Gramaccioni, Alessandro Nicolosi, Danilo Comminiello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, remarkable advancements have been achieved in the field of
image generation, primarily driven by the escalating demand for high-quality
outcomes across various image generation subtasks, such as inpainting,
denoising, and super resolution. A major effort is devoted to exploring the
application of super-resolution techniques to enhance the quality of
low-resolution images. In this context, our method explores in depth the
problem of ship image super resolution, which is crucial for coastal and port
surveillance. We investigate the opportunity given by the growing interest in
text-to-image diffusion models, taking advantage of the prior knowledge that
such foundation models have already learned. In particular, we present a
diffusion-model-based architecture that leverages text conditioning during
training while being class-aware, to best preserve the crucial details of the
ships during the generation of the super-resoluted image. Since the specificity
of this task and the scarcity availability of off-the-shelf data, we also
introduce a large labeled ship dataset scraped from online ship images, mostly
from ShipSpotting\footnote{\url{www.shipspotting.com}} website. Our method
achieves more robust results than other deep learning models previously
employed for super resolution, as proven by the multiple experiments performed.
Moreover, we investigate how this model can benefit downstream tasks, such as
classification and object detection, thus emphasizing practical implementation
in a real-world scenario. Experimental results show flexibility, reliability,
and impressive performance of the proposed framework over state-of-the-art
methods for different tasks. The code is available at:
https://github.com/LuigiSigillo/ShipinSight .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2024 International Joint Conference on Neural Networks
  (IJCNN)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intent-Aware DRL-Based Uplink Dynamic Scheduler for 5G-NR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salwa Mostafa, Mateus P. Mota, Alvaro Valcarce, Mehdi Bennis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the problem of supporting Industrial Internet of Things user
equipment (IIoT UEs) with intent (i.e., requested quality of service (QoS)) and
random traffic arrival. A deep reinforcement learning (DRL) based centralized
dynamic scheduler for time-frequency resources is proposed to learn how to
schedule the available communication resources among the IIoT UEs. The proposed
scheduler leverages an RL framework to adapt to the dynamic changes in the
wireless communication system and traffic arrivals. Moreover, a graph-based
reduction scheme is proposed to reduce the state and action space of the RL
framework to allow fast convergence and a better learning strategy. Simulation
results demonstrate the effectiveness of the proposed intelligent scheduler in
guaranteeing the expressed intent of IIoT UEs compared to several traditional
scheduling schemes, such as round-robin, semi-static, and heuristic approaches.
The proposed scheduler also outperforms the contention-free and
contention-based schemes in maximizing the number of successfully computed
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Supervised Multiple Kernel Learning approaches for multi-omics data
  integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18355v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18355v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mitja Briscik, Gabriele Tazza, Marie-Agnes Dillies, László Vidács, Sébastien Dejean
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in high-throughput technologies have originated an ever-increasing
availability of omics datasets. The integration of multiple heterogeneous data
sources is currently an issue for biology and bioinformatics. Multiple kernel
learning (MKL) has shown to be a flexible and valid approach to consider the
diverse nature of multi-omics inputs, despite being an underused tool in
genomic data mining.We provide novel MKL approaches based on different kernel
fusion strategies.To learn from the meta-kernel of input kernels, we
adaptedunsupervised integration algorithms for supervised tasks with support
vector machines.We also tested deep learning architectures for kernel fusion
and classification.The results show that MKL-based models can compete with more
complex, state-of-the-art, supervised multi-omics integrative approaches.
Multiple kernel learning offers a natural framework for predictive models in
multi-omics genomic data. Our results offer a direction for bio-data mining
research and further development of methods for heterogeneous data integration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Diverse Agricultural Data for Vision-Based Farming
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikolaj Cieslak, Umabharathi Govindarajan, Alejandro Garcia, Anuradha Chandrashekar, Torsten Hädrich, Aleksander Mendoza-Drosik, Dominik L. Michels, Sören Pirk, Chia-Chun Fu, Wojciech Pałubicki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a specialized procedural model for generating synthetic
agricultural scenes, focusing on soybean crops, along with various weeds. This
model is capable of simulating distinct growth stages of these plants, diverse
soil conditions, and randomized field arrangements under varying lighting
conditions. The integration of real-world textures and environmental factors
into the procedural generation process enhances the photorealism and
applicability of the synthetic data. Our dataset includes 12,000 images with
semantic labels, offering a comprehensive resource for computer vision tasks in
precision agriculture, such as semantic segmentation for autonomous weed
control. We validate our model's effectiveness by comparing the synthetic data
against real agricultural images, demonstrating its potential to significantly
augment training data for machine learning models in agriculture. This approach
not only provides a cost-effective solution for generating high-quality,
diverse data but also addresses specific needs in agricultural vision tasks
that are not fully covered by general-purpose models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Quantum Fuzzy-based Approach for Real-Time Detection of Solar Coronal
  Holes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanmoy Bandyopadhyay, Suman Kundu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The detection and analysis of the solar coronal holes (CHs) is an important
field of study in the domain of solar physics. Mainly, it is required for the
proper prediction of the geomagnetic storms which directly or indirectly affect
various space and ground-based systems. For the detection of CHs till date, the
solar scientist depends on manual hand-drawn approaches. However, with the
advancement of image processing technologies, some automated image segmentation
methods have been used for the detection of CHs. In-spite of this, fast and
accurate detection of CHs are till a major issues. Here in this work, a novel
quantum computing-based fast fuzzy c-mean technique has been developed for fast
detection of the CHs region. The task has been carried out in two stages, in
first stage the solar image has been segmented using a quantum computing based
fast fuzzy c-mean (QCFFCM) and in the later stage the CHs has been extracted
out from the segmented image based on image morphological operation. In the
work, quantum computing has been used to optimize the cost function of the fast
fuzzy c-mean (FFCM) algorithm, where quantum approximate optimization algorithm
(QAOA) has been used to optimize the quadratic part of the cost function. The
proposed method has been tested for 193 \AA{} SDO/AIA full-disk solar image
datasets and has been compared with the existing techniques. The outcome shows
the comparable performance of the proposed method with the existing one within
a very lesser time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Artificial Neural Twin -- Process Optimization and Continual
  Learning in Distributed Process Chains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes Emmert, Ronald Mendez, Houman Mirzaalian Dastjerdi, Christopher Syben, Andreas Maier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Industrial process optimization and control is crucial to increase economic
and ecologic efficiency. However, data sovereignty, differing goals, or the
required expert knowledge for implementation impede holistic implementation.
Further, the increasing use of data-driven AI-methods in process models and
industrial sensory often requires regular fine-tuning to accommodate
distribution drifts. We propose the Artificial Neural Twin, which combines
concepts from model predictive control, deep learning, and sensor networks to
address these issues. Our approach introduces differentiable data fusion to
estimate the state of distributed process steps and their dependence on input
data. By treating the interconnected process steps as a quasi neural-network,
we can backpropagate loss gradients for process optimization or model
fine-tuning to process parameters or AI models respectively. The concept is
demonstrated on a virtual machine park simulated in Unity, consisting of bulk
material processes in plastic recycling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Macroscale fracture surface segmentation via semi-supervised learning
  considering the structural similarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes Rosenberger, Johannes Tlatlik, Sebastian Münstermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To this date the safety assessment of materials, used for example in the
nuclear power sector, commonly relies on a fracture mechanical analysis
utilizing macroscopic concepts, where a global load quantity K or J is compared
to the materials fracture toughness curve. Part of the experimental effort
involved in these concepts is dedicated to the quantitative analysis of
fracture surfaces. Within the scope of this study a methodology for the
semi-supervised training of deep learning models for fracture surface
segmentation on a macroscopic level was established. Therefore, three distinct
and unique datasets were created to analyze the influence of structural
similarity on the segmentation capability. The structural similarity differs
due to the assessed materials and specimen, as well as imaging-induced variance
due to fluctuations in image acquisition in different laboratories. The
datasets correspond to typical isolated laboratory conditions, complex
real-world circumstances, and a curated subset of the two. We implemented a
weak-to-strong consistency regularization for semi-supervised learning. On the
heterogeneous dataset we were able to train robust and well-generalizing models
that learned feature representations from images across different domains
without observing a significant drop in prediction quality. Furthermore, our
approach reduced the number of labeled images required for training by a factor
of 6. To demonstrate the success of our method and the benefit of our approach
for the fracture mechanics assessment, we utilized the models for initial crack
size measurements with the area average method. For the laboratory setting, the
deep learning assisted measurements proved to have the same quality as manual
measurements. For models trained on the heterogeneous dataset, very good
measurement accuracies with mean deviations smaller than 1 % could be
achieved...
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>During review title changed to: Deep learning based initial crack
  size measurements utilizing macroscale fracture surface segmentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Dataset</span> for Pharmacovigilance in German, French, and Japanese:
  Annotating Adverse Drug Reactions across Languages <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lisa Raithel, Hui-Syuan Yeh, Shuntaro Yada, Cyril Grouin, Thomas Lavergne, Aurélie Névéol, Patrick Paroubek, Philippe Thomas, Tomohiro Nishiyama, Sebastian Möller, Eiji Aramaki, Yuji Matsumoto, Roland Roller, Pierre Zweigenbaum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User-generated data sources have gained significance in uncovering Adverse
Drug Reactions (ADRs), with an increasing number of discussions occurring in
the digital world. However, the existing clinical corpora predominantly revolve
around scientific articles in English. This work presents a multilingual corpus
of texts concerning ADRs gathered from diverse sources, including patient fora,
social media, and clinical reports in German, French, and Japanese. Our corpus
contains annotations covering 12 entity types, four attribute types, and 13
relation types. It contributes to the development of real-world multilingual
language models for healthcare. We provide statistics to highlight certain
challenges associated with the corpus and conduct preliminary experiments
resulting in strong baselines for extracting entities and relations between
these entities, both within and across languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tracking-Assisted Object Detection with Event Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ting-Kang Yen, Igor Morawski, Shusil Dangi, Kai He, Chung-Yi Lin, Jia-Fong Yeh, Hung-Ting Su, Winston Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event-based object detection has recently garnered attention in the computer
vision community due to the exceptional properties of event cameras, such as
high dynamic range and no motion blur. However, feature asynchronism and
sparsity cause invisible objects due to no relative motion to the camera,
posing a significant challenge in the task. Prior works have studied various
memory mechanisms to preserve as many features as possible at the current time,
guided by temporal clues. While these implicit-learned memories retain some
short-term information, they still struggle to preserve long-term features
effectively. In this paper, we consider those invisible objects as
pseudo-occluded objects and aim to reveal their features. Firstly, we introduce
visibility attribute of objects and contribute an auto-labeling algorithm to
append additional visibility labels on an existing event camera dataset.
Secondly, we exploit tracking strategies for pseudo-occluded objects to
maintain their permanence and retain their bounding boxes, even when features
have not been available for a very long time. These strategies can be treated
as an explicit-learned memory guided by the tracking objective to record the
displacements of objects across frames. Lastly, we propose a spatio-temporal
feature aggregation module to enrich the latent features and a consistency loss
to increase the robustness of the overall pipeline. We conduct comprehensive
experiments to verify our method's effectiveness where still objects are
retained but real occluded objects are discarded. The results demonstrate that
(1) the additional visibility labels can assist in supervised training, and (2)
our method outperforms state-of-the-art approaches with a significant
improvement of 7.9% absolute mAP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Privacy-Preserving Distributed Nonnegative Matrix Factorization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ehsan Lari, Reza Arablouei, Stefan Werner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nonnegative matrix factorization (NMF) is an effective data representation
tool with numerous applications in signal processing and machine learning.
However, deploying NMF in a decentralized manner over ad-hoc networks
introduces privacy concerns due to the conventional approach of sharing raw
data among network agents. To address this, we propose a privacy-preserving
algorithm for fully-distributed NMF that decomposes a distributed large data
matrix into left and right matrix factors while safeguarding each agent's local
data privacy. It facilitates collaborative estimation of the left matrix factor
among agents and enables them to estimate their respective right factors
without exposing raw data. To ensure data privacy, we secure information
exchanges between neighboring agents utilizing the Paillier cryptosystem, a
probabilistic asymmetric algorithm for public-key cryptography that allows
computations on encrypted data without decryption. Simulation results conducted
on synthetic and real-world datasets demonstrate the effectiveness of the
proposed algorithm in achieving privacy-preserving distributed NMF over ad-hoc
networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure, submitted to EUSIPCO 2024 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantum Algorithms: A New Frontier in Financial Crime Prevention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abraham Itzhak Weinberg, Alessio Faccia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Financial crimes fast proliferation and sophistication require novel
approaches that provide robust and effective solutions. This paper explores the
potential of quantum algorithms in combating financial crimes. It highlights
the advantages of quantum computing by examining traditional and Machine
Learning (ML) techniques alongside quantum approaches. The study showcases
advanced methodologies such as Quantum Machine Learning (QML) and Quantum
Artificial Intelligence (QAI) as powerful solutions for detecting and
preventing financial crimes, including money laundering, financial crime
detection, cryptocurrency attacks, and market manipulation. These quantum
approaches leverage the inherent computational capabilities of quantum
computers to overcome limitations faced by classical methods. Furthermore, the
paper illustrates how quantum computing can support enhanced financial risk
management analysis. Financial institutions can improve their ability to
identify and mitigate risks, leading to more robust risk management strategies
by exploiting the quantum advantage. This research underscores the
transformative impact of quantum algorithms on financial risk management. By
embracing quantum technologies, organisations can enhance their capabilities to
combat evolving threats and ensure the integrity and stability of financial
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Implementation of the Principal Component Analysis onto High-Performance
  Computer Facilities for Hyperspectral Dimensionality Reduction: Results and
  Comparisons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        E. Martel, R. Lazcano, J. Lopez, D. Madroñal, R. Salvador, S. Lopez, E. Juarez, R. Guerra, C. Sanz, R. Sarmiento
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dimensionality reduction represents a critical preprocessing step in order to
increase the efficiency and the performance of many hyperspectral imaging
algorithms. However, dimensionality reduction algorithms, such as the Principal
Component Analysis (PCA), suffer from their computationally demanding nature,
becoming advisable for their implementation onto high-performance computer
architectures for applications under strict latency constraints. This work
presents the implementation of the PCA algorithm onto two different
high-performance devices, namely, an NVIDIA Graphics Processing Unit (GPU) and
a Kalray manycore, uncovering a highly valuable set of tips and tricks in order
to take full advantage of the inherent parallelism of these high-performance
computing platforms, and hence, reducing the time that is required to process a
given hyperspectral image. Moreover, the achieved results obtained with
different hyperspectral images have been compared with the ones that were
obtained with a field programmable gate array (FPGA)-based implementation of
the PCA algorithm that has been recently published, providing, for the first
time in the literature, a comprehensive analysis in order to highlight the pros
and cons of each option.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Modal Contrastive Learning for Online Clinical Time-Series
  Applications <span class="chip">ICLR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Baldenweg, Manuel Burger, Gunnar Rätsch, Rita Kuznetsova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electronic Health Record (EHR) datasets from Intensive Care Units (ICU)
contain a diverse set of data modalities. While prior works have successfully
leveraged multiple modalities in supervised settings, we apply advanced
self-supervised multi-modal contrastive learning techniques to ICU data,
specifically focusing on clinical notes and time-series for clinically relevant
online prediction tasks. We introduce a loss function Multi-Modal Neighborhood
Contrastive Loss (MM-NCL), a soft neighborhood function, and showcase the
excellent linear probe and zero-shot performance of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a Workshop Paper at TS4H@ICLR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A thermodynamically consistent physics-informed deep learning material
  model for short fiber/polymer nanocomposites 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Betim Bahtiri, Behrouz Arash, Sven Scheffler, Maximilian Jux, Raimund Rolfes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work proposes a physics-informed deep learning (PIDL)-based constitutive
model for investigating the viscoelastic-viscoplastic behavior of short
fiber-reinforced nanoparticle-filled epoxies under various ambient conditions.
The deep-learning model is trained to enforce thermodynamic principles, leading
to a thermodynamically consistent constitutive model. To accomplish this, a
long short-term memory network is combined with a feed-forward neural network
to predict internal variables required for characterizing the internal
dissipation of the nanocomposite materials. In addition, another feed-forward
neural network is used to indicate the free-energy function, which enables
defining the thermodynamic state of the entire system. The PIDL model is
initially developed for the three-dimensional case by generating synthetic data
from a classical constitutive model. The model is then trained by extracting
the data directly from cyclic loading-unloading experimental tests. Numerical
examples show that the PIDL model can accurately predict the mechanical
behavior of epoxy-based nanocomposites for different volume fractions of fibers
and nanoparticles under various hygrothermal conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2305.08102</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Super-Resolution of SOHO/MDI Magnetograms of Solar Active Regions Using
  SDO/HMI Data and an Attention-Aided Convolutional Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunhui Xu, Jason T. L. Wang, Haimin Wang, Haodi Jiang, Qin Li, Yasser Abduallah, Yan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image super-resolution has been an important subject in image processing and
recognition. Here, we present an attention-aided convolutional neural network
(CNN) for solar image super-resolution. Our method, named SolarCNN, aims to
enhance the quality of line-of-sight (LOS) magnetograms of solar active regions
(ARs) collected by the Michelson Doppler Imager (MDI) on board the Solar and
Heliospheric Observatory (SOHO). The ground-truth labels used for training
SolarCNN are the LOS magnetograms collected by the Helioseismic and Magnetic
Imager (HMI) on board the Solar Dynamics Observatory (SDO). Solar ARs consist
of strong magnetic fields in which magnetic energy can suddenly be released to
produce extreme space weather events, such as solar flares, coronal mass
ejections, and solar energetic particles. SOHO/MDI covers Solar Cycle 23, which
is stronger with more eruptive events than Cycle 24. Enhanced SOHO/MDI
magnetograms allow for better understanding and forecasting of violent events
of space weather. Experimental results show that SolarCNN improves the quality
of SOHO/MDI magnetograms in terms of the structural similarity index measure
(SSIM), Pearson's correlation coefficient (PCC), and the peak signal-to-noise
ratio (PSNR).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shrinivas Ramasubramanian, Harsh Rangwani, Sho Takemori, Kunal Samanta, Yuhei Umeda, Venkatesh Babu Radhakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise in internet usage has led to the generation of massive amounts of
data, resulting in the adoption of various supervised and semi-supervised
machine learning algorithms, which can effectively utilize the colossal amount
of data to train models. However, before deploying these models in the real
world, these must be strictly evaluated on performance measures like worst-case
recall and satisfy constraints such as fairness. We find that current
state-of-the-art empirical techniques offer sub-optimal performance on these
practical, non-decomposable performance objectives. On the other hand, the
theoretical techniques necessitate training a new model from scratch for each
performance objective. To bridge the gap, we propose SelMix, a selective
mixup-based inexpensive fine-tuning technique for pre-trained models, to
optimize for the desired objective. The core idea of our framework is to
determine a sampling distribution to perform a mixup of features between
samples from particular classes such that it optimizes the given objective. We
comprehensively evaluate our technique against the existing empirical and
theoretically principled methods on standard benchmark datasets for imbalanced
classification. We find that proposed SelMix fine-tuning significantly improves
the performance for various practical non-decomposable objectives across
benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 SpotLight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GeNet: A <span class="highlight-title">Graph Neural Network</span>-based Anti-noise Task-Oriented Semantic
  <span class="highlight-title">Communication</span> Paradigm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunhang Zheng, Kechao Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional approaches to semantic communication tasks rely on the knowledge
of the signal-to-noise ratio (SNR) to mitigate channel noise. However, these
methods necessitate training under specific SNR conditions, entailing
considerable time and computational resources. In this paper, we propose GeNet,
a Graph Neural Network (GNN)-based paradigm for semantic communication aimed at
combating noise, thereby facilitating Task-Oriented Communication (TOC). We
propose a novel approach where we first transform the input data image into
graph structures. Then we leverage a GNN-based encoder to extract semantic
information from the source data. This extracted semantic information is then
transmitted through the channel. At the receiver's end, a GNN-based decoder is
utilized to reconstruct the relevant semantic information from the source data
for TOC. Through experimental evaluation, we show GeNet's effectiveness in
anti-noise TOC while decoupling the SNR dependency. We further evaluate GeNet's
performance by varying the number of nodes, revealing its versatility as a new
paradigm for semantic communication. Additionally, we show GeNet's robustness
to geometric transformations by testing it with different rotation angles,
without resorting to data augmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-Shot Recalibration of Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Lisa Li, Urvashi Khandelwal, Kelvin Guu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has uncovered promising ways to extract well-calibrated
confidence estimates from language models (LMs), where the model's confidence
score reflects how likely it is to be correct. However, while LMs may appear
well-calibrated over broad distributions, this often hides significant
miscalibration within narrower slices (e.g., systemic over-confidence in math
can balance out systemic under-confidence in history, yielding perfect
calibration in aggregate). To attain well-calibrated confidence estimates for
any slice of a distribution, we propose a new framework for few-shot
slice-specific recalibration. Specifically, we train a recalibration model that
takes in a few unlabeled examples from any given slice and predicts a curve
that remaps confidence scores to be more accurate for that slice. Our trained
model can recalibrate for arbitrary new slices, without using any labeled data
from that slice. This enables us to identify domain-specific confidence
thresholds above which the LM's predictions can be trusted, and below which it
should abstain. Experiments show that our few-shot recalibrator consistently
outperforms existing calibration methods, for instance improving calibration
error for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clustering Change Sign Detection by Fusing Mixture Complexity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kento Urano, Ryo Yuki, Kenji Yamanishi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes an early detection method for cluster structural changes.
Cluster structure refers to discrete structural characteristics, such as the
number of clusters, when data are represented using finite mixture models, such
as Gaussian mixture models. We focused on scenarios in which the cluster
structure gradually changed over time. For finite mixture models, the concept
of mixture complexity (MC) measures the continuous cluster size by considering
the cluster proportion bias and overlap between clusters. In this paper, we
propose MC fusion as an extension of MC to handle situations in which multiple
mixture numbers are possible in a finite mixture model. By incorporating the
fusion of multiple models, our approach accurately captured the cluster
structure during transitional periods of gradual change. Moreover, we introduce
a method for detecting changes in the cluster structure by examining the
transition of MC fusion. We demonstrate the effectiveness of our method through
empirical analysis using both artificial and real-world datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DSF-GAN: Down<span class="highlight-title">Stream</span> Feedback Generative Adversarial Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oriel Perets, Nadav Rappoport
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Utility and privacy are two crucial measurements of the quality of synthetic
tabular data. While significant advancements have been made in privacy
measures, generating synthetic samples with high utility remains challenging.
To enhance the utility of synthetic samples, we propose a novel architecture
called the DownStream Feedback Generative Adversarial Network (DSF-GAN). This
approach incorporates feedback from a downstream prediction model during
training to augment the generator's loss function with valuable information.
Thus, DSF-GAN utilizes a downstream prediction task to enhance the utility of
synthetic samples. To evaluate our method, we tested it using two popular
datasets. Our experiments demonstrate improved model performance when training
on synthetic samples generated by DSF-GAN, compared to those generated by the
same GAN architecture without feedback. The evaluation was conducted on the
same validation set comprising real samples. All code and datasets used in this
research will be made openly available for ease of reproduction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Branch-Tuning: Balancing Stability and Plasticity for Continual
  Self-Supervised Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenzhuo Liu, Fei Zhu, Cheng-Lin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) has emerged as an effective paradigm for
deriving general representations from vast amounts of unlabeled data. However,
as real-world applications continually integrate new content, the high
computational and resource demands of SSL necessitate continual learning rather
than complete retraining. This poses a challenge in striking a balance between
stability and plasticity when adapting to new information. In this paper, we
employ Centered Kernel Alignment for quantitatively analyzing model stability
and plasticity, revealing the critical roles of batch normalization layers for
stability and convolutional layers for plasticity. Motivated by this, we
propose Branch-tuning, an efficient and straightforward method that achieves a
balance between stability and plasticity in continual SSL. Branch-tuning
consists of branch expansion and compression, and can be easily applied to
various SSL methods without the need of modifying the original methods,
retaining old data or models. We validate our method through incremental
experiments on various benchmark datasets, demonstrating its effectiveness and
practical value in real-world scenarios. We hope our work offers new insights
for future continual self-supervised learning research. The code will be made
publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond <span class="highlight-title">Embedding</span>s: The Promise of Visual Table in Multi-Modal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwu Zhong, Zi-Yuan Hu, Michael R. Lyu, Liwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual representation learning has been a cornerstone in computer vision,
evolving from supervised learning with human-annotated labels to aligning
image-text pairs from the Internet. Despite recent advancements in multi-modal
large language models (MLLMs), the visual representations they rely on, such as
CLIP embeddings, often lack access to external world knowledge critical for
real-world visual reasoning. In this work, we propose Visual Table, a novel
visual representation tailored for MLLMs. It provides hierarchical text
descriptions of holistic visual scenes, consisting of a scene description and
multiple object-centric descriptions that encompass categories, attributes, and
knowledge at instance level. We further develop a scalable generator for visual
table generation and train it on small-scale annotations from GPT4V. Extensive
evaluations demonstrate that, with generated visual tables as additional visual
representations, our model can consistently outperform the state-of-the-art
(SOTA) MLLMs across diverse benchmarks. When visual tables serve as standalone
visual representations, our model can closely match or even beat the SOTA MLLMs
that are built on CLIP visual embeddings. Our code is available at
https://github.com/LaVi-Lab/Visual-Table.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/LaVi-Lab/Visual-Table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion,
  Reconstruction, and Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruikai Cui, Weizhe Liu, Weixuan Sun, Senbo Wang, Taizhang Shang, Yang Li, Xibin Song, Han Yan, Zhennan Wu, Shenzhou Chen, Hongdong Li, Pan Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D shape generation aims to produce innovative 3D content adhering to
specific conditions and constraints. Existing methods often decompose 3D shapes
into a sequence of localized components, treating each element in isolation
without considering spatial consistency. As a result, these approaches exhibit
limited versatility in 3D data representation and shape generation, hindering
their ability to generate highly diverse 3D shapes that comply with the
specified constraints. In this paper, we introduce a novel spatial-aware 3D
shape generation framework that leverages 2D plane representations for enhanced
3D shape modeling. To ensure spatial coherence and reduce memory usage, we
incorporate a hybrid shape representation technique that directly learns a
continuous signed distance field representation of the 3D shape using
orthogonal 2D planes. Additionally, we meticulously enforce spatial
correspondences across distinct planes using a transformer-based autoencoder
structure, promoting the preservation of spatial relationships in the generated
3D shapes. This yields an algorithm that consistently outperforms
state-of-the-art 3D shape generation methods on various tasks, including
unconditional shape generation, multi-modal shape completion, single-view
reconstruction, and text-to-shape synthesis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Benchmark</span>ing Image Transformers for Prostate Cancer Detection from
  Ultrasound Data <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Harmanani, Paul F. R. Wilson, Fahimeh Fooladgar, Amoon Jamzad, Mahdi Gilany, Minh Nguyen Nhat To, Brian Wodlinger, Purang Abolmaesumi, Parvin Mousavi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  PURPOSE: Deep learning methods for classifying prostate cancer (PCa) in
ultrasound images typically employ convolutional networks (CNNs) to detect
cancer in small regions of interest (ROI) along a needle trace region. However,
this approach suffers from weak labelling, since the ground-truth
histopathology labels do not describe the properties of individual ROIs.
Recently, multi-scale approaches have sought to mitigate this issue by
combining the context awareness of transformers with a CNN feature extractor to
detect cancer from multiple ROIs using multiple-instance learning (MIL). In
this work, we present a detailed study of several image transformer
architectures for both ROI-scale and multi-scale classification, and a
comparison of the performance of CNNs and transformers for ultrasound-based
prostate cancer classification. We also design a novel multi-objective learning
strategy that combines both ROI and core predictions to further mitigate label
noise. METHODS: We evaluate 3 image transformers on ROI-scale cancer
classification, then use the strongest model to tune a multi-scale classifier
with MIL. We train our MIL models using our novel multi-objective learning
strategy and compare our results to existing baselines. RESULTS: We find that
for both ROI-scale and multi-scale PCa detection, image transformer backbones
lag behind their CNN counterparts. This deficit in performance is even more
noticeable for larger models. When using multi-objective learning, we can
improve performance of MIL, with a 77.9% AUROC, a sensitivity of 75.9%, and a
specificity of 66.3%. CONCLUSION: Convolutional networks are better suited for
modelling sparse datasets of prostate ultrasounds, producing more robust
features than transformers in PCa detection. Multi-scale methods remain the
best architecture for this task, with multi-objective learning presenting an
effective way to improve performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>early draft, 7 pages; Accepted to SPIE Medical Imaging 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fourier or Wavelet bases as counterpart self-attention in spikformer for
  efficient visual classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyu Wang, Duzhen Zhang, Tilelin Zhang, Bo Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Energy-efficient spikformer has been proposed by integrating the biologically
plausible spiking neural network (SNN) and artificial Transformer, whereby the
Spiking Self-Attention (SSA) is used to achieve both higher accuracy and lower
computational cost. However, it seems that self-attention is not always
necessary, especially in sparse spike-form calculation manners. In this paper,
we innovatively replace vanilla SSA (using dynamic bases calculating from Query
and Key) with spike-form Fourier Transform, Wavelet Transform, and their
combinations (using fixed triangular or wavelets bases), based on a key
hypothesis that both of them use a set of basis functions for information
transformation. Hence, the Fourier-or-Wavelet-based spikformer (FWformer) is
proposed and verified in visual classification tasks, including both static
image and event-based video datasets. The FWformer can achieve comparable or
even higher accuracies ($0.4\%$-$1.5\%$), higher running speed ($9\%$-$51\%$
for training and $19\%$-$70\%$ for inference), reduced theoretical energy
consumption ($20\%$-$25\%$), and reduced GPU memory usage ($4\%$-$26\%$),
compared to the standard spikformer. Our result indicates the continuous
refinement of new Transformers, that are inspired either by biological
discovery (spike-form), or information theory (Fourier or Wavelet Transform),
is promising.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 2 figures. arXiv admin note: substantial text overlap with
  arXiv:2308.02557</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Transformer-Based Framework for Payload Malware Detection and
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyle Stein, Arash Mahyari, Guillermo Francia III, Eman El-Sheikh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As malicious cyber threats become more sophisticated in breaching computer
networks, the need for effective intrusion detection systems (IDSs) becomes
crucial. Techniques such as Deep Packet Inspection (DPI) have been introduced
to allow IDSs analyze the content of network packets, providing more context
for identifying potential threats. IDSs traditionally rely on using
anomaly-based and signature-based detection techniques to detect unrecognized
and suspicious activity. Deep learning techniques have shown great potential in
DPI for IDSs due to their efficiency in learning intricate patterns from the
packet content being transmitted through the network. In this paper, we propose
a revolutionary DPI algorithm based on transformers adapted for the purpose of
detecting malicious traffic with a classifier head. Transformers learn the
complex content of sequence data and generalize them well to similar scenarios
thanks to their self-attention mechanism. Our proposed method uses the raw
payload bytes that represent the packet contents and is deployed as
man-in-the-middle. The payload bytes are used to detect malicious packets and
classify their types. Experimental results on the UNSW-NB15 and CIC-IOT23
datasets demonstrate that our transformer-based model is effective in
distinguishing malicious from benign traffic in the test dataset, attaining an
average accuracy of 79\% using binary classification and 72\% on the
multi-classification experiment, both using solely payload bytes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty-Aware Deployment of Pre-trained Language-Conditioned
  Imitation Learning Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18222v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18222v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Wu, Bruce D. Lee, Kostas Daniilidis, Bernadette Bucher, Nikolai Matni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale robotic policies trained on data from diverse tasks and robotic
platforms hold great promise for enabling general-purpose robots; however,
reliable generalization to new environment conditions remains a major
challenge. Toward addressing this challenge, we propose a novel approach for
uncertainty-aware deployment of pre-trained language-conditioned imitation
learning agents. Specifically, we use temperature scaling to calibrate these
models and exploit the calibrated model to make uncertainty-aware decisions by
aggregating the local information of candidate actions. We implement our
approach in simulation using three such pre-trained models, and showcase its
potential to significantly enhance task completion rates. The accompanying code
is accessible at the link:
https://github.com/BobWu1998/uncertainty_quant_all.git
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Two-Dimensional to Three-Dimensional Environment with Q-Learning:
  Modeling Autonomous Navigation with Reinforcement Learning and no Libraries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ergon Cugler de Moraes Silva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) algorithms have become indispensable tools in
artificial intelligence, empowering agents to acquire optimal decision-making
policies through interactions with their environment and feedback mechanisms.
This study explores the performance of RL agents in both two-dimensional (2D)
and three-dimensional (3D) environments, aiming to research the dynamics of
learning across different spatial dimensions. A key aspect of this
investigation is the absence of pre-made libraries for learning, with the
algorithm developed exclusively through computational mathematics. The
methodological framework centers on RL principles, employing a Q-learning agent
class and distinct environment classes tailored to each spatial dimension. The
research aims to address the question: How do reinforcement learning agents
adapt and perform in environments of varying spatial dimensions, particularly
in 2D and 3D settings? Through empirical analysis, the study evaluates agents'
learning trajectories and adaptation processes, revealing insights into the
efficacy of RL algorithms in navigating complex, multi-dimensional spaces.
Reflections on the findings prompt considerations for future research,
particularly in understanding the dynamics of learning in higher-dimensional
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Minimax Optimal Fair Classification with Bounded Demographic Disparity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18216v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18216v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianli Zeng, Guang Cheng, Edgar Dobriban
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mitigating the disparate impact of statistical machine learning methods is
crucial for ensuring fairness. While extensive research aims to reduce
disparity, the effect of using a \emph{finite dataset} -- as opposed to the
entire population -- remains unclear. This paper explores the statistical
foundations of fair binary classification with two protected groups, focusing
on controlling demographic disparity, defined as the difference in acceptance
rates between the groups. Although fairness may come at the cost of accuracy
even with infinite data, we show that using a finite sample incurs additional
costs due to the need to estimate group-specific acceptance thresholds. We
study the minimax optimal classification error while constraining demographic
disparity to a user-specified threshold. To quantify the impact of fairness
constraints, we introduce a novel measure called \emph{fairness-aware excess
risk} and derive a minimax lower bound on this measure that all classifiers
must satisfy. Furthermore, we propose FairBayes-DDP+, a group-wise thresholding
method with an offset that we show attains the minimax lower bound. Our lower
bound proofs involve several innovations. Experiments support that
FairBayes-DDP+ controls disparity at the user-specified level, while being
faster and having a more favorable fairness-accuracy tradeoff than several
baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A 4D Hybrid Algorithm to Scale Parallel Training to Thousands of GPUs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.13525v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.13525v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Singh, Prajwal Singhania, Aditya K. Ranjan, Zack Sating, Abhinav Bhatele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large communication costs are a critical bottleneck in training
state-of-the-art neural networks on distributed systems. This paper introduces
AxoNN, a novel four-dimensional (4D) parallelization approach, inspired by
Agarwal's algorithm for matrix multiplication, for parallelizing tensor
computations in deep learning, AxoNN employs two key strategies to minimize
communication overhead. First, we optimize communication by overlapping
expensive collective operations (reduce-scatter, all-gather, all-reduce) with
computations. Our experiments with a 20-billion parameter transformer model
demonstrate that these optimizations deliver nearly 53\% improvement. Second,
we present an analytical model to assist users in identifying
communication-minimizing configurations within the vast search space defined by
our 4D algorithm. This model empowers practitioners by simplifying the tuning
process for their specific training workloads. When training an 80-billion
parameter model on 1024 GPUs of Perlmutter, AxoNN surpasses Megatron-LM, a
state-of-the-art framework, by a significant 26%. Additionally, it achieves 57%
of the theoretical peak FLOP/s.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CrystalBox: Future-Based Explanations for Input-Driven Deep RL Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.13483v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.13483v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sagar Patel, Sangeetha Abdu Jyothi, Nina Narodytska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present CrystalBox, a novel, model-agnostic, posthoc explainability
framework for Deep Reinforcement Learning (DRL) controllers in the large family
of input-driven environments which includes computer systems. We combine the
natural decomposability of reward functions in input-driven environments with
the explanatory power of decomposed returns. We propose an efficient algorithm
to generate future-based explanations across both discrete and continuous
control environments. Using applications such as adaptive bitrate streaming and
congestion control, we demonstrate CrystalBox's capability to generate
high-fidelity explanations. We further illustrate its higher utility across
three practical use cases: contrastive explanations, network observability, and
guided reward design, as opposed to prior explainability techniques that
identify salient features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalization Bounds: Perspectives from Information Theory and
  PAC-Bayes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.04381v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.04381v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fredrik Hellström, Giuseppe Durisi, Benjamin Guedj, Maxim Raginsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental question in theoretical machine learning is generalization.
Over the past decades, the PAC-Bayesian approach has been established as a
flexible framework to address the generalization capabilities of machine
learning algorithms, and design new ones. Recently, it has garnered increased
interest due to its potential applicability for a variety of learning
algorithms, including deep neural networks. In parallel, an
information-theoretic view of generalization has developed, wherein the
relation between generalization and various information measures has been
established. This framework is intimately connected to the PAC-Bayesian
approach, and a number of results have been independently discovered in both
strands. In this monograph, we highlight this strong connection and present a
unified treatment of PAC-Bayesian and information-theoretic generalization
bounds. We present techniques and results that the two perspectives have in
common, and discuss the approaches and interpretations that differ. In
particular, we demonstrate how many proofs in the area share a modular
structure, through which the underlying ideas can be intuited. We pay special
attention to the conditional mutual information (CMI) framework; analytical
studies of the information complexity of learning algorithms; and the
application of the proposed methods to deep learning. This monograph is
intended to provide a comprehensive introduction to information-theoretic
generalization bounds and their connection to PAC-Bayes, serving as a
foundation from which the most recent developments are accessible. It is aimed
broadly towards researchers with an interest in generalization and theoretical
machine learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>228 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoupled Data Consistency with Diffusion Purification for Image
  Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06054v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06054v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Soo Min Kwon, Ismail R. Alkhouri, Saiprasad Ravishankar, Qing Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have recently gained traction as a powerful class of deep
generative priors, excelling in a wide range of image restoration tasks due to
their exceptional ability to model data distributions. To solve image
restoration problems, many existing techniques achieve data consistency by
incorporating additional likelihood gradient steps into the reverse sampling
process of diffusion models. However, the additional gradient steps pose a
challenge for real-world practical applications as they incur a large
computational overhead, thereby increasing inference time. They also present
additional difficulties when using accelerated diffusion model samplers, as the
number of data consistency steps is limited by the number of reverse sampling
steps. In this work, we propose a novel diffusion-based image restoration
solver that addresses these issues by decoupling the reverse process from the
data consistency steps. Our method involves alternating between a
reconstruction phase to maintain data consistency and a refinement phase that
enforces the prior via diffusion purification. Our approach demonstrates
versatility, making it highly adaptable for efficient problem-solving in latent
space. Additionally, it reduces the necessity for numerous sampling steps
through the integration of consistency models. The efficacy of our approach is
validated through comprehensive experiments across various image restoration
tasks, including image denoising, deblurring, inpainting, and super-resolution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedSN: A Novel <span class="highlight-title">Federated</span> Learning Framework over LEO Satellite Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01483v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01483v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Lin, Zhe Chen, Zihan Fang, Xianhao Chen, Xiong Wang, Yue Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, a large number of Low Earth Orbit (LEO) satellites have been
launched and deployed successfully in space by commercial companies, such as
SpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve
not only for communication but also for various machine learning applications,
such as space modulation recognition, remote sensing image classification, etc.
However, the ground station (GS) may be incapable of downloading such a large
volume of raw sensing data for centralized model training due to the limited
contact time with LEO satellites (e.g. 5 minutes). Therefore, federated
learning (FL) has emerged as the promising solution to address this problem via
on-device training. Unfortunately, to enable FL on LEO satellites, we still
face three critical challenges that are i) heterogeneous computing and memory
capabilities, ii) limited uplink rate, and iii) model staleness. To this end,
we propose FedSN as a general FL framework to tackle the above challenges, and
fully explore data diversity on LEO satellites. Specifically, we first present
a novel sub-structure scheme to enable heterogeneous local model training
considering different computing, memory, and communication constraints on LEO
satellites. Additionally, we propose a pseudo-synchronous model aggregation
strategy to dynamically schedule model aggregation for compensating model
staleness. To further demonstrate the effectiveness of the FedSN, we evaluate
it using space modulation recognition and remote sensing image classification
tasks by leveraging the data from real-world satellite networks. Extensive
experimental results demonstrate that FedSN framework achieves higher accuracy,
lower computing, and communication overhead than the state-of-the-art
benchmarks and the effectiveness of each components in FedSN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simplified Diffusion Schrödinger Bridge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14623v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14623v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhicong Tang, Tiankai Hang, Shuyang Gu, Dong Chen, Baining Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel theoretical simplification of the Diffusion
Schr\"odinger Bridge (DSB) that facilitates its unification with Score-based
Generative Models (SGMs), addressing the limitations of DSB in complex data
generation and enabling faster convergence and enhanced performance. By
employing SGMs as an initial solution for DSB, our approach capitalizes on the
strengths of both frameworks, ensuring a more efficient training process and
improving the performance of SGM. We also propose a reparameterization
technique that, despite theoretical approximations, practically improves the
network's fitting capabilities. Our extensive experimental evaluations confirm
the effectiveness of the simplified DSB, demonstrating its significant
improvements. We believe the contributions of this work pave the way for
advanced generative modeling. The code is available at
https://github.com/checkcrab/SDSB.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preventing Arbitrarily High Confidence on Far-Away Data in
  Point-Estimated Discriminative Neural Networks <span class="chip">AISTATS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.03683v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.03683v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Rashid, Serena Hacker, Guojun Zhang, Agustinus Kristiadi, Pascal Poupart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discriminatively trained, deterministic neural networks are the de facto
choice for classification problems. However, even though they achieve
state-of-the-art results on in-domain test sets, they tend to be overconfident
on out-of-distribution (OOD) data. For instance, ReLU networks - a popular
class of neural network architectures - have been shown to almost always yield
high confidence predictions when the test data are far away from the training
set, even when they are trained with OOD data. We overcome this problem by
adding a term to the output of the neural network that corresponds to the logit
of an extra class, that we design to dominate the logits of the original
classes as we move away from the training data.This technique provably prevents
arbitrarily high confidence on far-away test data while maintaining a simple
discriminative point-estimate training. Evaluation on various benchmarks
demonstrates strong performance against competitive baselines on both far-away
and realistic OOD data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AISTATS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03100v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03100v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeqian Ju, Yuancheng Wang, Kai Shen, Xu Tan, Detai Xin, Dongchao Yang, Yanqing Liu, Yichong Leng, Kaitao Song, Siliang Tang, Zhizheng Wu, Tao Qin, Xiang-Yang Li, Wei Ye, Shikun Zhang, Jiang Bian, Lei He, Jinyu Li, Sheng Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent large-scale text-to-speech (TTS) models have achieved
significant progress, they still fall short in speech quality, similarity, and
prosody. Considering speech intricately encompasses various attributes (e.g.,
content, prosody, timbre, and acoustic details) that pose significant
challenges for generation, a natural idea is to factorize speech into
individual subspaces representing different attributes and generate them
individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with
novel factorized diffusion models to generate natural speech in a zero-shot
way. Specifically, 1) we design a neural codec with factorized vector
quantization (FVQ) to disentangle speech waveform into subspaces of content,
prosody, timbre, and acoustic details; 2) we propose a factorized diffusion
model to generate attributes in each subspace following its corresponding
prompt. With this factorization design, NaturalSpeech 3 can effectively and
efficiently model intricate speech with disentangled subspaces in a
divide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the
state-of-the-art TTS systems on quality, similarity, prosody, and
intelligibility, and achieves on-par quality with human recordings.
Furthermore, we achieve better performance by scaling to 1B parameters and 200K
hours of training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Achieving human-level quality and naturalness on multi-speaker
  datasets (e.g., LibriSpeech) in a zero-shot way</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nesting Particle Filters for Experimental Design in Dynamical Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07868v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07868v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sahel Iqbal, Adrien Corenflos, Simo Särkkä, Hany Abdulsamad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel approach to Bayesian experimental design
for non-exchangeable data that formulates it as risk-sensitive policy
optimization. We develop the Inside-Out SMC$^2$ algorithm, a nested sequential
Monte Carlo technique to infer optimal designs, and embed it into a particle
Markov chain Monte Carlo framework to perform gradient-based policy
amortization. Our approach is distinct from other amortized experimental design
techniques, as it does not rely on contrastive estimators. Numerical validation
on a set of dynamical systems showcases the efficacy of our method in
comparison to other state-of-the-art strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive Review of Community Detection in Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11798v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11798v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiakang Li, Songning Lai, Zhihao Shuai, Yuan Tan, Yifan Jia, Mianyang Yu, Zichen Song, Xiaokang Peng, Ziyang Xu, Yongxin Ni, Haifeng Qiu, Jiayu Yang, Yutong Liu, Yonggang Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The study of complex networks has significantly advanced our understanding of
community structures which serves as a crucial feature of real-world graphs.
Detecting communities in graphs is a challenging problem with applications in
sociology, biology, and computer science. Despite the efforts of an
interdisciplinary community of scientists, a satisfactory solution to this
problem has not yet been achieved. This review article delves into the topic of
community detection in graphs, which serves as a thorough exposition of various
community detection methods from perspectives of modularity-based method,
spectral clustering, probabilistic modelling, and deep learning. Along with the
methods, a new community detection method designed by us is also presented.
Additionally, the performance of these methods on the datasets with and without
ground truth is compared. In conclusion, this comprehensive review provides a
deep understanding of community detection in graphs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Input Convex Lipschitz RNN: A Fast and Robust Approach for Engineering
  Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07494v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07494v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Wang, P S Pravin, Zhe Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational efficiency and non-adversarial robustness are critical factors
in real-world engineering applications. Yet, conventional neural networks often
fall short in addressing both simultaneously, or even separately. Drawing
insights from natural physical systems and existing literature, it is known
that an input convex architecture enhances computational efficiency, while a
Lipschitz-constrained architecture bolsters non-adversarial robustness. By
leveraging the strengths of convexity and Lipschitz continuity, we develop a
novel network architecture, termed Input Convex Lipschitz Recurrent Neural
Networks. This model is explicitly designed for fast and robust
optimization-based tasks and outperforms existing recurrent units across a
spectrum of engineering tasks in terms of computational efficiency and
non-adversarial robustness, including real-world solar irradiance prediction
for Solar PV system planning at LHT Holdings in Singapore and real-time Model
Predictive Control optimization for a nonlinear chemical reactor.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and
  Ethics) Evaluation: A Review 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03123v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03123v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunder Ali Khowaja, Parus Khuwaja, Kapal Dev, Weizheng Wang, Lewis Nkenyereye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT is another large language model (LLM) vastly available for the
consumers on their devices but due to its performance and ability to converse
effectively, it has gained a huge popularity amongst research as well as
industrial community. Recently, many studies have been published to show the
effectiveness, efficiency, integration, and sentiments of chatGPT and other
LLMs. In contrast, this study focuses on the important aspects that are mostly
overlooked, i.e. sustainability, privacy, digital divide, and ethics and
suggests that not only chatGPT but every subsequent entry in the category of
conversational bots should undergo Sustainability, PrivAcy, Digital divide, and
Ethics (SPADE) evaluation. This paper discusses in detail the issues and
concerns raised over chatGPT in line with aforementioned characteristics. We
also discuss the recent EU AI Act briefly in accordance with the SPADE
evaluation. We support our hypothesis by some preliminary data collection and
visualizations along with hypothesized facts. We also suggest mitigations and
recommendations for each of the concerns. Furthermore, we also suggest some
policies and recommendations for EU AI policy act concerning ethics, digital
divide, and sustainability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 8 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Empowering Data Mesh with <span class="highlight-title">Federated</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17878v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17878v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyuan Li, Salman Toor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution of data architecture has seen the rise of data lakes, aiming to
solve the bottlenecks of data management and promote intelligent
decision-making. However, this centralized architecture is limited by the
proliferation of data sources and the growing demand for timely analysis and
processing. A new data paradigm, Data Mesh, is proposed to overcome these
challenges. Data Mesh treats domains as a first-class concern by distributing
the data ownership from the central team to each data domain, while keeping the
federated governance to monitor domains and their data products. Many
multi-million dollar organizations like Paypal, Netflix, and Zalando have
already transformed their data analysis pipelines based on this new
architecture. In this decentralized architecture where data is locally
preserved by each domain team, traditional centralized machine learning is
incapable of conducting effective analysis across multiple domains, especially
for security-sensitive organizations. To this end, we introduce a pioneering
approach that incorporates Federated Learning into Data Mesh. To the best of
our knowledge, this is the first open-source applied work that represents a
critical advancement toward the integration of federated learning methods into
the Data Mesh paradigm, underscoring the promising prospects for
privacy-preserving and decentralized data analysis strategies within Data Mesh
architecture.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Stochastic Approximation with Delayed Updates: Finite-Time Rates under
  Markovian Sampling <span class="chip">AISTATS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11800v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11800v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arman Adibi, Nicolo Dal Fabbro, Luca Schenato, Sanjeev Kulkarni, <span class="highlight-author">H. Vincent Poor</span>, George J. Pappas, Hamed Hassani, Aritra Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by applications in large-scale and multi-agent reinforcement
learning, we study the non-asymptotic performance of stochastic approximation
(SA) schemes with delayed updates under Markovian sampling. While the effect of
delays has been extensively studied for optimization, the manner in which they
interact with the underlying Markov process to shape the finite-time
performance of SA remains poorly understood. In this context, our first main
contribution is to show that under time-varying bounded delays, the delayed SA
update rule guarantees exponentially fast convergence of the \emph{last
iterate} to a ball around the SA operator's fixed point. Notably, our bound is
\emph{tight} in its dependence on both the maximum delay $\tau_{max}$, and the
mixing time $\tau_{mix}$. To achieve this tight bound, we develop a novel
inductive proof technique that, unlike various existing delayed-optimization
analyses, relies on establishing uniform boundedness of the iterates. As such,
our proof may be of independent interest. Next, to mitigate the impact of the
maximum delay on the convergence rate, we provide the first finite-time
analysis of a delay-adaptive SA scheme under Markovian sampling. In particular,
we show that the exponent of convergence of this scheme gets scaled down by
$\tau_{avg}$, as opposed to $\tau_{max}$ for the vanilla delayed SA rule; here,
$\tau_{avg}$ denotes the average delay across all iterations. Moreover, the
adaptive scheme requires no prior knowledge of the delay sequence for step-size
tuning. Our theoretical findings shed light on the finite-time effects of
delays for a broad class of algorithms, including TD learning, Q-learning, and
stochastic gradient descent under Markovian sampling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 27th International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2024!</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Negative Evidential Deep Learning for Open-set Semi-supervised
  Learning <span class="chip">AAAI2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12091v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12091v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Yu, Danruo Deng, Furui Liu, Yueming Jin, Qi Dou, Guangyong Chen, Pheng-Ann Heng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning (SSL) methods assume that labeled data, unlabeled
data and test data are from the same distribution. Open-set semi-supervised
learning (Open-set SSL) considers a more practical scenario, where unlabeled
data and test data contain new categories (outliers) not observed in labeled
data (inliers). Most previous works focused on outlier detection via binary
classifiers, which suffer from insufficient scalability and inability to
distinguish different types of uncertainty. In this paper, we propose a novel
framework, Adaptive Negative Evidential Deep Learning (ANEDL) to tackle these
limitations. Concretely, we first introduce evidential deep learning (EDL) as
an outlier detector to quantify different types of uncertainty, and design
different uncertainty metrics for self-training and inference. Furthermore, we
propose a novel adaptive negative optimization strategy, making EDL more
tailored to the unlabeled dataset containing both inliers and outliers. As
demonstrated empirically, our proposed method outperforms existing
state-of-the-art methods across four datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guided Distant Supervision for Multilingual Relation Extraction Data:
  Adapting to a New Language <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17143v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17143v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alistair Plum, Tharindu Ranasinghe, Christoph Purschke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relation extraction is essential for extracting and understanding
biographical information in the context of digital humanities and related
subjects. There is a growing interest in the community to build datasets
capable of training machine learning models to extract relationships. However,
annotating such datasets can be expensive and time-consuming, in addition to
being limited to English. This paper applies guided distant supervision to
create a large biographical relationship extraction dataset for German. Our
dataset, composed of more than 80,000 instances for nine relationship types, is
the largest biographical German relationship extraction dataset. We also create
a manually annotated dataset with 2000 instances to evaluate the models and
release it together with the dataset compiled using guided distant supervision.
We train several state-of-the-art machine learning models on the automatically
created dataset and release them as well. Furthermore, we experiment with
multilingual and cross-lingual experiments that could benefit many low-resource
languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024 (The 2024 Joint International Conference
  on Computational Linguistics, Language Resources and Evaluation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Byzantine-resilient <span class="highlight-title">Federated</span> Learning With Adaptivity to Data
  Heterogeneity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13374v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13374v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyuan Zuo, Xingrun Yan, Rongfei Fan, Han Hu, Hangguan Shan, Tony Q. S. Quek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper deals with federated learning (FL) in the presence of malicious
Byzantine attacks and data heterogeneity. A novel Robust Average Gradient
Algorithm (RAGA) is proposed, which leverages the geometric median for
aggregation and can freely select the round number for local updating.
Different from most existing resilient approaches, which perform convergence
analysis based on strongly-convex loss function or homogeneously distributed
dataset, we conduct convergence analysis for not only strongly-convex but also
non-convex loss function over heterogeneous dataset. According to our
theoretical analysis, as long as the fraction of dataset from malicious users
is less than half, RAGA can achieve convergence at rate
$\mathcal{O}({1}/{T^{2/3- \delta}})$ where $T$ is the iteration number and
$\delta \in (0, 2/3)$ for non-convex loss function, and at linear rate for
strongly-convex loss function. Moreover, stationary point or global optimal
solution is proved to obtainable as data heterogeneity vanishes. Experimental
results corroborate the robustness of RAGA to Byzantine attacks and verifies
the advantage of RAGA over baselines on convergence performance under various
intensity of Byzantine attacks, for heterogeneous dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Demystifying Misconceptions in Social Bots Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17251v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17251v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefano Cresci, Kai-Cheng Yang, Angelo Spognardi, Roberto Di Pietro, Filippo Menczer, Marinella Petrocchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research on social bots aims at advancing knowledge and providing solutions
to one of the most debated forms of online manipulation. Yet, social bot
research is plagued by widespread biases, hyped results, and misconceptions
that set the stage for ambiguities, unrealistic expectations, and seemingly
irreconcilable findings. Overcoming such issues is instrumental towards
ensuring reliable solutions and reaffirming the validity of the scientific
method. In this contribution, we review some recent results in social bots
research, highlighting and revising factual errors as well as methodological
and conceptual biases. More importantly, we demystify common misconceptions,
addressing fundamental points on how social bots research is discussed. Our
analysis surfaces the need to discuss research about online disinformation and
manipulation in a rigorous, unbiased, and responsible way. This article
bolsters such effort by identifying and refuting common fallacious arguments
used by both proponents and opponents of social bots research, as well as
providing directions toward sound methodologies for future research in the
field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LCANets++: Robust Audio Classification using Multi-layer Neural Networks
  with Lateral Competition <span class="chip">ICASSP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12882v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12882v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sayanton V. Dibbo, Juston S. Moore, Garrett T. Kenyon, Michael A. Teti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio classification aims at recognizing audio signals, including speech
commands or sound events. However, current audio classifiers are susceptible to
perturbations and adversarial attacks. In addition, real-world audio
classification tasks often suffer from limited labeled data. To help bridge
these gaps, previous work developed neuro-inspired convolutional neural
networks (CNNs) with sparse coding via the Locally Competitive Algorithm (LCA)
in the first layer (i.e., LCANets) for computer vision. LCANets learn in a
combination of supervised and unsupervised learning, reducing dependency on
labeled samples. Motivated by the fact that auditory cortex is also sparse, we
extend LCANets to audio recognition tasks and introduce LCANets++, which are
CNNs that perform sparse coding in multiple layers via LCA. We demonstrate that
LCANets++ are more robust than standard CNNs and LCANets against perturbations,
e.g., background noise, as well as black-box and white-box attacks, e.g.,
evasion and fast gradient sign (FGSM) attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2024 IEEE International Conference on Acoustics, Speech
  and Signal Processing Workshops (ICASSPW)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepMachining: Online <span class="highlight-title">Predict</span>ion of Machining Errors of Lathe Machines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16451v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16451v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang-Li Lu, Hwai-Jung Hsu, Che-Wei Chou, H. T. Kung, Chen-Hsin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe DeepMachining, a deep learning-based AI system for online
prediction of machining errors of lathe machine operations. We have built and
evaluated DeepMachining based on manufacturing data from factories.
Specifically, we first pretrain a deep learning model for a given lathe
machine's operations to learn the salient features of machining states. Then,
we fine-tune the pretrained model to adapt to specific machining tasks. We
demonstrate that DeepMachining achieves high prediction accuracy for multiple
tasks that involve different workpieces and cutting tools. To the best of our
knowledge, this work is one of the first factory experiments using pre-trained
deep-learning models to predict machining errors of lathe machines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recurrent Action Transformer with Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.09459v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.09459v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexey Staroverov, Egor Cherepanov, Dmitry Yudin, Alexey K. Kovalev, Aleksandr I. Panov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the use of transformers in offline reinforcement learning has
become a rapidly developing area. This is due to their ability to treat the
agent's trajectory in the environment as a sequence, thereby reducing the
policy learning problem to sequence modeling. In environments where the agent's
decisions depend on past events, it is essential to capture both the event
itself and the decision point in the context of the model. However, the
quadratic complexity of the attention mechanism limits the potential for
context expansion. One solution to this problem is to enhance transformers with
memory mechanisms. In this paper, we propose the Recurrent Action Transformer
with Memory (RATE) - a model that incorporates recurrent memory. To evaluate
our model, we conducted extensive experiments on both memory-intensive
environments (VizDoom-Two-Color, T-Maze) and classic Atari games and MuJoCo
control environments. The results show that the use of memory can significantly
improve performance in memory-intensive environments while maintaining or
improving results in classic environments. We hope that our findings will
stimulate research on memory mechanisms for transformers applicable to offline
reinforcement learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Pre-Training of Time-Series Data for Unsupervised Fault
  Detection in Semiconductor Manufacturing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sewoong Lee, JinKyou Choi, Min Su Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces TRACE-GPT, which stands for Time-seRies
Anomaly-detection with Convolutional Embedding and Generative Pre-trained
Transformers. TRACE-GPT is designed to pre-train univariate time-series sensor
data and detect faults on unlabeled datasets in semiconductor manufacturing. In
semiconductor industry, classifying abnormal time-series sensor data from
normal data is important because it is directly related to wafer defect.
However, small, unlabeled, and even mixed training data without enough
anomalies make classification tasks difficult. In this research, we capture
features of time-series data with temporal convolutional embedding and
Generative Pre-trained Transformer (GPT) to classify abnormal sequences from
normal sequences using cross entropy loss. We prove that our model shows better
performance than previous unsupervised models with both an open dataset, the
University of California Riverside (UCR) time-series classification archive,
and the process log of our Chemical Vapor Deposition (CVD) equipment. Our model
has the highest F1 score at Equal Error Rate (EER) across all datasets and is
only 0.026 below the supervised state-of-the-art baseline on the open dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attacks, Defenses and Evaluations for LLM Conversation Safety: A <span class="highlight-title">Survey</span> <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09283v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09283v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichen Dong, Zhanhui Zhou, Chao Yang, Jing Shao, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are now commonplace in conversation
applications. However, their risks of misuse for generating harmful responses
have raised serious societal concerns and spurred recent research on LLM
conversation safety. Therefore, in this survey, we provide a comprehensive
overview of recent studies, covering three critical aspects of LLM conversation
safety: attacks, defenses, and evaluations. Our goal is to provide a structured
summary that enhances understanding of LLM conversation safety and encourages
further investigation into this important subject. For easy reference, we have
categorized all the studies mentioned in this survey according to our taxonomy,
available at: https://github.com/niconi19/LLM-conversation-safety.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ No-Regret Learning in Bilateral Trade via Global Budget Balance <span class="chip">STOC 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12370v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12370v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martino Bernasconi, Matteo Castiglioni, Andrea Celli, Federico Fusco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bilateral trade models the problem of intermediating between two rational
agents -- a seller and a buyer -- both characterized by a private valuation for
an item they want to trade. We study the online learning version of the
problem, in which at each time step a new seller and buyer arrive and the
learner has to set prices for them without any knowledge about their
(adversarially generated) valuations.
  In this setting, known impossibility results rule out the existence of
no-regret algorithms when budget balanced has to be enforced at each time step.
In this paper, we introduce the notion of \emph{global budget balance}, which
only requires the learner to fulfill budget balance over the entire time
horizon. Under this natural relaxation, we provide the first no-regret
algorithms for adversarial bilateral trade under various feedback models.
First, we show that in the full-feedback model, the learner can guarantee
$\tilde O(\sqrt{T})$ regret against the best fixed prices in hindsight, and
that this bound is optimal up to poly-logarithmic terms. Second, we provide a
learning algorithm guaranteeing a $\tilde O(T^{3/4})$ regret upper bound with
one-bit feedback, which we complement with a $\Omega(T^{5/7})$ lower bound that
holds even in the two-bit feedback model. Finally, we introduce and analyze an
alternative benchmark that is provably stronger than the best fixed prices in
hindsight and is inspired by the literature on bandits with knapsacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at STOC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Few-Shot Detection of Machine-Generated Text using Style Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06712v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06712v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Rivera Soto, Kailin Koch, Aleem Khan, Barry Chen, Marcus Bishop, Nicholas Andrews
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of instruction-tuned language models that convincingly mimic human
writing poses a significant risk of abuse. However, such abuse may be
counteracted with the ability to detect whether a piece of text was composed by
a language model rather than a human author. Some previous approaches to this
problem have relied on supervised methods by training on corpora of confirmed
human- and machine- written documents. Unfortunately, model under-specification
poses an unavoidable challenge for neural network-based detectors, making them
brittle in the face of data shifts, such as the release of newer language
models producing still more fluent text than the models used to train the
detectors. Other approaches require access to the models that may have
generated a document in question, which is often impractical. In light of these
challenges, we pursue a fundamentally different approach not relying on samples
from language models of concern at training time. Instead, we propose to
leverage representations of writing style estimated from human-authored text.
Indeed, we find that features effective at distinguishing among human authors
are also effective at distinguishing human from machine authors, including
state-of-the-art large language models like Llama-2, ChatGPT, and GPT-4.
Furthermore, given a handful of examples composed by each of several specific
language models of interest, our approach affords the ability to predict which
model generated a given document. The code and data to reproduce our
experiments are available at
https://github.com/LLNL/LUAR/tree/main/fewshot_iclr2024.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ABScribe: Rapid Exploration & Organization of Multiple Writing
  Variations in Human-AI Co-Writing Tasks using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00117v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00117v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohi Reza, Nathan Laundry, Ilya Musabirov, Peter Dushniku, Zhi Yuan "Michael" Yu, Kashish Mittal, Tovi Grossman, Michael Liut, Anastasia Kuzminykh, Joseph Jay Williams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploring alternative ideas by rewriting text is integral to the writing
process. State-of-the-art Large Language Models (LLMs) can simplify writing
variation generation. However, current interfaces pose challenges for
simultaneous consideration of multiple variations: creating new variations
without overwriting text can be difficult, and pasting them sequentially can
clutter documents, increasing workload and disrupting writers' flow. To tackle
this, we present ABScribe, an interface that supports rapid, yet visually
structured, exploration and organization of writing variations in human-AI
co-writing tasks. With ABScribe, users can swiftly modify variations using LLM
prompts, which are auto-converted into reusable buttons. Variations are stored
adjacently within text fields for rapid in-place comparisons using mouse-over
interactions on a popup toolbar. Our user study with 12 writers shows that
ABScribe significantly reduces task workload (d = 1.20, p < 0.001), enhances
user perceptions of the revision process (d = 2.41, p < 0.001) compared to a
popular baseline workflow, and provides insights into how writers explore
variations using LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CHI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label
  Learning <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10365v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10365v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyu Tian, Hongxin Wei, Yiqun Wang, Lei Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partial-label learning (PLL) is an important weakly supervised learning
problem, which allows each training example to have a candidate label set
instead of a single ground-truth label. Identification-based methods have been
widely explored to tackle label ambiguity issues in PLL, which regard the true
label as a latent variable to be identified. However, identifying the true
labels accurately and completely remains challenging, causing noise in pseudo
labels during model training. In this paper, we propose a new method called
CroSel, which leverages historical predictions from the model to identify true
labels for most training examples. First, we introduce a cross selection
strategy, which enables two deep models to select true labels of partially
labeled data for each other. Besides, we propose a novel consistency
regularization term called co-mix to avoid sample waste and tiny noise caused
by false selection. In this way, CroSel can pick out the true labels of most
examples with high precision. Extensive experiments demonstrate the superiority
of CroSel, which consistently outperforms previous state-of-the-art methods on
benchmark datasets. Additionally, our method achieves over 90\% accuracy and
quantity for selecting true labels on CIFAR-type datasets under various
settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine Learning Optimized Orthogonal Basis Piecewise Polynomial
  Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08579v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08579v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannes Waclawek, Stefan Huber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Piecewise Polynomials (PPs) are utilized in several engineering disciplines,
like trajectory planning, to approximate position profiles given in the form of
a set of points. While the approximation target along with domain-specific
requirements, like Ck -continuity, can be formulated as a system of equations
and a result can be computed directly, such closed-form solutions posses
limited flexibility with respect to polynomial degrees, polynomial bases or
adding further domain-specific requirements. Sufficiently complex optimization
goals soon call for the use of numerical methods, like gradient descent. Since
gradient descent lies at the heart of training Artificial Neural Networks
(ANNs), modern Machine Learning (ML) frameworks like TensorFlow come with a set
of gradient-based optimizers potentially suitable for a wide range of
optimization problems beyond the training task for ANNs. Our approach is to
utilize the versatility of PP models and combine it with the potential of
modern ML optimizers for the use in function approximation in 1D trajectory
planning in the context of electronic cam design. We utilize available
optimizers of the ML framework TensorFlow directly, outside of the scope of
ANNs, to optimize model parameters of our PP model. In this paper, we show how
an orthogonal polynomial basis contributes to improving approximation and
continuity optimization performance. Utilizing Chebyshev polynomials of the
first kind, we develop a novel regularization approach enabling clearly
improved convergence behavior. We show that, using this regularization
approach, Chebyshev basis performs better than power basis for all relevant
optimizers in the combined approximation and continuity optimization setting
and demonstrate usability of the presented approach within the electronic cam
domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to LION18</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Challenging Common Paradigms in Multi-Task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.04698v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.04698v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cathrin Elich, Lukas Kirchdorfer, Jan M. Köhler, Lukas Schott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While multi-task learning (MTL) has gained significant attention in recent
years, its underlying mechanisms remain poorly understood. Recent methods did
not yield consistent performance improvements over single task learning (STL)
baselines, underscoring the importance of gaining more profound insights about
challenges specific to MTL. In our study, we challenge paradigms in MTL in the
context of STL: First, the impact of the choice of optimizer has only been
mildly investigated in MTL. We show the pivotal role of common STL tools such
as the Adam optimizer in MTL empirically in various experiments. To further
investigate Adam's effectiveness, we theoretical derive a partial loss-scale
invariance under mild assumptions. Second, the notion of gradient conflicts has
often been phrased as a specific problem in MTL. We delve into the role of
gradient conflicts in MTL and compare it to STL. For angular gradient alignment
we find no evidence that this is a unique problem in MTL. We emphasize
differences in gradient magnitude as the main distinguishing factor. Lastly, we
compare the transferability of features learned through MTL and STL on common
image corruptions, and find light evidence that MTL can lead to superior
transferability. Overall, we find surprising similarities between STL and MTL
suggesting to consider methods from both fields in a broader context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>-</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose
  Estimation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12028v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12028v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Li, Mengyuan Liu, Hong Liu, Pichao Wang, Jialun Cai, Nicu Sebe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have been successfully applied in the field of video-based 3D
human pose estimation. However, the high computational costs of these video
pose transformers (VPTs) make them impractical on resource-constrained devices.
In this paper, we present a plug-and-play pruning-and-recovering framework,
called Hourglass Tokenizer (HoT), for efficient transformer-based 3D human pose
estimation from videos. Our HoT begins with pruning pose tokens of redundant
frames and ends with recovering full-length tokens, resulting in a few pose
tokens in the intermediate transformer blocks and thus improving the model
efficiency. To effectively achieve this, we propose a token pruning cluster
(TPC) that dynamically selects a few representative tokens with high semantic
diversity while eliminating the redundancy of video frames. In addition, we
develop a token recovering attention (TRA) to restore the detailed
spatio-temporal information based on the selected tokens, thereby expanding the
network output to the original full-length temporal resolution for fast
inference. Extensive experiments on two benchmark datasets (i.e., Human3.6M and
MPI-INF-3DHP) demonstrate that our method can achieve both high efficiency and
estimation accuracy compared to the original VPT models. For instance, applying
to MotionBERT and MixSTE on Human3.6M, our HoT can save nearly 50% FLOPs
without sacrificing accuracy and nearly 40% FLOPs with only 0.2% accuracy drop,
respectively. Code and models are available at
https://github.com/NationalGAILab/HoT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024, Open Sourced</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Limit Order Book <span class="highlight-title">Forecast</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09267v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09267v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Briola, Silvia Bartolucci, Tomaso Aste
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We exploit cutting-edge deep learning methodologies to explore the
predictability of high-frequency Limit Order Book mid-price changes for a
heterogeneous set of stocks traded on the NASDAQ exchange. In so doing, we
release `LOBFrame', an open-source code base to efficiently process large-scale
Limit Order Book data and quantitatively assess state-of-the-art deep learning
models' forecasting capabilities. Our results are twofold. We demonstrate that
the stocks' microstructural characteristics influence the efficacy of deep
learning methods and that their high forecasting power does not necessarily
correspond to actionable trading signals. We argue that traditional machine
learning metrics fail to adequately assess the quality of forecasts in the
Limit Order Book context. As an alternative, we propose an innovative
operational framework that evaluates predictions' practicality by focusing on
the probability of accurately forecasting complete transactions. This work
offers academics and practitioners an avenue to make informed and robust
decisions on the application of deep learning techniques, their scope and
limitations, effectively exploiting emergent statistical properties of the
Limit Order Book.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, 14 figures, 12 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01739v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01739v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, Yang You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To help the open-source community have a better understanding of
Mixture-of-Experts (MoE) based large language models (LLMs), we train and
release OpenMoE, a series of fully open-sourced and reproducible decoder-only
MoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T
tokens. Our investigation confirms that MoE-based LLMs can offer a more
favorable cost-effectiveness trade-off than dense LLMs, highlighting the
potential effectiveness for future LLM development.
  One more important contribution of this study is an in-depth analysis of the
routing mechanisms within our OpenMoE models, leading to three significant
findings: Context-Independent Specialization, Early Routing Learning, and
Drop-towards-the-End. We discovered that routing decisions in MoE models are
predominantly based on token IDs, with minimal context relevance. The
token-to-expert assignments are determined early in the pre-training phase and
remain largely unchanged. This imperfect routing can result in performance
degradation, particularly in sequential tasks like multi-turn conversations,
where tokens appearing later in a sequence are more likely to be dropped.
Finally, we rethink our design based on the above-mentioned observations and
analysis. To facilitate future MoE LLM development, we propose potential
strategies for mitigating the issues we found and further improving
off-the-shelf MoE LLM designs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VIGraph: Generative Self-supervised Learning for Class-Imbalanced Node
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01191v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01191v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulan Hu, Sheng Ouyang, Zhirui Yang, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class imbalance in graph data presents significant challenges for node
classification. While existing methods, such as SMOTE-based approaches,
partially mitigate this issue, they still exhibit limitations in constructing
imbalanced graphs. Generative self-supervised learning (SSL) methods,
exemplified by graph autoencoders (GAEs), offer a promising solution by
directly generating minority nodes from the data itself, yet their potential
remains underexplored. In this paper, we delve into the shortcomings of
SMOTE-based approaches in the construction of imbalanced graphs. Furthermore,
we introduce VIGraph, a simple yet effective generative SSL approach that
relies on the Variational GAE as the fundamental model. VIGraph strictly
adheres to the concept of imbalance when constructing imbalanced graphs and
innovatively leverages the variational inference (VI) ability of Variational
GAE to generate nodes for minority classes. VIGraph introduces comprehensive
training strategies, including cross-view contrastive learning at the decoding
phase to capture semantic knowledge, adjacency matrix reconstruction to
preserve graph structure, and alignment strategy to ensure stable training.
VIGraph can generate high-quality nodes directly usable for classification,
eliminating the need to integrate the generated nodes back to the graph as well
as additional retraining found in SMOTE-based methods. We conduct extensive
experiments, results from which demonstrate the superiority and generality of
our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A2V: A Semi-Supervised Domain Adaptation Framework for Brain Vessel
  Segmentation via Two-Phase Training Angiography-to-Venography Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.06075v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.06075v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Galati, Daniele Falcetta, Rosa Cortese, Barbara Casolla, Ferran Prados, Ninon Burgos, Maria A. Zuluaga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a semi-supervised domain adaptation framework for brain vessel
segmentation from different image modalities. Existing state-of-the-art methods
focus on a single modality, despite the wide range of available cerebrovascular
imaging techniques. This can lead to significant distribution shifts that
negatively impact the generalization across modalities. By relying on annotated
angiographies and a limited number of annotated venographies, our framework
accomplishes image-to-image translation and semantic segmentation, leveraging a
disentangled and semantically rich latent space to represent heterogeneous data
and perform image-level adaptation from source to target domains. Moreover, we
reduce the typical complexity of cycle-based architectures and minimize the use
of adversarial training, which allows us to build an efficient and intuitive
model with stable training. We evaluate our method on magnetic resonance
angiographies and venographies. While achieving state-of-the-art performance in
the source domain, our method attains a Dice score coefficient in the target
domain that is only 8.9% lower, highlighting its promising potential for robust
cerebrovascular image segmentation across different modalities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 34th British Machine Vision Conference (BMVC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Planning to Go Out-of-Distribution in Offline-to-Online Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05723v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05723v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trevor McInroe, Adam Jelley, Stefano V. Albrecht, Amos Storkey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline pretraining with a static dataset followed by online fine-tuning
(offline-to-online, or OtO) is a paradigm well matched to a real-world RL
deployment process. In this scenario, we aim to find the best-performing policy
within a limited budget of online interactions. Previous work in the OtO
setting has focused on correcting for bias introduced by the policy-constraint
mechanisms of offline RL algorithms. Such constraints keep the learned policy
close to the behavior policy that collected the dataset, but we show this can
unnecessarily limit policy performance if the behavior policy is far from
optimal. Instead, we forgo constraints and frame OtO RL as an exploration
problem that aims to maximize the benefit of online data-collection. We first
study the major online RL exploration methods based on intrinsic rewards and
UCB in the OtO setting, showing that intrinsic rewards add training instability
through reward-function modification, and UCB methods are myopic and it is
unclear which learned-component's ensemble to use for action selection. We then
introduce an algorithm for planning to go out-of-distribution (PTGOOD) that
avoids these issues. PTGOOD uses a non-myopic planning procedure that targets
exploration in relatively high-reward regions of the state-action space
unlikely to be visited by the behavior policy. By leveraging concepts from the
Conditional Entropy Bottleneck, PTGOOD encourages data collected online to
provide new information relevant to improving the final deployment policy
without altering rewards. We show empirically in several continuous control
tasks that PTGOOD significantly improves agent returns during online
fine-tuning and avoids the suboptimal policy convergence that many of our
baselines exhibit in several environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 17 figures, preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ World Models via Policy-Guided Trajectory Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.08533v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.08533v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marc Rigter, Jun Yamada, Ingmar Posner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  World models are a powerful tool for developing intelligent agents. By
predicting the outcome of a sequence of actions, world models enable policies
to be optimised via on-policy reinforcement learning (RL) using synthetic data,
i.e. in "in imagination". Existing world models are autoregressive in that they
interleave predicting the next state with sampling the next action from the
policy. Prediction error inevitably compounds as the trajectory length grows.
In this work, we propose a novel world modelling approach that is not
autoregressive and generates entire on-policy trajectories in a single pass
through a diffusion model. Our approach, Policy-Guided Trajectory Diffusion
(PolyGRAD), leverages a denoising model in addition to the gradient of the
action distribution of the policy to diffuse a trajectory of initially random
states and actions into an on-policy synthetic trajectory. We analyse the
connections between PolyGRAD, score-based generative models, and
classifier-guided diffusion models. Our results demonstrate that PolyGRAD
outperforms state-of-the-art baselines in terms of trajectory prediction error
for short trajectories, with the exception of autoregressive diffusion. For
short trajectories, PolyGRAD obtains similar errors to autoregressive
diffusion, but with lower computational requirements. For long trajectories,
PolyGRAD obtains comparable performance to baselines. Our experiments
demonstrate that PolyGRAD enables performant policies to be trained via
on-policy RL in imagination for MuJoCo continuous control domains. Thus,
PolyGRAD introduces a new paradigm for accurate on-policy world modelling
without autoregressive sampling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in TMLR, March 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Emerging Trends in <span class="highlight-title">Federated</span> Learning: From Model Fusion to <span class="highlight-title">Federated</span> X
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.12920v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.12920v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaoxiong Ji, Yue Tan, Teemu Saravirta, Zhiqin Yang, Yixin Liu, Lauri Vasankari, Shirui Pan, Guodong Long, Anwar Walid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is a new learning paradigm that decouples data collection
and model training via multi-party computation and model aggregation. As a
flexible learning setting, federated learning has the potential to integrate
with other learning frameworks. We conduct a focused survey of federated
learning in conjunction with other learning algorithms. Specifically, we
explore various learning algorithms to improve the vanilla federated averaging
algorithm and review model fusion methods such as adaptive aggregation,
regularization, clustered methods, and Bayesian methods. Following the emerging
trends, we also discuss federated learning in the intersection with other
learning paradigms, termed federated X learning, where X includes multitask
learning, meta-learning, transfer learning, unsupervised learning, and
reinforcement learning. In addition to reviewing state-of-the-art studies, this
paper also identifies key challenges and applications in this field, while also
highlighting promising future directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the International Journal of Machine Learning and
  Cybernetics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17905v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17905v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Chen, Chao Tang, Amir Aghabiglou, Chung San Chu, Yves Wiaux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new approach for non-Cartesian magnetic resonance image
reconstruction. While unrolled architectures provide robustness via
data-consistency layers, embedding measurement operators in Deep Neural Network
(DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP)
approaches, where the denoising DNNs are blind to the measurement setting, are
not affected by this limitation and have also proven effective, but their
highly iterative nature also affects scalability. To address this scalability
challenge, we leverage the "Residual-to-Residual DNN series for high-Dynamic
range imaging (R2D2)" approach recently introduced in astronomical imaging.
R2D2's reconstruction is formed as a series of residual images, iteratively
estimated as outputs of DNNs taking the previous iteration's image estimate and
associated data residual as inputs. The method can be interpreted as a learned
version of the Matching Pursuit algorithm. We demonstrate R2D2 in simulation,
considering radial k-space sampling acquisition sequences. Our preliminary
results suggest that R2D2 achieves: (i) suboptimal performance compared to its
unrolled incarnation R2D2-Net, which is however non-scalable due to the
necessary embedding of NUFFT-based data-consistency layers; (ii) superior
reconstruction quality to a scalable version of R2D2-Net embedding an FFT-based
approximation for data consistency; (iii) superior reconstruction quality to
PnP, while only requiring few iterations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High Dimensional Distributed Gradient Descent with Arbitrary Number of
  Byzantine Attackers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.13352v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.13352v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Puning Zhao, Zhiguo Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust distributed learning with Byzantine failures has attracted extensive
research interests in recent years. However, most of existing methods suffer
from curse of dimensionality, which is increasingly serious with the growing
complexity of modern machine learning models. In this paper, we design a new
method that is suitable for high dimensional problems, under arbitrary number
of Byzantine attackers. The core of our design is a direct high dimensional
semi-verified mean estimation method. Our idea is to identify a subspace first.
The components of mean value perpendicular to this subspace can be estimated
via gradient vectors uploaded from worker machines, while the components within
this subspace are estimated using auxiliary dataset. We then use our new method
as the aggregator of distributed learning problems. Our theoretical analysis
shows that the new method has minimax optimal statistical rates. In particular,
the dependence on dimensionality is significantly improved compared with
previous works.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Functional Graph Convolutional Networks: A unified multi-task and
  multi-modal learning framework to facilitate health and social-care insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10158v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10158v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobia Boschi, Francesca Bonin, Rodrigo Ordonez-Hurtado, Cécile Rousseau, Alessandra Pascale, John Dinsmore
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel Functional Graph Convolutional Network (funGCN)
framework that combines Functional Data Analysis and Graph Convolutional
Networks to address the complexities of multi-task and multi-modal learning in
digital health and longitudinal studies. With the growing importance of health
solutions to improve health care and social support, ensure healthy lives, and
promote well-being at all ages, funGCN offers a unified approach to handle
multivariate longitudinal data for multiple entities and ensures
interpretability even with small sample sizes. Key innovations include
task-specific embedding components that manage different data types, the
ability to perform classification, regression, and forecasting, and the
creation of a knowledge graph for insightful data interpretation. The efficacy
of funGCN is validated through simulation experiments and a real-data
application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Concept-Based Causal Transition and Symbolic Reasoning for
  Visual Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03325v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03325v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilue Qian, Peiyu Yu, Ying Nian Wu, Yao Su, Wei Wang, Lifeng Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual planning simulates how humans make decisions to achieve desired goals
in the form of searching for visual causal transitions between an initial
visual state and a final visual goal state. It has become increasingly
important in egocentric vision with its advantages in guiding agents to perform
daily tasks in complex environments. In this paper, we propose an interpretable
and generalizable visual planning framework consisting of i) a novel
Substitution-based Concept Learner (SCL) that abstracts visual inputs into
disentangled concept representations, ii) symbol abstraction and reasoning that
performs task planning via the self-learned symbols, and iii) a Visual Causal
Transition model (ViCT) that grounds visual causal transitions to semantically
similar real-world actions. Given an initial state, we perform goal-conditioned
visual planning with a symbolic reasoning method fueled by the learned
representations and causal transitions to reach the goal state. To verify the
effectiveness of the proposed model, we collect a large-scale visual planning
dataset based on AI2-THOR, dubbed as CCTP. Extensive experiments on this
challenging dataset demonstrate the superior performance of our method in
visual task planning. Empirically, we show that our framework can generalize to
unseen task trajectories, unseen object categories, and real-world data.
Further details of this work are provided at
https://fqyqc.github.io/ConTranPlan/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Centered Masking for Language-Image Pre-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15837v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15837v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingliang Liang, Martha Larson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Gaussian masking for Language-Image Pre-Training (GLIP) a novel,
straightforward, and effective technique for masking image patches during
pre-training of a vision-language model. GLIP builds on Fast Language-Image
Pre-Training (FLIP), which randomly masks image patches while training a CLIP
model. GLIP replaces random masking with centered masking, that uses a Gaussian
distribution and is inspired by the importance of image patches at the center
of the image. GLIP retains the same computational savings as FLIP, while
improving performance across a range of downstream datasets and tasks, as
demonstrated by our experimental results. We show the benefits of GLIP to be
easy to obtain, requiring no delicate tuning of the Gaussian, and also
applicable to data sets containing images without an obvious center focus.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Asymptotic Bayes risk of semi-supervised learning with uncertain
  labeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17767v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17767v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor Leger, Romain Couillet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article considers a semi-supervised classification setting on a Gaussian
mixture model, where the data is not labeled strictly as usual, but instead
with uncertain labels. Our main aim is to compute the Bayes risk for this
model. We compare the behavior of the Bayes risk and the best known algorithm
for this model. This comparison eventually gives new insights over the
algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identifying the Correlation Between Language Distance and Cross-Lingual
  <span class="highlight-title">Transfer</span> in a Multilingual Representation Space <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.02151v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.02151v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fred Philippy, Siwen Guo, Shohreh Haddadan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior research has investigated the impact of various linguistic features on
cross-lingual transfer performance. In this study, we investigate the manner in
which this effect can be mapped onto the representation space. While past
studies have focused on the impact on cross-lingual alignment in multilingual
language models during fine-tuning, this study examines the absolute evolution
of the respective language representation spaces produced by MLLMs. We place a
specific emphasis on the role of linguistic characteristics and investigate
their inter-correlation with the impact on representation spaces and
cross-lingual transfer performance. Additionally, this paper provides
preliminary evidence of how these findings can be leveraged to enhance transfer
to linguistically distant languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGTYP Workshop 2023 (co-located with EACL 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Physics-embedded Deep Learning Framework for Cloth Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12820v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12820v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwei Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Delicate cloth simulations have long been desired in computer graphics.
Various methods were proposed to improve engaged force interactions, collision
handling, and numerical integrations. Deep learning has the potential to
achieve fast and real-time simulation, but common neural network structures
often demand many parameters to capture cloth dynamics. This paper proposes a
physics-embedded learning framework that directly encodes physical features of
cloth simulation. The convolutional neural network is used to represent spatial
correlations of the mass-spring system, after which three branches are designed
to learn linear, nonlinear, and time derivate features of cloth physics. The
framework can also integrate with other external forces and collision handling
through either traditional simulators or sub neural networks. The model is
tested across different cloth animation cases, without training with new data.
Agreement with baselines and predictive realism successfully validate its
generalization ability. Inference efficiency of the proposed model also defeats
traditional physics simulation. This framework is also designed to easily
integrate with other visual refinement techniques like wrinkle carving, which
leaves significant chances to incorporate prevailing macing learning techniques
in 3D cloth amination.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A derivation is incomplete, and updations are being processed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMP++: Motion Manifold Primitives with Parametric Curve Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17072v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17072v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonghyeon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion Manifold Primitives (MMP), a manifold-based approach for encoding
basic motion skills, can produce diverse trajectories, enabling the system to
adapt to unseen constraints. Nonetheless, we argue that current MMP models lack
crucial functionalities of movement primitives, such as temporal and via-points
modulation, found in traditional approaches. This shortfall primarily stems
from MMP's reliance on discrete-time trajectories. To overcome these
limitations, we introduce Motion Manifold Primitives++ (MMP++), a new model
that integrates the strengths of both MMP and traditional methods by
incorporating parametric curve representations into the MMP framework.
Furthermore, we identify a significant challenge with MMP++: performance
degradation due to geometric distortions in the latent space, meaning that
similar motions are not closely positioned. To address this, Isometric Motion
Manifold Primitives++ (IMMP++) is proposed to ensure the latent space
accurately preserves the manifold's geometry. Our experimental results across
various applications, including 2-DoF planar motions, 7-DoF robot arm motions,
and SE(3) trajectory planning, show that MMP++ and IMMP++ outperform existing
methods in trajectory generation tasks, achieving substantial improvements in
some cases. Moreover, they enable the modulation of latent coordinates and
via-points, thereby allowing efficient online adaptation to dynamic
environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages. This work has been submitted to the IEEE for possible
  publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regret-Based Defense in Adversarial Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.06912v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.06912v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roman Belaire, Pradeep Varakantham, Thanh Nguyen, David Lo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Reinforcement Learning (DRL) policies have been shown to be vulnerable
to small adversarial noise in observations. Such adversarial noise can have
disastrous consequences in safety-critical environments. For instance, a
self-driving car receiving adversarially perturbed sensory observations about
nearby signs (e.g., a stop sign physically altered to be perceived as a speed
limit sign) or objects (e.g., cars altered to be recognized as trees) can be
fatal. Existing approaches for making RL algorithms robust to an
observation-perturbing adversary have focused on reactive approaches that
iteratively improve against adversarial examples generated at each iteration.
While such approaches have been shown to provide improvements over regular RL
methods, they are reactive and can fare significantly worse if certain
categories of adversarial examples are not generated during training. To that
end, we pursue a more proactive approach that relies on directly optimizing a
well-studied robustness measure, regret instead of expected value. We provide a
principled approach that minimizes maximum regret over a "neighborhood" of
observations to the received "observation". Our regret criterion can be used to
modify existing value- and policy-based Deep RL methods. We demonstrate that
our approaches provide a significant improvement in performance across a wide
variety of benchmarks against leading approaches for robust Deep RL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAMAS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sample Efficient Reinforcement Learning with Partial Dynamics Knowledge <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12558v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12558v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meshal Alharbi, Mardavij Roozbehani, Munther Dahleh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The problem of sample complexity of online reinforcement learning is often
studied in the literature without taking into account any partial knowledge
about the system dynamics that could potentially accelerate the learning
process. In this paper, we study the sample complexity of online Q-learning
methods when some prior knowledge about the dynamics is available or can be
learned efficiently. We focus on systems that evolve according to an additive
disturbance model of the form $S_{h+1} = f(S_h, A_h) + W_h$, where $f$
represents the underlying system dynamics, and $W_h$ are unknown disturbances
independent of states and actions. In the setting of finite episodic Markov
decision processes with $S$ states, $A$ actions, and episode length $H$, we
present an optimistic Q-learning algorithm that achieves
$\tilde{\mathcal{O}}(\text{Poly}(H)\sqrt{T})$ regret under perfect knowledge of
$f$, where $T$ is the total number of interactions with the system. This is in
contrast to the typical $\tilde{\mathcal{O}}(\text{Poly}(H)\sqrt{SAT})$ regret
for existing Q-learning methods. Further, if only a noisy estimate $\hat{f}$ of
$f$ is available, our method can learn an approximately optimal policy in a
number of samples that is independent of the cardinalities of state and action
spaces. The sub-optimality gap depends on the approximation error $\hat{f}-f$,
as well as the Lipschitz constant of the corresponding optimal value function.
Our approach does not require modeling of the transition probabilities and
enjoys the same memory complexity as model-free methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in the 38th Annual AAAI Conference on Artificial
  Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Weakly Supervised AUC Optimization: A Unified Partial AUC Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14258v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14258v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Xie, Yu Liu, Hao-Yuan He, Ming Li, Zhi-Hua Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since acquiring perfect supervision is usually difficult, real-world machine
learning tasks often confront inaccurate, incomplete, or inexact supervision,
collectively referred to as weak supervision. In this work, we present WSAUC, a
unified framework for weakly supervised AUC optimization problems, which covers
noisy label learning, positive-unlabeled learning, multi-instance learning, and
semi-supervised learning scenarios. Within the WSAUC framework, we first frame
the AUC optimization problems in various weakly supervised scenarios as a
common formulation of minimizing the AUC risk on contaminated sets, and
demonstrate that the empirical risk minimization problems are consistent with
the true AUC. Then, we introduce a new type of partial AUC, specifically, the
reversed partial AUC (rpAUC), which serves as a robust training objective for
AUC maximization in the presence of contaminated labels. WSAUC offers a
universal solution for AUC optimization in various weakly supervised scenarios
by maximizing the empirical rpAUC. Theoretical and experimental results under
multiple settings support the effectiveness of WSAUC on a range of weakly
supervised AUC optimization tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TPAMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Foundation Model Makes Clustering A Better Initialization For Cold-Start
  Active Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02561v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02561v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Yuan, Chuan Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active learning selects the most informative samples from the unlabelled
dataset to annotate in the context of a limited annotation budget. While
numerous methods have been proposed for subsequent sample selection based on an
initialized model, scant attention has been paid to the indispensable phase of
active learning: selecting samples for model cold-start initialization. Most of
the previous studies resort to random sampling or naive clustering. However,
random sampling is prone to fluctuation, and naive clustering suffers from
convergence speed, particularly when dealing with high-dimensional data such as
imaging data. In this work, we propose to integrate foundation models with
clustering methods to select samples for cold-start active learning
initialization. Foundation models refer to those trained on massive datasets by
the self-supervised paradigm and capable of generating informative and
compacted embeddings for various downstream tasks. Leveraging these embeddings
to replace raw features such as pixel values, clustering quickly converges and
identifies better initial samples. For a comprehensive comparison, we included
a classic ImageNet-supervised model to acquire embeddings. Experiments on two
clinical tasks of image classification and segmentation demonstrated that
foundation model-based clustering efficiently pinpointed informative initial
samples, leading to models showcasing enhanced performance than the baseline
methods. We envisage that this study provides an effective paradigm for future
cold-start active learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Expectations Versus Reality: Evaluating Intrusion Detection Systems in
  Practice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17458v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17458v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jake Hesford, Daniel Cheng, Alan Wan, Larry Huynh, Seungho Kim, Hyoungshick Kim, Jin B. Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our paper provides empirical comparisons between recent IDSs to provide an
objective comparison between them to help users choose the most appropriate
solution based on their requirements. Our results show that no one solution is
the best, but is dependent on external variables such as the types of attacks,
complexity, and network environment in the dataset. For example, BoT_IoT and
Stratosphere IoT datasets both capture IoT-related attacks, but the deep neural
network performed the best when tested using the BoT_IoT dataset while HELAD
performed the best when tested using the Stratosphere IoT dataset. So although
we found that a deep neural network solution had the highest average F1 scores
on tested datasets, it is not always the best-performing one. We further
discuss difficulties in using IDS from literature and project repositories,
which complicated drawing definitive conclusions regarding IDS selection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CBQ: Cross-Block <span class="highlight-title">Quantization</span> for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07950v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07950v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Ding, Xiaoyu Liu, Zhijun Tu, Yun Zhang, Wei Li, Jie Hu, Hanting Chen, Yehui Tang, Zhiwei Xiong, Baoqun Yin, Yunhe Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-training quantization (PTQ) has played a key role in compressing large
language models (LLMs) with ultra-low costs. However, existing PTQ methods only
focus on handling the outliers within one layer or one block, which ignores the
dependency of blocks and leads to severe performance degradation in low-bit
settings. In this paper, we propose CBQ, a cross-block reconstruction-based PTQ
method for LLMs. CBQ employs a cross-block dependency using a homologous
reconstruction scheme, establishing long-range dependencies across multiple
blocks to minimize error accumulation. Furthermore, CBQ incorporates a
coarse-to-fine preprocessing (CFP) strategy for suppressing weight and
activation outliers, coupled with an adaptive LoRA-Rounding technique for
precise weight quantization. These innovations enable CBQ to not only handle
extreme outliers effectively but also improve overall quantization accuracy.
Extensive experiments show that CBQ achieves superior low-bit quantization
(W4A4, W4A8, W2A16) and outperforms existing state-of-the-art methods across
various LLMs and datasets. Notably, CBQ quantizes the 4-bit LLAMA1-65B model
within only 4.3 hours on a single GPU, achieving a commendable tradeoff between
performance and quantization efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simple Policy Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.16025v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.16025v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengpeng Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  PPO (Proximal Policy Optimization) algorithm has demonstrated excellent
performance in many fields, and it is considered as a simple version of TRPO
(Trust Region Policy Optimization) algorithm. However, the ratio clipping
operation in PPO may not always effectively enforce the trust region
constraints, this can be a potential factor affecting the stability of the
algorithm. In this paper, we propose Simple Policy Optimization (SPO)
algorithm, which introduces a novel clipping method for KL divergence between
the old and current policies. Extensive experimental results in Atari 2600
environments indicate that, compared to the mainstream variants of PPO, SPO
achieves better sample efficiency, extremely low KL divergence, and higher
policy entropy, and is robust to the increase in network depth or complexity.
More importantly, SPO maintains the simplicity of an unconstrained first-order
algorithm. Code is available at
https://github.com/MyRepositories-hub/Simple-Policy-Optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BridgeTower: Building Bridges Between Encoders in Vision-Language
  Representation Learning <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.08657v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.08657v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language (VL) models with the Two-Tower architecture have dominated
visual-language representation learning in recent years. Current VL models
either use lightweight uni-modal encoders and learn to extract, align and fuse
both modalities simultaneously in a deep cross-modal encoder, or feed the
last-layer uni-modal representations from the deep pre-trained uni-modal
encoders into the top cross-modal encoder. Both approaches potentially restrict
vision-language representation learning and limit model performance. In this
paper, we propose BridgeTower, which introduces multiple bridge layers that
build a connection between the top layers of uni-modal encoders and each layer
of the cross-modal encoder. This enables effective bottom-up cross-modal
alignment and fusion between visual and textual representations of different
semantic levels of pre-trained uni-modal encoders in the cross-modal encoder.
Pre-trained with only 4M images, BridgeTower achieves state-of-the-art
performance on various downstream vision-language tasks. In particular, on the
VQAv2 test-std set, BridgeTower achieves an accuracy of 78.73%, outperforming
the previous state-of-the-art model METER by 1.09% with the same pre-training
data and almost negligible additional parameters and computational costs.
Notably, when further scaling the model, BridgeTower achieves an accuracy of
81.15%, surpassing models that are pre-trained on orders-of-magnitude larger
datasets. Code and checkpoints are available at
https://github.com/microsoft/BridgeTower.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2023, Oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discovering and Mitigating Visual Biases through Keyword Explanation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11104v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11104v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Younghyun Kim, Sangwoo Mo, Minkyu Kim, Kyungmin Lee, Jaeho Lee, Jinwoo Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Addressing biases in computer vision models is crucial for real-world AI
deployments. However, mitigating visual biases is challenging due to their
unexplainable nature, often identified indirectly through visualization or
sample statistics, which necessitates additional human supervision for
interpretation. To tackle this issue, we propose the Bias-to-Text (B2T)
framework, which interprets visual biases as keywords. Specifically, we extract
common keywords from the captions of mispredicted images to identify potential
biases in the model. We then validate these keywords by measuring their
similarity to the mispredicted images using a vision-language scoring model.
The keyword explanation form of visual bias offers several advantages, such as
a clear group naming for bias discovery and a natural extension for debiasing
using these group names. Our experiments demonstrate that B2T can identify
known biases, such as gender bias in CelebA, background bias in Waterbirds, and
distribution shifts in ImageNet-R/C. Additionally, B2T uncovers novel biases in
larger datasets, such as Dollar Street and ImageNet. For example, we discovered
a contextual bias between "bee" and "flower" in ImageNet. We also highlight
various applications of B2T keywords, including debiased training, CLIP
prompting, and model comparison.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024. First two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CAFE: Towards Compact, Adaptive, and Fast <span class="highlight-title">Embedding</span> for Large-scale
  <span class="highlight-title">Recommendation</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03256v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03256v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hailin Zhang, Zirui Liu, Boxuan Chen, Yikai Zhao, Tong Zhao, Tong Yang, Bin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the growing memory demands of embedding tables in Deep Learning
Recommendation Models (DLRMs) pose great challenges for model training and
deployment. Existing embedding compression solutions cannot simultaneously meet
three key design requirements: memory efficiency, low latency, and adaptability
to dynamic data distribution. This paper presents CAFE, a Compact, Adaptive,
and Fast Embedding compression framework that addresses the above requirements.
The design philosophy of CAFE is to dynamically allocate more memory resources
to important features (called hot features), and allocate less memory to
unimportant ones. In CAFE, we propose a fast and lightweight sketch data
structure, named HotSketch, to capture feature importance and report hot
features in real time. For each reported hot feature, we assign it a unique
embedding. For the non-hot features, we allow multiple features to share one
embedding by using hash embedding technique. Guided by our design philosophy,
we further propose a multi-level hash embedding framework to optimize the
embedding tables of non-hot features. We theoretically analyze the accuracy of
HotSketch, and analyze the model convergence against deviation. Extensive
experiments show that CAFE significantly outperforms existing embedding
compression methods, yielding 3.92% and 3.68% superior testing AUC on Criteo
Kaggle dataset and CriteoTB dataset at a compression ratio of 10000x. The
source codes of CAFE are available at GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Superior Parallel Big Data Clustering through Competitive Stochastic
  Sample Size Optimization in Big-means 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rustam Mussabayev, Ravil Mussabayev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel K-means clustering algorithm, an advancement on
the conventional Big-means methodology. The proposed method efficiently
integrates parallel processing, stochastic sampling, and competitive
optimization to create a scalable variant designed for big data applications.
It addresses scalability and computation time challenges typically faced with
traditional techniques. The algorithm adjusts sample sizes dynamically for each
worker during execution, optimizing performance. Data from these sample sizes
are continually analyzed, facilitating the identification of the most efficient
configuration. By incorporating a competitive element among workers using
different sample sizes, efficiency within the Big-means algorithm is further
stimulated. In essence, the algorithm balances computational time and
clustering quality by employing a stochastic, competitive sampling strategy in
a parallel computing setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Laws For Dense Retrieval <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18684v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18684v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Fang, Jingtao Zhan, Qingyao Ai, Jiaxin Mao, Weihang Su, Jia Chen, Yiqun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling up neural models has yielded significant advancements in a wide array
of tasks, particularly in language generation. Previous studies have found that
the performance of neural models frequently adheres to predictable scaling
laws, correlated with factors such as training set size and model size. This
insight is invaluable, especially as large-scale experiments grow increasingly
resource-intensive. Yet, such scaling law has not been fully explored in dense
retrieval due to the discrete nature of retrieval metrics and complex
relationships between training data and model sizes in retrieval tasks. In this
study, we investigate whether the performance of dense retrieval models follows
the scaling law as other neural models. We propose to use contrastive
log-likelihood as the evaluation metric and conduct extensive experiments with
dense retrieval models implemented with different numbers of parameters and
trained with different amounts of annotated data. Results indicate that, under
our settings, the performance of dense retrieval models follows a precise
power-law scaling related to the model size and the number of annotations.
Additionally, we examine scaling with prevalent data augmentation methods to
assess the impact of annotation quality, and apply the scaling law to find the
best resource allocation strategy under a budget constraint. We believe that
these insights will significantly contribute to understanding the scaling
effect of dense retrieval models and offer meaningful guidance for future
research endeavors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SIGIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Content <span class="highlight-title">Recommendation</span>: Knowledge Graph-Based Semantic
  Contrastive Learning for Diversity and Cold-Start Users <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18667v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18667v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yejin Kim, Scott Rome, Kevin Foley, Mayur Nankani, Rimon Melamed, Javier Morales, Abhay Yadav, Maria Peifer, Sardar Hamidian, H. Howie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Addressing the challenges related to data sparsity, cold-start problems, and
diversity in recommendation systems is both crucial and demanding. Many current
solutions leverage knowledge graphs to tackle these issues by combining both
item-based and user-item collaborative signals. A common trend in these
approaches focuses on improving ranking performance at the cost of escalating
model complexity, reducing diversity, and complicating the task. It is
essential to provide recommendations that are both personalized and diverse,
rather than solely relying on achieving high rank-based performance, such as
Click-through Rate, Recall, etc. In this paper, we propose a hybrid multi-task
learning approach, training on user-item and item-item interactions. We apply
item-based contrastive learning on descriptive text, sampling positive and
negative pairs based on item metadata. Our approach allows the model to better
understand the relationships between entities within the knowledge graph by
utilizing semantic information from text. It leads to more accurate, relevant,
and diverse user recommendations and a benefit that extends even to cold-start
users who have few interactions with items. We perform extensive experiments on
two widely used datasets to validate the effectiveness of our approach. Our
findings demonstrate that jointly training user-item interactions and
item-based signals using synopsis text is highly effective. Furthermore, our
results provide evidence that item-based contrastive learning enhances the
quality of entity embeddings, as indicated by metrics such as uniformity and
alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> To Recommend or Not: Recommendability Identification in Conversations
  with Pre-trained Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18628v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18628v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhefan Wang, Weizhi Ma, <span class="highlight-author">Min Zhang</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most current recommender systems primarily focus on what to recommend,
assuming users always require personalized recommendations. However, with the
widely spread of ChatGPT and other chatbots, a more crucial problem in the
context of conversational systems is how to minimize user disruption when we
provide recommendation services for users. While previous research has
extensively explored different user intents in dialogue systems, fewer efforts
are made to investigate whether recommendations should be provided. In this
paper, we formally define the recommendability identification problem, which
aims to determine whether recommendations are necessary in a specific scenario.
First, we propose and define the recommendability identification task, which
investigates the need for recommendations in the current conversational
context. A new dataset is constructed. Subsequently, we discuss and evaluate
the feasibility of leveraging pre-trained language models (PLMs) for
recommendability identification. Finally, through comparative experiments, we
demonstrate that directly employing PLMs with zero-shot results falls short of
meeting the task requirements. Besides, fine-tuning or utilizing soft prompt
techniques yields comparable results to traditional classification methods. Our
work is the first to study recommendability before recommendation and provides
preliminary ways to make it a fundamental component of the future
recommendation system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Antitrust, Amazon, and Algorithmic Auditing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhisek Dash, Abhijnan Chakraborty, Saptarshi Ghosh, Animesh Mukherjee, Jens Frankenreiter, Stefan Bechtold, Krishna P. Gummadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In digital markets, antitrust law and special regulations aim to ensure that
markets remain competitive despite the dominating role that digital platforms
play today in everyone's life. Unlike traditional markets, market participant
behavior is easily observable in these markets. We present a series of
empirical investigations into the extent to which Amazon engages in practices
that are typically described as self-preferencing. We discuss how the computer
science tools used in this paper can be used in a regulatory environment that
is based on algorithmic auditing and requires regulating digital markets at
scale.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been accepted to appear at Journal of Institutional and
  Theoretical Economics (JITE) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Sustainable City Trips: Integrating CO2 Emissions, Popularity,
  and Seasonality into Tourism Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashmi Banerjee, Tunar Mahmudov, Emil Adler, Fitri Nur Aisyah, Wolfgang Wörndl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In an era of information overload and complex decision-making processes,
Recommender Systems (RS) have emerged as indispensable tools across diverse
domains, particularly travel and tourism. These systems simplify trip planning
by offering personalized recommendations that consider individual preferences
and address broader challenges like seasonality, travel regulations, and
capacity constraints. The intricacies of the tourism domain, characterized by
multiple stakeholders, including consumers, item providers, platforms, and
society, underscore the complexity of achieving balance among diverse
interests. Although previous research has focused on fairness in Tourism
Recommender Systems (TRS) from a multistakeholder perspective, limited work has
focused on generating sustainable recommendations.
  Our paper introduces a novel approach for assigning a sustainability
indicator (SF index) for city trips accessible from the users' starting point,
integrating Co2e analysis, destination popularity, and seasonal demand. Our
methodology involves comprehensive data gathering on transportation modes and
emissions, complemented by analyses of destination popularity and seasonal
demand. A user study validates our index, showcasing its practicality and
efficacy in providing well-rounded and sustainable city trip recommendations.
Our findings contribute significantly to the evolution of responsible tourism
strategies, harmonizing the interests of tourists, local communities, and the
environment while paving the way for future research in responsible and
equitable tourism practices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Behavior-Based <span class="highlight-title">Recommendation</span> System for E-commerce 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reza Barzegar Nozari, Mahdi Divsalar, Sepehr Akbarzadeh Abkenar, Mohammadreza Fadavi Amiri, Ali Divsalar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The majority of existing recommender systems rely on user ratings, which are
limited by the lack of user collaboration and the sparsity problem. To address
these issues, this study proposes a behavior-based recommender system that
leverages customers' natural behaviors, such as browsing and clicking, on
e-commerce platforms. The proposed recommendation system involves clustering
active customers, determining neighborhoods, collecting similar users,
calculating product reputation based on similar users, and recommending
high-reputation products. To overcome the complexity of customer behaviors and
traditional clustering methods, an unsupervised clustering approach based on
product categories is developed to enhance the recommendation methodology. This
study makes notable contributions in several aspects. Firstly, a groundbreaking
behavior-based recommendation methodology is developed, incorporating customer
behavior to generate accurate and tailored recommendations leading to improved
customer satisfaction and engagement. Secondly, an original unsupervised
clustering method, focusing on product categories, enables more precise
clustering and facilitates accurate recommendations. Finally, an approach to
determine neighborhoods for active customers within clusters is established,
ensuring grouping of customers with similar behavioral patterns to enhance
recommendation accuracy and relevance. The proposed recommendation methodology
and clustering method contribute to improved recommendation performance,
offering valuable insights for researchers and practitioners in the field of
e-commerce recommendation systems. Additionally, the proposed method
outperforms benchmark methods in experiments conducted using a behavior dataset
from the well-known e-commerce site Alibaba.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced Generative <span class="highlight-title">Recommendation</span> via Content and Collaboration
  Integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yidan Wang, Zhaochun Ren, Weiwei Sun, Jiyuan Yang, Zhixiang Liang, Xin Chen, Ruobing Xie, Su Yan, Xu Zhang, Pengjie Ren, Zhumin Chen, Xin Xin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative recommendation has emerged as a promising paradigm aimed at
augmenting recommender systems with recent advancements in generative
artificial intelligence. This task has been formulated as a
sequence-to-sequence generation process, wherein the input sequence encompasses
data pertaining to the user's previously interacted items, and the output
sequence denotes the generative identifier for the suggested item. However,
existing generative recommendation approaches still encounter challenges in (i)
effectively integrating user-item collaborative signals and item content
information within a unified generative framework, and (ii) executing an
efficient alignment between content information and collaborative signals.
  In this paper, we introduce content-based collaborative generation for
recommender systems, denoted as ColaRec. To capture collaborative signals, the
generative item identifiers are derived from a pretrained collaborative
filtering model, while the user is represented through the aggregation of
interacted items' content. Subsequently, the aggregated textual description of
items is fed into a language model to encapsulate content information. This
integration enables ColaRec to amalgamate collaborative signals and content
information within an end-to-end framework. Regarding the alignment, we propose
an item indexing task to facilitate the mapping between the content-based
semantic space and the interaction-based collaborative space. Additionally, a
contrastive loss is introduced to ensure that items with similar collaborative
GIDs possess comparable content representations, thereby enhancing alignment.
To validate the efficacy of ColaRec, we conduct experiments on three benchmark
datasets. Empirical results substantiate the superior performance of ColaRec.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lightweight <span class="highlight-title">Embedding</span>s for Graph Collaborative Filtering <span class="chip">SIGIR '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xurong Liang, Tong Chen, Lizhen Cui, Yang Wang, Meng Wang, Hongzhi Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) are currently one of the most performant
collaborative filtering methods. Meanwhile, owing to the use of an embedding
table to represent each user/item as a distinct vector, GNN-based recommenders
have inherited the long-standing defect of parameter inefficiency. As a common
practice for scalable embeddings, parameter sharing enables the use of fewer
embedding vectors (i.e., meta-embeddings). When assigning meta-embeddings, most
existing methods are a heuristically designed, predefined mapping from each
user's/item's ID to the corresponding meta-embedding indexes, thus simplifying
the optimization problem into learning only the meta-embeddings. However, in
the context of GNN-based collaborative filtering, such a fixed mapping omits
the semantic correlations between entities that are evident in the user-item
interaction graph, leading to suboptimal recommendation performance. To this
end, we propose Lightweight Embeddings for Graph Collaborative Filtering
(LEGCF), a parameter-efficient embedding framework dedicated to GNN-based
recommenders. LEGCF innovatively introduces an assignment matrix as an extra
learnable component on top of meta-embeddings. To jointly optimize these two
heavily entangled components, aside from learning the meta-embeddings by
minimizing the recommendation loss, LEGCF further performs efficient assignment
update by enforcing a novel semantic similarity constraint and finding its
closed-form solution based on matrix pseudo-inverse. The meta-embeddings and
assignment matrix are alternately updated, where the latter is sparsified on
the fly to ensure negligible storage overhead. Extensive experiments on three
benchmark datasets have verified LEGCF's smallest trade-off between size and
performance, with consistent accuracy gain over state-of-the-art baselines. The
codebase of LEGCF is available in https://github.com/xurong-liang/LEGCF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR '24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoy Effect In Search Interaction: Understanding User Behavior and
  Measuring System Vulnerability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18462v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18462v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nuo Chen, Jiqun Liu, Hanpei Fang, Yuankai Luo, Tetsuya Sakai, Xiao-Ming Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study examines the decoy effect's underexplored influence on user search
interactions and methods for measuring information retrieval (IR) systems'
vulnerability to this effect. It explores how decoy results alter users'
interactions on search engine result pages, focusing on metrics like
click-through likelihood, browsing time, and perceived document usefulness. By
analyzing user interaction logs from multiple datasets, the study demonstrates
that decoy results significantly affect users' behavior and perceptions.
Furthermore, it investigates how different levels of task difficulty and user
knowledge modify the decoy effect's impact, finding that easier tasks and lower
knowledge levels lead to higher engagement with target documents. In terms of
IR system evaluation, the study introduces the DEJA-VU metric to assess
systems' susceptibility to the decoy effect, testing it on specific retrieval
tasks. The results show differences in systems' effectiveness and
vulnerability, contributing to our understanding of cognitive biases in search
behavior and suggesting pathways for creating more balanced and bias-aware IR
evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DELTA: Pre-train a Discriminative Encoder for Legal Case Retrieval via
  Structural Word Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haitao Li, Qingyao Ai, Xinyan Han, Jia Chen, Qian Dong, Yiqun Liu, Chong Chen, Qi Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research demonstrates the effectiveness of using pre-trained language
models for legal case retrieval. Most of the existing works focus on improving
the representation ability for the contextualized embedding of the [CLS] token
and calculate relevance using textual semantic similarity. However, in the
legal domain, textual semantic similarity does not always imply that the cases
are relevant enough. Instead, relevance in legal cases primarily depends on the
similarity of key facts that impact the final judgment. Without proper
treatments, the discriminative ability of learned representations could be
limited since legal cases are lengthy and contain numerous non-key facts. To
this end, we introduce DELTA, a discriminative model designed for legal case
retrieval. The basic idea involves pinpointing key facts in legal cases and
pulling the contextualized embedding of the [CLS] token closer to the key facts
while pushing away from the non-key facts, which can warm up the case embedding
space in an unsupervised manner. To be specific, this study brings the word
alignment mechanism to the contextual masked auto-encoder. First, we leverage
shallow decoders to create information bottlenecks, aiming to enhance the
representation ability. Second, we employ the deep decoder to enable
translation between different structures, with the goal of pinpointing key
facts to enhance discriminative ability. Comprehensive experiments conducted on
publicly available legal benchmarks show that our approach can outperform
existing state-of-the-art methods in legal case retrieval. It provides a new
perspective on the in-depth understanding and processing of legal case
documents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Sequential <span class="highlight-title">Recommendation</span> with Latent Relations based on Large Language
  Model <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18348v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18348v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenghao Yang, Weizhi Ma, Peijie Sun, Qingyao Ai, Yiqun Liu, Mingchen Cai, <span class="highlight-author">Min Zhang</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommender systems predict items that may interest users by
modeling their preferences based on historical interactions. Traditional
sequential recommendation methods rely on capturing implicit collaborative
filtering signals among items. Recent relation-aware sequential recommendation
models have achieved promising performance by explicitly incorporating item
relations into the modeling of user historical sequences, where most relations
are extracted from knowledge graphs. However, existing methods rely on manually
predefined relations and suffer the sparsity issue, limiting the generalization
ability in diverse scenarios with varied item relations. In this paper, we
propose a novel relation-aware sequential recommendation framework with Latent
Relation Discovery (LRD). Different from previous relation-aware models that
rely on predefined rules, we propose to leverage the Large Language Model (LLM)
to provide new types of relations and connections between items. The motivation
is that LLM contains abundant world knowledge, which can be adopted to mine
latent relations of items for recommendation. Specifically, inspired by that
humans can describe relations between items using natural language, LRD
harnesses the LLM that has demonstrated human-like knowledge to obtain language
knowledge representations of items. These representations are fed into a latent
relation discovery module based on the discrete state variational autoencoder
(DVAE). Then the self-supervised relation discovery tasks and recommendation
tasks are jointly optimized. Experimental results on multiple public datasets
demonstrate our proposed latent relations discovery method can be incorporated
with existing relation-aware sequential recommendation models and significantly
improve the performance. Further analysis experiments indicate the
effectiveness and reliability of the discovered latent relations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Common Sense Enhanced Knowledge-based <span class="highlight-title">Recommendation</span> with Large Language
  Model <span class="chip">DASFAA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18325v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18325v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenghao Yang, Weizhi Ma, Peijie Sun, <span class="highlight-author">Min Zhang</span>, Qingyao Ai, Yiqun Liu, Mingchen Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge-based recommendation models effectively alleviate the data sparsity
issue leveraging the side information in the knowledge graph, and have achieved
considerable performance. Nevertheless, the knowledge graphs used in previous
work, namely metadata-based knowledge graphs, are usually constructed based on
the attributes of items and co-occurring relations (e.g., also buy), in which
the former provides limited information and the latter relies on sufficient
interaction data and still suffers from cold start issue. Common sense, as a
form of knowledge with generality and universality, can be used as a supplement
to the metadata-based knowledge graph and provides a new perspective for
modeling users' preferences. Recently, benefiting from the emergent world
knowledge of the large language model, efficient acquisition of common sense
has become possible. In this paper, we propose a novel knowledge-based
recommendation framework incorporating common sense, CSRec, which can be
flexibly coupled to existing knowledge-based methods. Considering the challenge
of the knowledge gap between the common sense-based knowledge graph and
metadata-based knowledge graph, we propose a knowledge fusion approach based on
mutual information maximization theory. Experimental results on public datasets
demonstrate that our approach significantly improves the performance of
existing knowledge-based recommendation models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by DASFAA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> A Situation-aware Enhancer for Personalized <span class="highlight-title">Recommendation</span> <span class="chip">DASFAA 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayu Li, Peijie Sun, Chumeng Jiang, Weizhi Ma, Qingyao Ai, <span class="highlight-author">Min Zhang</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When users interact with Recommender Systems (RecSys), current situations,
such as time, location, and environment, significantly influence their
preferences. Situations serve as the background for interactions, where
relationships between users and items evolve with situation changes. However,
existing RecSys treat situations, users, and items on the same level. They can
only model the relations between situations and users/items respectively,
rather than the dynamic impact of situations on user-item associations (i.e.,
user preferences). In this paper, we provide a new perspective that takes
situations as the preconditions for users' interactions. This perspective
allows us to separate situations from user/item representations, and capture
situations' influences over the user-item relationship, offering a more
comprehensive understanding of situations. Based on it, we propose a novel
Situation-Aware Recommender Enhancer (SARE), a pluggable module to integrate
situations into various existing RecSys. Since users' perception of situations
and situations' impact on preferences are both personalized, SARE includes a
Personalized Situation Fusion (PSF) and a User-Conditioned Preference Encoder
(UCPE) to model the perception and impact of situations, respectively. We
conduct experiments of applying SARE on seven backbones in various settings on
two real-world datasets. Experimental results indicate that SARE improves the
recommendation performances significantly compared with backbones and SOTA
situation-aware baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the International Conference on Database Systems for
  Advanced Applications (DASFAA 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Recommender System for NFT Collectibles with Item Feature <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minjoo Choi, Seonmi Kim, Yejin Kim, Youngbin Lee, Joohwan Hong, Yongjae Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems have been actively studied and applied in various domains
to deal with information overload. Although there are numerous studies on
recommender systems for movies, music, and e-commerce, comparatively less
attention has been paid to the recommender system for NFTs despite the
continuous growth of the NFT market. This paper presents a recommender system
for NFTs that utilizes a variety of data sources, from NFT transaction records
to external item features, to generate precise recommendations that cater to
individual preferences. We develop a data-efficient graph-based recommender
system to efficiently capture the complex relationship between each item and
users and generate node(item) embeddings which incorporate both node feature
information and graph structure. Furthermore, we exploit inputs beyond
user-item interactions, such as image feature, text feature, and price feature.
Numerical experiments verify the performance of the graph-based recommender
system improves significantly after utilizing all types of item features as
side information, thereby outperforming all other baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the AAAI 2023 Bridge on AI for Financial Services
  (https://sites.google.com/view/aaai-ai-fin/home)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Out-of-Vocabulary Handling in <span class="highlight-title">Recommendation</span> Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18280v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18280v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Shiao, Mingxuan Ju, Zhichun Guo, Xin Chen, Evangelos Papalexakis, Tong Zhao, Neil Shah, Yozen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommendation systems (RS) are an increasingly relevant area for both
academic and industry researchers, given their widespread impact on the daily
online experiences of billions of users. One common issue in real RS is the
cold-start problem, where users and items may not contain enough information to
produce high-quality recommendations. This work focuses on a complementary
problem: recommending new users and items unseen (out-of-vocabulary, or OOV) at
training time. This setting is known as the inductive setting and is especially
problematic for factorization-based models, which rely on encoding only those
users/items seen at training time with fixed parameter vectors. Many existing
solutions applied in practice are often naive, such as assigning OOV
users/items to random buckets. In this work, we tackle this problem and propose
approaches that better leverage available user/item features to improve OOV
handling at the embedding table level. We discuss general-purpose plug-and-play
approaches that are easily applicable to most RS models and improve inductive
performance without negatively impacting transductive model performance. We
extensively evaluate 9 OOV embedding methods on 5 models across 4 datasets
(spanning different domains). One of these datasets is a proprietary production
dataset from a prominent RS employed by a large social platform serving
hundreds of millions of daily active users. In our experiments, we find that
several proposed methods that exploit feature similarity using LSH consistently
outperform alternatives on most model-dataset combinations, with the best
method showing a mean improvement of 3.74% over the industry standard baseline
in inductive performance. We release our code and hope our work helps
practitioners make more informed decisions when handling OOV for their RS and
further inspires academic research into improving OOV support in RS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RankMamba, <span class="highlight-title">Benchmark</span>ing Mamba's Document Ranking Performance in the Era
  of Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichao Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer structure has achieved great success in multiple applied machine
learning communities, such as natural language processing (NLP), computer
vision (CV) and information retrieval (IR). Transformer architecture's core
mechanism -- attention requires $O(n^2)$ time complexity in training and $O(n)$
time complexity in inference. Many works have been proposed to improve the
attention mechanism's scalability, such as Flash Attention and Multi-query
Attention. A different line of work aims to design new mechanisms to replace
attention. Recently, a notable model structure -- Mamba, which is based on
state space models, has achieved transformer-equivalent performance in multiple
sequence modeling tasks.
  In this work, we examine \mamba's efficacy through the lens of a classical IR
task -- document ranking. A reranker model takes a query and a document as
input, and predicts a scalar relevance score. This task demands the language
model's ability to comprehend lengthy contextual inputs and to capture the
interaction between query and document tokens. We find that (1) Mamba models
achieve competitive performance compared to transformer-based models with the
same training recipe; (2) but also have a lower training throughput in
comparison to efficient transformer implementations such as flash attention. We
hope this study can serve as a starting point to explore Mamba models in other
classical IR tasks. Our code implementation and trained checkpoints are made
public to facilitate
reproducibility.\footnote{https://github.com/zhichaoxu-shufe/RankMamba}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One Backpropagation in Two Tower <span class="highlight-title">Recommendation</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18227v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18227v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erjia Chen, Bang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed extensive researches on developing two tower
recommendation models for relieving information overload. Four building modules
can be identified in such models, namely, user-item encoding, negative
sampling, loss computing and back-propagation updating. To the best of our
knowledge, existing algorithms have researched only on the first three modules,
yet neglecting the backpropagation module. They all adopt a kind of two
backpropagation strategy, which are based on an implicit assumption of equally
treating users and items in the training phase. In this paper, we challenge
such an equal training assumption and propose a novel one backpropagation
updating strategy, which keeps the normal gradient backpropagation for the item
encoding tower, but cuts off the backpropagation for the user encoding tower.
Instead, we propose a moving-aggregation updating strategy to update a user
encoding in each training epoch. Except the proposed backpropagation updating
module, we implement the other three modules with the most straightforward
choices. Experiments on four public datasets validate the effectiveness and
efficiency of our model in terms of improved recommendation performance and
reduced computation overload over the state-of-the-art competitors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can AI Models Appreciate Document Aesthetics? An Exploration of
  Legibility and Layout Quality in Relation to <span class="highlight-title">Predict</span>ion Confidence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hsiu-Wei Yang, Abhinav Agrawal, Pavlos Fragkogiannis, Shubham Nitin Mulay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A well-designed document communicates not only through its words but also
through its visual eloquence. Authors utilize aesthetic elements such as
colors, fonts, graphics, and layouts to shape the perception of information.
Thoughtful document design, informed by psychological insights, enhances both
the visual appeal and the comprehension of the content. While state-of-the-art
document AI models demonstrate the benefits of incorporating layout and image
data, it remains unclear whether the nuances of document aesthetics are
effectively captured. To bridge the gap between human cognition and AI
interpretation of aesthetic elements, we formulated hypotheses concerning AI
behavior in document understanding tasks, specifically anchored in document
design principles. With a focus on legibility and layout quality, we tested
four aspects of aesthetic effects: noise, font-size contrast, alignment, and
complexity, on model confidence using correlational analysis. The results and
observations highlight the value of model analysis rooted in document design
theories. Our work serves as a trailhead for further studies and we advocate
for continued research in this topic to deepen our understanding of how AI
interprets document aesthetics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMs in HCI Data Work: Bridging the Gap Between Information Retrieval
  and Responsible Research Practices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18173v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18173v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neda Taghizadeh Serajeh, Iman Mohammadi, Vittorio Fuccella, Mattia De Rosa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient and accurate information extraction from scientific papers is
significant in the rapidly developing human-computer interaction research in
the literature review process. Our paper introduces and analyses a new
information retrieval system using state-of-the-art Large Language Models
(LLMs) in combination with structured text analysis techniques to extract
experimental data from HCI literature, emphasizing key elements. Then We
analyze the challenges and risks of using LLMs in the world of research. We
performed a comprehensive analysis on our conducted dataset, which contained
the specified information of 300 CHI 2020-2022 papers, to evaluate the
performance of the two large language models, GPT-3.5 (text-davinci-003) and
Llama-2-70b, paired with structured text analysis techniques. The GPT-3.5 model
gains an accuracy of 58\% and a mean absolute error of 7.00. In contrast, the
Llama2 model indicates an accuracy of 56\% with a mean absolute error of 7.63.
The ability to answer questions was also included in the system in order to
work with streamlined data. By evaluating the risks and opportunities presented
by LLMs, our work contributes to the ongoing dialogue on establishing
methodological validity and ethical guidelines for LLM use in HCI data work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, CHI2024 Workshop on LLMs as Research Tools: Applications and
  Evaluations in HCI Data Work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Trustworthy Reranking: A Simple yet Effective Abstention
  Mechanism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12997v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12997v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hippolyte Gisserot-Boukhlef, Manuel Faysse, Emmanuel Malherbe, Céline Hudelot, Pierre Colombo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Information Retrieval (NIR) has significantly improved upon
heuristic-based IR systems. Yet, failures remain frequent, the models used
often being unable to retrieve documents relevant to the user's query. We
address this challenge by proposing a lightweight abstention mechanism tailored
for real-world constraints, with particular emphasis placed on the reranking
phase. We introduce a protocol for evaluating abstention strategies in a
black-box scenario, demonstrating their efficacy, and propose a simple yet
effective data-driven mechanism. We provide open-source code for experiment
replication and abstention implementation, fostering wider adoption and
application in diverse contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLatrieval: LLM-Verified Retrieval for Verifiable Generation <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07838v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07838v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaonan Li, Changtai Zhu, Linyang Li, Zhangyue Yin, Tianxiang Sun, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Verifiable generation aims to let the large language model (LLM) generate
text with supporting documents, which enables the user to flexibly verify the
answer and makes the LLM's output more reliable. Retrieval plays a crucial role
in verifiable generation. Specifically, the retrieved documents not only
supplement knowledge to help the LLM generate correct answers, but also serve
as supporting evidence for the user to verify the LLM's output. However, the
widely used retrievers become the bottleneck of the entire pipeline and limit
the overall performance. Their capabilities are usually inferior to LLMs since
they often have much fewer parameters than the large language model and have
not been demonstrated to scale well to the size of LLMs. If the retriever does
not correctly find the supporting documents, the LLM can not generate the
correct and verifiable answer, which overshadows the LLM's remarkable
abilities. To address these limitations, we propose \LLatrieval (Large Language
Model Verified Retrieval), where the LLM updates the retrieval result until it
verifies that the retrieved documents can sufficiently support answering the
question. Thus, the LLM can iteratively provide feedback to retrieval and
facilitate the retrieval result to fully support verifiable generation.
Experiments show that LLatrieval significantly outperforms extensive baselines
and achieves state-of-the-art results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NAACL 2024 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MetaSplit: Meta-Split Network for Limited-Stock Product <span class="highlight-title">Recommendation</span> <span class="chip">WWW 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06747v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06747v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Wu, Jialiang Zhou, Ailong He, Shuguang Han, Jufeng Chen, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compared to business-to-consumer (B2C) e-commerce systems,
consumer-to-consumer (C2C) e-commerce platforms usually encounter the
limited-stock problem, that is, a product can only be sold one time in a C2C
system. This poses several unique challenges for click-through rate (CTR)
prediction. Due to limited user interactions for each product (i.e. item), the
corresponding item embedding in the CTR model may not easily converge. This
makes the conventional sequence modeling based approaches cannot effectively
utilize user history information since historical user behaviors contain a
mixture of items with different volume of stocks. Particularly, the attention
mechanism in a sequence model tends to assign higher score to products with
more accumulated user interactions, making limited-stock products being ignored
and contribute less to the final output. To this end, we propose the Meta-Split
Network (MSNet) to split user history sequence regarding to the volume of stock
for each product, and adopt differentiated modeling approaches for different
sequences. As for the limited-stock products, a meta-learning approach is
applied to address the problem of inconvergence, which is achieved by designing
meta scaling and shifting networks with ID and side information. In addition,
traditional approach can hardly update item embedding once the product is
consumed. Thereby, we propose an auxiliary loss that makes the parameters
updatable even when the product is no longer in distribution. To the best of
our knowledge, this is the first solution addressing the recommendation of
limited-stock product. Experimental results on the production dataset and
online A/B testing demonstrate the effectiveness of our proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WWW 2024. This work has already been deployed on the
  Xianyu platform in Alibaba. The first two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COPR -- Efficient, large-scale log storage and retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18355v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18355v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Reichinger, Thomas Krismayer, Jan Rellermeyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern, large scale monitoring systems have to process and store vast amounts
of log data in near real-time. At query time the systems have to find relevant
logs based on the content of the log message using support structures that can
scale to these amounts of data while still being efficient to use. We present
our novel Compressed Probabilistic Retrieval algorithm (COPR), capable of
answering Multi-Set Multi-Membership-Queries, that can be used as an
alternative to existing indexing structures for streamed log data. In our
experiments, COPR required up to 93% less storage space than the tested
state-of-the-art inverted index and had up to four orders of magnitude less
false-positives than the tested state-of-the-art membership sketch.
Additionally, COPR achieved up to 250 times higher query throughput than the
tested inverted index and up to 240 times higher query throughput than the
tested membership sketch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MA4DIV: Multi-Agent Reinforcement Learning for Search Result
  Diversification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17421v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17421v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqun Chen, Jiaxin Mao, Yi Zhang, Dehong Ma, Long Xia, Jun Fan, Daiting Shi, Zhicong Cheng, Simiu Gu, Dawei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The objective of search result diversification (SRD) is to ensure that
selected documents cover as many different subtopics as possible. Existing
methods primarily utilize a paradigm of "greedy selection", i.e., selecting one
document with the highest diversity score at a time. These approaches tend to
be inefficient and are easily trapped in a suboptimal state. In addition, some
other methods aim to approximately optimize the diversity metric, such as
$\alpha$-NDCG, but the results still remain suboptimal. To address these
challenges, we introduce Multi-Agent reinforcement learning (MARL) for search
result DIVersity, which called MA4DIV. In this approach, each document is an
agent and the search result diversification is modeled as a cooperative task
among multiple agents. This approach allows for directly optimizing the
diversity metrics, such as $\alpha$-NDCG, while achieving high training
efficiency. We conducted preliminary experiments on public TREC datasets to
demonstrate the effectiveness and potential of MA4DIV. Considering the limited
number of queries in public TREC datasets, we construct a large-scale dataset
from industry sources and show that MA4DIV achieves substantial improvements in
both effectiveness and efficiency than existing baselines on a industrial scale
dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language
  Models <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16915v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16915v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atsushi Keyaki, Ribeka Keyaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning in information retrieval systems using pre-trained language
models (PLM-based IR) requires learning query representations and
query-document relations, in addition to downstream task-specific learning.
This study introduces coarse-tuning as an intermediate learning stage that
bridges pre-training and fine-tuning. By learning query representations and
query-document relations in coarse-tuning, we aim to reduce the load of
fine-tuning and improve the learning effect of downstream IR tasks. We propose
Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the
appropriateness of query-document pairs. Evaluation experiments show that the
proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc
document retrieval datasets. Furthermore, the results of the query prediction
task suggested that coarse-tuning facilitated learning of query representation
and query-document relations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Network
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Peregrine: ML-based Malicious Traffic Detection for Terabit Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        João Romeiras Amado, Francisco Pereira, David Pissarra, Salvatore Signorello, Miguel Correia, Fernando M. V. Ramos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Malicious traffic detectors leveraging machine learning (ML), namely those
incorporating deep learning techniques, exhibit impressive detection
capabilities across multiple attacks. However, their effectiveness becomes
compromised when deployed in networks handling Terabit-speed traffic. In
practice, these systems require substantial traffic sampling to reconcile the
high data plane packet rates with the comparatively slower processing speeds of
ML detection. As sampling significantly reduces traffic observability, it
fundamentally undermines their detection capability.
  We present Peregrine, an ML-based malicious traffic detector for Terabit
networks. The key idea is to run the detection process partially in the network
data plane. Specifically, we offload the detector's ML feature computation to a
commodity switch. The Peregrine switch processes a diversity of features
per-packet, at Tbps line rates - three orders of magnitude higher than the
fastest detector - to feed the ML-based component in the control plane. Our
offloading approach presents a distinct advantage. While, in practice, current
systems sample raw traffic, in Peregrine sampling occurs after feature
computation. This essential trait enables computing features over all traffic,
significantly enhancing detection performance. The Peregrine detector is not
only effective for Terabit networks, but it is also energy- and cost-efficient.
Further, by shifting a compute-heavy component to the switch, it saves precious
CPU cycles and improves detection throughput.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast Decision Algorithms for Efficient Access Point Assignment in
  SDN-Controlled Wireless Access Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18745v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18745v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo Fondo-Ferreiro, Saber Mhiri, Cristina López-Bravo, Francisco Javier González-Castaño, Felipe Gil-Castiñeira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Global optimization of access point (AP) assignment to user terminals
requires efficient monitoring of user behavior, fast decision algorithms,
efficient control signaling, and fast AP reassignment mechanisms. In this
scenario, software defined networking (SDN) technology may be suitable for
network monitoring, signaling, and control. We recently proposed embedding
virtual switches in user terminals for direct management by an SDN controller,
further contributing to SDN-oriented access network optimization. However,
since users may restrict terminal-side traffic monitoring for privacy reasons
(a common assumption by previous authors), we infer user traffic classes at the
APs. On the other hand, since handovers will be more frequent in dense
small-cell networks (e.g., mmWave-based 5G deployments will require dense
network topologies with inter-site distances of ~150-200 m), the delay to take
assignment decisions should be minimal. To this end, we propose taking fast
decisions based exclusively on extremely simple network-side application
flow-type predictions based on past user behavior. Using real data we show that
a centralized allocation algorithm based on those predictions achieves network
utilization levels that approximate those of optimal allocations. We also test
a distributed version of this algorithm. Finally, we quantify the elapsed time
since a user traffic event takes place until its terminal is assigned an AP,
when needed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted version of the article published in IEEE Transactions on
  Network and Service Management</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ qIoV: A Quantum-Driven Internet-of-Vehicles-Based Approach for
  Environmental Monitoring and Rapid Response Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18622v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18622v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ankur Nahar, Koustav Kumar Mondal, Debasis Das, Rajkumar Buyya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research addresses the critical necessity for advanced rapid response
operations in managing a spectrum of environmental hazards. We propose a novel
framework, qIoV that integrates quantum computing with the Internet-of-Vehicles
(IoV) to leverage the computational efficiency, parallelism, and entanglement
properties of quantum mechanics. Our approach involves the use of environmental
sensors mounted on vehicles for precise air quality assessment. These sensors
are designed to be highly sensitive and accurate, leveraging the principles of
quantum mechanics to detect and measure environmental parameters. A salient
feature of our proposal is the Quantum Mesh Network Fabric (QMF), a system
designed to dynamically adjust the quantum network topology in accordance with
vehicular movements. This capability is critical to maintaining the integrity
of quantum states against environmental and vehicular disturbances, thereby
ensuring reliable data transmission and processing. Moreover, our methodology
is further augmented by the incorporation of a variational quantum classifier
(VQC) with advanced quantum entanglement techniques. This integration offers a
significant reduction in latency for hazard alert transmission, thus enabling
expedited communication of crucial data to emergency response teams and the
public. Our study on the IBM OpenQSAM 3 platform, utilizing a 127 Qubit system,
revealed significant advancements in pair plot analysis, achieving over 90% in
precision, recall, and F1-Score metrics and an 83% increase in the speed of
toxic gas detection compared to conventional methods.Additionally, theoretical
analyses validate the efficiency of quantum rotation, teleportation protocols,
and the fidelity of quantum entanglement, further underscoring the potential of
quantum computing in enhancing analytical performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How to Cache Important Contents for Multi-modal Service in Dynamic
  Networks: A DRL-based Caching Scheme 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18323v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18323v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Zhang, Marc St-Hilaire, Xin Wei, Haiwei Dong, Abdulmotaleb El Saddik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the continuous evolution of networking technologies, multi-modal
services that involve video, audio, and haptic contents are expected to become
the dominant multimedia service in the near future. Edge caching is a key
technology that can significantly reduce network load and content transmission
latency, which is critical for the delivery of multi-modal contents. However,
existing caching approaches only rely on a limited number of factors, e.g.,
popularity, to evaluate their importance for caching, which is inefficient for
caching multi-modal contents, especially in dynamic network environments. To
overcome this issue, we propose a content importance-based caching scheme which
consists of a content importance evaluation model and a caching model. By
leveraging dueling double deep Q networks (D3QN) model, the content importance
evaluation model can adaptively evaluate contents' importance in dynamic
networks. Based on the evaluated contents' importance, the caching model can
easily cache and evict proper contents to improve caching efficiency. The
simulation results show that the proposed content importance-based caching
scheme outperforms existing caching schemes in terms of caching hit ratio (at
least 15% higher), reduced network load (up to 22% reduction), average number
of hops (up to 27% lower), and unsatisfied requests ratio (more than 47%
reduction).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CrystalBox: Future-Based Explanations for Input-Driven Deep RL Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.13483v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.13483v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sagar Patel, Sangeetha Abdu Jyothi, Nina Narodytska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present CrystalBox, a novel, model-agnostic, posthoc explainability
framework for Deep Reinforcement Learning (DRL) controllers in the large family
of input-driven environments which includes computer systems. We combine the
natural decomposability of reward functions in input-driven environments with
the explanatory power of decomposed returns. We propose an efficient algorithm
to generate future-based explanations across both discrete and continuous
control environments. Using applications such as adaptive bitrate streaming and
congestion control, we demonstrate CrystalBox's capability to generate
high-fidelity explanations. We further illustrate its higher utility across
three practical use cases: contrastive explanations, network observability, and
guided reward design, as opposed to prior explainability techniques that
identify salient features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RACH-less Handover with Early Timing Advance Acquisition for Outage
  Reduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10286v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10286v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subhyal Bin Iqbal, Umur Karabulut, Ahmad Awada, Philipp Schulz, Gerhard P. Fettweis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For fifth-generation (5G) and 5G-Advanced networks, outage reduction within
the context of reliability is a key objective since outage denotes the time
period when a user equipment (UE) cannot communicate with the network. Earlier
studies have shown that in the experimental high mobility scenario considered,
outage is dominated by the interruption time that stems from the random access
channel (RACH)-based handover process from the serving cell to the target cell.
A handover by itself is a necessary mobility process to prevent mobility
failures and their associated outage. This paper proposes a RACH-less handover
signaling scheme for the 3rd Generation Partnership Project (3GPP) conditional
handover (CHO) mechanism. The proposed scheme exploits the decoupling between
the CHO preparation and execution phases to establish initial synchronization
between the UE and the target cell through an early acquisition of the timing
advance. This significantly curtails the RACH process and therefore the
handover interruption time. Results based on a system-level simulation-based
mobility study have shown that the proposed scheme significantly reduces the
outage and its constituent handover interruption time relatively by 18.7% and
43.2%, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures. Accepted for presentation at the 2024 IEEE 99th
  Vehicular Technology Conference (VTC2024)-Spring to be held in Singapore</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Throughput Maximization in Multi-Band Optical Networks with Column
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07335v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07335v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cao Chen, Shilin Xiao, Fen Zhou, Massimo Tornatore
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-band transmission is a promising technical direction for spectrum and
capacity expansion of existing optical networks. Due to the increase in the
number of usable wavelengths in multi-band optical networks, the complexity of
resource allocation problems becomes a major concern. Moreover, the
transmission performance, spectrum width, and cost constraint across optical
bands may be heterogeneous. Assuming a worst-case transmission margin in U, L,
and C-bands, this paper investigates the problem of throughput maximization in
multi-band optical networks, including the optimization of route, wavelength,
and band assignment. We propose a low-complexity decomposition approach based
on Column Generation (CG) to address the scalability issue faced by traditional
methodologies. We numerically compare the results obtained by our CG-based
approach to an integer linear programming model, confirming the near-optimal
network throughput. Our results also demonstrate the scalability of the
CG-based approach when the number of wavelengths increases, with the
computation time in the magnitude order of 10 s for cases varying from 75 to
1200 wavelength channels per link in a 14-node network. Code of this
publication is available at github.com/cchen000/CG-Multi-Band.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures, accepted by IEEE International Conference on
  Communications 2024 (ICC2024)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-26T00:00:00Z">2024-03-26</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Oh! We Freeze: Improving <span class="highlight-title">Quantized</span> Knowledge Distillation via Signal
  Propagation Analysis for Large Language Models <span class="chip">ICLR
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kartikeya Bhardwaj, Nilesh Prasad Pandey, Sweta Priyadarshi, Kyunggeun Lee, Jun Ma, Harris Teague
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large generative models, such as large language models (LLMs) and diffusion
models have as revolutionized the fields of NLP and computer vision
respectively. However, their slow inference, high computation and memory
requirement makes it challenging to deploy them on edge devices. In this study,
we propose a light-weight quantization aware fine tuning technique using
knowledge distillation (KD-QAT) to improve the performance of 4-bit weight
quantized LLMs using commonly available datasets to realize a popular language
use case, on device chat applications. To improve this paradigm of finetuning,
as main contributions, we provide insights into stability of KD-QAT by
empirically studying the gradient propagation during training to better
understand the vulnerabilities of KD-QAT based approaches to low-bit
quantization errors. Based on our insights, we propose ov-freeze, a simple
technique to stabilize the KD-QAT process. Finally, we experiment with the
popular 7B LLaMAv2-Chat model at 4-bit quantization level and demonstrate that
ov-freeze results in near float-point precision performance, i.e., less than
0.7% loss of accuracy on Commonsense Reasoning benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Practical ML for Low Resource Settings Workshop at ICLR
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models Produce Responses Perceived to be Empathic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18148v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18148v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoon Kyung Lee, Jina Suh, Hongli Zhan, Junyi Jessy Li, Desmond C. Ong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated surprising performance on many
tasks, including writing supportive messages that display empathy. Here, we had
these models generate empathic messages in response to posts describing common
life experiences, such as workplace situations, parenting, relationships, and
other anxiety- and anger-eliciting situations. Across two studies (N=192, 202),
we showed human raters a variety of responses written by several models (GPT4
Turbo, Llama2, and Mistral), and had people rate these responses on how
empathic they seemed to be. We found that LLM-generated responses were
consistently rated as more empathic than human-written responses. Linguistic
analyses also show that these models write in distinct, predictable ``styles",
in terms of their use of punctuation, emojis, and certain words. These results
highlight the potential of using LLMs to enhance human peer support in contexts
where empathy is important.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Real-Time Rescheduling Algorithm for Multi-robot Plan Execution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Feng, Adittyo Paul, Zhe Chen, Jiaoyang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One area of research in multi-agent path finding is to determine how
replanning can be efficiently achieved in the case of agents being delayed
during execution. One option is to reschedule the passing order of agents,
i.e., the sequence in which agents visit the same location. In response, we
propose Switchable-Edge Search (SES), an A*-style algorithm designed to find
optimal passing orders. We prove the optimality of SES and evaluate its
efficiency via simulations. The best variant of SES takes less than 1 second
for small- and medium-sized problems and runs up to 4 times faster than
baselines for large-sized problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICAPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Juru: Legal Brazilian Large Language Model from Reputable Sources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roseval Malaquias Junior, Ramon Pires, Roseli Romero, Rodrigo Nogueira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The high computational cost associated with pretraining large language models
limits their research. Two strategies have emerged to address this issue:
domain specialization and pretraining with high-quality data. To explore these
strategies, we specialized the Sabi\'a-2 Small model with 1.9 billion unique
tokens from reputable Brazilian legal sources and conducted few-shot
evaluations on legal and general knowledge exams. Our model, Juru, demonstrates
the benefits of domain specialization with a reduced amount of pretraining
data. However, this specialization comes at the expense of degrading
performance in other knowledge areas within the same language. This study
contributes to the growing body of scientific evidence showing that pretraining
data selection may enhance the performance of large language models, enabling
the exploration of these models at a lower cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Securing GNNs: Explanation-Based Identification of Backdoored Training
  Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18136v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18136v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jane Downer, Ren Wang, Binghui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have gained popularity in numerous domains, yet
they are vulnerable to backdoor attacks that can compromise their performance
and ethical application. The detection of these attacks is crucial for
maintaining the reliability and security of GNN classification tasks, but
effective detection techniques are lacking. Following an initial investigation,
we observed that while graph-level explanations can offer limited insights,
their effectiveness in detecting backdoor triggers is inconsistent and
incomplete. To bridge this gap, we extract and transform secondary outputs of
GNN explanation mechanisms, designing seven novel metrics that more effectively
detect backdoor attacks. Additionally, we develop an adaptive attack to
rigorously evaluate our approach. We test our method on multiple benchmark
datasets and examine its efficacy against various attack models. Our results
show that our method can achieve high detection performance, marking a
significant advancement in safeguarding GNNs against backdoor attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AE SemRL: Learning Semantic Association Rules with Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18133v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18133v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erkan Karabulut, Victoria Degeler, Paul Groth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Association Rule Mining (ARM) is the task of learning associations among data
features in the form of logical rules. Mining association rules from
high-dimensional numerical data, for example, time series data from a large
number of sensors in a smart environment, is a computationally intensive task.
In this study, we propose an Autoencoder-based approach to learn and extract
association rules from time series data (AE SemRL). Moreover, we argue that in
the presence of semantic information related to time series data sources,
semantics can facilitate learning generalizable and explainable association
rules. Despite enriching time series data with additional semantic features, AE
SemRL makes learning association rules from high-dimensional data feasible. Our
experiments show that semantic association rules can be extracted from a latent
representation created by an Autoencoder and this method has in the order of
hundreds of times faster execution time than state-of-the-art ARM approaches in
many scenarios. We believe that this study advances a new way of extracting
associations from representations and has the potential to inspire more
research in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Recommendation</span> of data-free class-incremental learning algorithms by
  simulating future data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eva Feillet, Adrian Popescu, Céline Hudelot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-incremental learning deals with sequential data streams composed of
batches of classes. Various algorithms have been proposed to address the
challenging case where samples from past classes cannot be stored. However,
selecting an appropriate algorithm for a user-defined setting is an open
problem, as the relative performance of these algorithms depends on the
incremental settings. To solve this problem, we introduce an algorithm
recommendation method that simulates the future data stream. Given an initial
set of classes, it leverages generative models to simulate future classes from
the same visual domain. We evaluate recent algorithms on the simulated stream
and recommend the one which performs best in the user-defined incremental
setting. We illustrate the effectiveness of our method on three large datasets
using six algorithms and six incremental settings. Our method outperforms
competitive baselines, and performance is close to that of an oracle choosing
the best algorithm in each setting. This work contributes to facilitate the
practical deployment of incremental learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with
  Autoformalization <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Peng Zhou, Charles Staats, Wenda Li, Christian Szegedy, Kilian Q. Weinberger, Yuhuai Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLM), such as Google's Minerva and OpenAI's GPT
families, are becoming increasingly capable of solving mathematical
quantitative reasoning problems. However, they still make unjustified logical
and computational errors in their reasoning steps and answers. In this paper,
we leverage the fact that if the training corpus of LLMs contained sufficiently
many examples of formal mathematics (e.g. in Isabelle, a formal theorem proving
environment), they can be prompted to translate i.e. autoformalize informal
mathematical statements into formal Isabelle code -- which can be verified
automatically for internal consistency. This provides a mechanism to
automatically reject solutions whose formalized versions are inconsistent
within themselves or with the formalized problem statement. We evaluate our
method on GSM8K, MATH and MultiArith datasets and demonstrate that our approach
provides a consistently better heuristic than vanilla majority voting -- the
previously best method to identify correct answers, by more than 12% on GSM8K.
In our experiments it improves results consistently across all datasets and LLM
model sizes. The code can be found at https://github.com/jinpz/dtv.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sabiá-2: A New Generation of Portuguese Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09887v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09887v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thales Sales Almeida, Hugo Abonizio, Rodrigo Nogueira, Ramon Pires
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Sabi\'a-2, a family of large language models trained on
Portuguese texts. The models are evaluated on a diverse range of exams,
including entry-level tests for Brazilian universities, professional
certification exams, and graduate-level exams for various disciplines such as
accounting, economics, engineering, law and medicine. Our results reveal that
our best model so far, Sabi\'a-2 Medium, matches or surpasses GPT-4's
performance in 23 out of 64 exams and outperforms GPT-3.5 in 58 out of 64
exams. Notably, specialization has a significant impact on a model's
performance without the need to increase its size, allowing us to offer
Sabi\'a-2 Medium at a price per token that is 10 times cheaper than GPT-4.
Finally, we identified that math and coding are key abilities that need
improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HIVE: Harnessing Human Feedback for Instructional Visual Editing <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09618v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09618v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, Caiming Xiong, Ran Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incorporating human feedback has been shown to be crucial to align text
generated by large language models to human preferences. We hypothesize that
state-of-the-art instructional image editing models, where outputs are
generated based on an input image and an editing instruction, could similarly
benefit from human feedback, as their outputs may not adhere to the correct
instructions and preferences of users. In this paper, we present a novel
framework to harness human feedback for instructional visual editing (HIVE).
Specifically, we collect human feedback on the edited images and learn a reward
function to capture the underlying user preferences. We then introduce scalable
diffusion model fine-tuning methods that can incorporate human preferences
based on the estimated reward. Besides, to mitigate the bias brought by the
limitation of data, we contribute a new 1M training dataset, a 3.6K reward
dataset for rewards learning, and a 1K evaluation dataset to boost the
performance of instructional image editing. We conduct extensive empirical
experiments quantitatively and qualitatively, showing that HIVE is favored over
previous state-of-the-art instructional image editing approaches by a large
margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In CVPR, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Batched Low-Rank Adaptation of Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05677v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05677v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeming Wen, Swarat Chaudhuri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-Rank Adaptation (LoRA) has recently gained attention for fine-tuning
foundation models by incorporating trainable low-rank matrices, thereby
reducing the number of trainable parameters. While LoRA offers numerous
advantages, its applicability for real-time serving to a diverse and global
user base is constrained by its incapability to handle multiple task-specific
adapters efficiently. This imposes a performance bottleneck in scenarios
requiring personalized, task-specific adaptations for each incoming request. To
mitigate this constraint, we introduce Fast LoRA (FLoRA), a framework in which
each input example in a minibatch can be associated with its unique low-rank
adaptation weights, allowing for efficient batching of heterogeneous requests.
We empirically demonstrate that FLoRA retains the performance merits of LoRA,
showcasing competitive results on the MultiPL-E code generation benchmark
spanning over 8 languages and a multilingual speech recognition task across 6
languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Pre-trained Language Model Sensitivity via Mask Specific
  losses: A case study on Biomedical NER <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18025v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18025v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Micheal Abaho, Danushka Bollegala, Gary Leeming, Dan Joyce, Iain E Buchan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adapting language models (LMs) to novel domains is often achieved through
fine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning
introduces new knowledge into an LM, enabling it to comprehend and efficiently
perform a target domain task. Fine-tuning can however be inadvertently
insensitive if it ignores the wide array of disparities (e.g in word meaning)
between source and target domains. For instance, words such as chronic and
pressure may be treated lightly in social conversations, however, clinically,
these words are usually an expression of concern. To address insensitive
fine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach
that efficiently acquires target domain knowledge by appropriately weighting
the importance of domain-specific terms (DS-terms) during fine-tuning. MSLM
jointly masks DS-terms and generic words, then learns mask-specific losses by
ensuring LMs incur larger penalties for inaccurately predicting DS-terms
compared to generic words. Results of our analysis show that MSLM improves LMs
sensitivity and detection of DS-terms. We empirically show that an optimal
masking rate not only depends on the LM, but also on the dataset and the length
of sequences. Our proposed masking strategy outperforms advanced masking
strategies such as span- and PMI-based masking.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper alrerady accepted for publishing by the NAACL 2024 conference
  (main conference paper)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Search and Society: Reimagining Information Access for Radical Futures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17901v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17901v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhaskar Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information retrieval (IR) technologies and research are undergoing
transformative changes. It is our perspective that the community should accept
this opportunity to re-center our research agendas on societal needs while
dismantling the artificial separation between the work on fairness,
accountability, transparency, and ethics in IR and the rest of IR research.
Instead of adopting a reactionary strategy of trying to mitigate potential
social harms from emerging technologies, the community should aim to
proactively set the research agenda for the kinds of systems we should build
inspired by diverse explicitly stated sociotechnical imaginaries. The
sociotechnical imaginaries that underpin the design and development of
information access technologies needs to be explicitly articulated, and we need
to develop theories of change in context of these diverse perspectives. Our
guiding future imaginaries must be informed by other academic fields, such as
democratic theory and critical theory, and should be co-developed with social
science scholars, legal scholars, civil rights and social justice activists,
and artists, among others. In this perspective paper, we motivate why the
community must consider this radical shift in how we do research and what we
work on, and sketch a path forward towards this transformation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIND Your Language: A Multilingual <span class="highlight-title">Dataset</span> for Cross-lingual News
  <span class="highlight-title">Recommendation</span> <span class="chip">SIGIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17876v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17876v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreea Iana, Goran Glavaš, Heiko Paulheim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Digital news platforms use news recommenders as the main instrument to cater
to the individual information needs of readers. Despite an increasingly
language-diverse online community, in which many Internet users consume news in
multiple languages, the majority of news recommendation focuses on major,
resource-rich languages, and English in particular. Moreover, nearly all news
recommendation efforts assume monolingual news consumption, whereas more and
more users tend to consume information in at least two languages. Accordingly,
the existing body of work on news recommendation suffers from a lack of
publicly available multilingual benchmarks that would catalyze development of
news recommenders effective in multilingual settings and for low-resource
languages. Aiming to fill this gap, we introduce xMIND, an open, multilingual
news recommendation dataset derived from the English MIND dataset using machine
translation, covering a set of 14 linguistically and geographically diverse
languages, with digital footprints of varying sizes. Using xMIND, we
systematically benchmark several state-of-the-art content-based neural news
recommenders (NNRs) in both zero-shot (ZS-XLT) and few-shot (FS-XLT)
cross-lingual transfer scenarios, considering both monolingual and bilingual
news consumption patterns. Our findings reveal that (i) current NNRs, even when
based on a multilingual language model, suffer from substantial performance
losses under ZS-XLT and that (ii) inclusion of target-language data in FS-XLT
training has limited benefits, particularly when combined with a bilingual news
consumption. Our findings thus warrant a broader research effort in
multilingual and cross-lingual news recommendation. The xMIND dataset is
available at https://github.com/andreeaiana/xMIND.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 47th International ACM SIGIR Conference on Research
  and Development in Information Retrieval (SIGIR 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ArabicaQA: A Comprehensive <span class="highlight-title">Dataset</span> for Arabic Question Answering <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrahman Abdallah, Mahmoud Kasem, Mahmoud Abdalla, Mohamed Mahmoud, Mohamed Elkasaby, Yasser Elbendary, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address the significant gap in Arabic natural language
processing (NLP) resources by introducing ArabicaQA, the first large-scale
dataset for machine reading comprehension and open-domain question answering in
Arabic. This comprehensive dataset, consisting of 89,095 answerable and 3,701
unanswerable questions created by crowdworkers to look similar to answerable
ones, along with additional labels of open-domain questions marks a crucial
advancement in Arabic NLP resources. We also present AraDPR, the first dense
passage retrieval model trained on the Arabic Wikipedia corpus, specifically
designed to tackle the unique challenges of Arabic text retrieval. Furthermore,
our study includes extensive benchmarking of large language models (LLMs) for
Arabic question answering, critically evaluating their performance in the
Arabic language context. In conclusion, ArabicaQA, AraDPR, and the benchmarking
of LLMs in Arabic question answering offer significant advancements in the
field of Arabic NLP. The dataset and code are publicly accessible for further
research https://github.com/DataScienceUIBK/ArabicaQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SIGIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CaseLink: Inductive Graph Learning for Legal Case Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanran Tang, Ruihong Qiu, Hongzhi Yin, Xue Li, Zi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In case law, the precedents are the relevant cases that are used to support
the decisions made by the judges and the opinions of lawyers towards a given
case. This relevance is referred to as the case-to-case reference relation. To
efficiently find relevant cases from a large case pool, retrieval tools are
widely used by legal practitioners. Existing legal case retrieval models mainly
work by comparing the text representations of individual cases. Although they
obtain a decent retrieval accuracy, the intrinsic case connectivity
relationships among cases have not been well exploited for case encoding,
therefore limiting the further improvement of retrieval performance. In a case
pool, there are three types of case connectivity relationships: the case
reference relationship, the case semantic relationship, and the case legal
charge relationship. Due to the inductive manner in the task of legal case
retrieval, using case reference as input is not applicable for testing. Thus,
in this paper, a CaseLink model based on inductive graph learning is proposed
to utilise the intrinsic case connectivity for legal case retrieval, a novel
Global Case Graph is incorporated to represent both the case semantic
relationship and the case legal charge relationship. A novel contrastive
objective with a regularisation on the degree of case nodes is proposed to
leverage the information carried by the case reference relationship to optimise
the model. Extensive experiments have been conducted on two benchmark datasets,
which demonstrate the state-of-the-art performance of CaseLink. The code has
been released on https://github.com/yanran-tang/CaseLink.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TWOLAR: a TWO-step LLM-Augmented distillation method for passage
  Reranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Baldelli, Junfeng Jiang, Akiko Aizawa, Paolo Torroni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present TWOLAR: a two-stage pipeline for passage reranking
based on the distillation of knowledge from Large Language Models (LLM). TWOLAR
introduces a new scoring strategy and a distillation process consisting in the
creation of a novel and diverse training dataset. The dataset consists of 20K
queries, each associated with a set of documents retrieved via four distinct
retrieval methods to ensure diversity, and then reranked by exploiting the
zero-shot reranking capabilities of an LLM. Our ablation studies demonstrate
the contribution of each new component we introduced. Our experimental results
show that TWOLAR significantly enhances the document reranking ability of the
underlying model, matching and in some cases even outperforming
state-of-the-art models with three orders of magnitude more parameters on the
TREC-DL test sets and the zero-shot evaluation benchmark BEIR. To facilitate
future work we release our data set, finetuned models, and code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ All-in-One: Heterogeneous Interaction Modeling for Cold-Start Rating
  <span class="highlight-title">Predict</span>ion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuheng Fang, Kangfei Zhao, Yu Rong, Zhixun Li, Jeffrey Xu Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cold-start rating prediction is a fundamental problem in recommender systems
that has been extensively studied. Many methods have been proposed that exploit
explicit relations among existing data, such as collaborative filtering, social
recommendations and heterogeneous information network, to alleviate the data
insufficiency issue for cold-start users and items. However, the explicit
relations constructed based on data between different roles may be unreliable
and irrelevant, which limits the performance ceiling of the specific
recommendation task. Motivated by this, in this paper, we propose a flexible
framework dubbed heterogeneous interaction rating network (HIRE). HIRE dose not
solely rely on the pre-defined interaction pattern or the manually constructed
heterogeneous information network. Instead, we devise a Heterogeneous
Interaction Module (HIM) to jointly model the heterogeneous interactions and
directly infer the important interactions via the observed data. In the
experiments, we evaluate our model under three cold-start settings on three
real-world datasets. The experimental results show that HIRE outperforms other
baselines by a large margin. Furthermore, we visualize the inferred
interactions of HIRE to confirm the contribution of our model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EulerFormer: Sequential User Behavior Modeling with Complex Vector
  Attention <span class="chip">SIGIR'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Tian, Wayne Xin Zhao, Changwang Zhang, Xin Zhao, Zhongrui Ma, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To capture user preference, transformer models have been widely applied to
model sequential user behavior data. The core of transformer architecture lies
in the self-attention mechanism, which computes the pairwise attention scores
in a sequence. Due to the permutation-equivariant nature, positional encoding
is used to enhance the attention between token representations. In this
setting, the pairwise attention scores can be derived by both semantic
difference and positional difference. However, prior studies often model the
two kinds of difference measurements in different ways, which potentially
limits the expressive capacity of sequence modeling. To address this issue,
this paper proposes a novel transformer variant with complex vector attention,
named EulerFormer, which provides a unified theoretical framework to formulate
both semantic difference and positional difference. The EulerFormer involves
two key technical improvements. First, it employs a new transformation function
for efficiently transforming the sequence tokens into polar-form complex
vectors using Euler's formula, enabling the unified modeling of both semantic
and positional information in a complex rotation form.Secondly, it develops a
differential rotation mechanism, where the semantic rotation angles can be
controlled by an adaptation function, enabling the adaptive integration of the
semantic and positional information according to the semantic
contexts.Furthermore, a phase contrastive learning task is proposed to improve
the anisotropy of contextual representations in EulerFormer. Our theoretical
framework possesses a high degree of completeness and generality. It is more
robust to semantic variations and possesses moresuperior theoretical properties
in principle. Extensive experiments conducted on four public datasets
demonstrate the effectiveness and efficiency of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in SIGIR'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models Enhanced Collaborative Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17688v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17688v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongxiang Sun, Zihua Si, Xiaoxue Zang, Kai Zheng, Yang Song, Xiao Zhang, Jun Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have attracted
considerable interest among researchers to leverage these models to enhance
Recommender Systems (RSs). Existing work predominantly utilizes LLMs to
generate knowledge-rich texts or utilizes LLM-derived embeddings as features to
improve RSs. Although the extensive world knowledge embedded in LLMs generally
benefits RSs, the application can only take limited number of users and items
as inputs, without adequately exploiting collaborative filtering information.
Considering its crucial role in RSs, one key challenge in enhancing RSs with
LLMs lies in providing better collaborative filtering information through LLMs.
In this paper, drawing inspiration from the in-context learning and chain of
thought reasoning in LLMs, we propose the Large Language Models enhanced
Collaborative Filtering (LLM-CF) framework, which distils the world knowledge
and reasoning capabilities of LLMs into collaborative filtering. We also
explored a concise and efficient instruction-tuning method, which improves the
recommendation capabilities of LLMs while preserving their general
functionalities (e.g., not decreasing on the LLM benchmark). Comprehensive
experiments on three real-world datasets demonstrate that LLM-CF significantly
enhances several backbone recommendation models and consistently outperforms
competitive baselines, showcasing its effectiveness in distilling the world
knowledge and reasoning capabilities of LLM into collaborative filtering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ S+t-SNE -- Bringing dimensionality reduction to data <span class="highlight-title">stream</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro C. Vieira, João P. Montrezol, João T. Vieira, João Gama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present S+t-SNE, an adaptation of the t-SNE algorithm designed to handle
infinite data streams. The core idea behind S+t-SNE is to update the t-SNE
embedding incrementally as new data arrives, ensuring scalability and
adaptability to handle streaming scenarios. By selecting the most important
points at each step, the algorithm ensures scalability while keeping
informative visualisations. Employing a blind method for drift management
adjusts the embedding space, facilitating continuous visualisation of evolving
data dynamics. Our experimental evaluations demonstrate the effectiveness and
efficiency of S+t-SNE. The results highlight its ability to capture patterns in
a streaming scenario. We hope our approach offers researchers and practitioners
a real-time tool for understanding and interpreting high-dimensional data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This preprint has not undergone peer review or any post-submission
  improvements or corrections. We will soon add a link to the final version of
  this contribution that underwent peer-review and post-acceptance improvements
  and was presented at IDA2024 (https://ida2024.org/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retentive Decision Transformer with Adaptive Masking for Reinforcement
  Learning based <span class="highlight-title">Recommendation</span> Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyu Wang, Xiaocong Chen, Lina Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning-based Recommender Systems (RLRS) have shown promise
across a spectrum of applications, from e-commerce platforms to streaming
services. Yet, they grapple with challenges, notably in crafting reward
functions and harnessing large pre-existing datasets within the RL framework.
Recent advancements in offline RLRS provide a solution for how to address these
two challenges. However, existing methods mainly rely on the transformer
architecture, which, as sequence lengths increase, can introduce challenges
associated with computational resources and training costs. Additionally, the
prevalent methods employ fixed-length input trajectories, restricting their
capacity to capture evolving user preferences. In this study, we introduce a
new offline RLRS method to deal with the above problems. We reinterpret the
RLRS challenge by modeling sequential decision-making as an inference task,
leveraging adaptive masking configurations. This adaptive approach selectively
masks input tokens, transforming the recommendation task into an inference
challenge based on varying token subsets, thereby enhancing the agent's ability
to infer across diverse trajectory lengths. Furthermore, we incorporate a
multi-scale segmented retention mechanism that facilitates efficient modeling
of long sequences, significantly enhancing computational efficiency. Our
experimental analysis, conducted on both online simulator and offline datasets,
clearly demonstrates the advantages of our proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ END4Rec: Efficient Noise-Decoupling for Multi-Behavior Sequential
  <span class="highlight-title">Recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongqiang Han, Hao Wang, Kefan Wang, Likang Wu, Zhi Li, Wei Guo, Yong Liu, Defu Lian, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recommendation systems, users frequently engage in multiple types of
behaviors, such as clicking, adding to a cart, and purchasing. However, with
diversified behavior data, user behavior sequences will become very long in the
short term, which brings challenges to the efficiency of the sequence
recommendation model. Meanwhile, some behavior data will also bring inevitable
noise to the modeling of user interests. To address the aforementioned issues,
firstly, we develop the Efficient Behavior Sequence Miner (EBM) that
efficiently captures intricate patterns in user behavior while maintaining low
time complexity and parameter count. Secondly, we design hard and soft
denoising modules for different noise types and fully explore the relationship
between behaviors and noise. Finally, we introduce a contrastive loss function
along with a guided training strategy to compare the valid information in the
data with the noisy signal, and seamlessly integrate the two denoising
processes to achieve a high degree of decoupling of the noisy signal.
Sufficient experiments on real-world datasets demonstrate the effectiveness and
efficiency of our approach in dealing with multi-behavior sequential
recommendation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Document Set Expansion with Positive-Unlabelled Learning Using
  Intractable Density Estimation <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17473v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17473v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiyang Zhang, Qiuyi Chen, Yuanjie Zou, Yushan Pan, Jia Wang, Mark Stevenson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Document Set Expansion (DSE) task involves identifying relevant documents
from large collections based on a limited set of example documents. Previous
research has highlighted Positive and Unlabeled (PU) learning as a promising
approach for this task. However, most PU methods rely on the unrealistic
assumption of knowing the class prior for positive samples in the collection.
To address this limitation, this paper introduces a novel PU learning framework
that utilizes intractable density estimation models. Experiments conducted on
PubMed and Covid datasets in a transductive setting showcase the effectiveness
of the proposed method for DSE. Code is available from
https://github.com/Beautifuldog01/Document-set-expansion-puDE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024. arXiv admin note: text overlap with
  arXiv:2401.11145</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Touch the Core: Exploring Task Dependence Among Hybrid Targets for
  <span class="highlight-title">Recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xing Tang, Yang Qiao, Fuyuan Lyu, Dugang Liu, Xiuqiang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As user behaviors become complicated on business platforms, online
recommendations focus more on how to touch the core conversions, which are
highly related to the interests of platforms. These core conversions are
usually continuous targets, such as \textit{watch time}, \textit{revenue}, and
so on, whose predictions can be enhanced by previous discrete conversion
actions. Therefore, multi-task learning (MTL) can be adopted as the paradigm to
learn these hybrid targets. However, existing works mainly emphasize
investigating the sequential dependence among discrete conversion actions,
which neglects the complexity of dependence between discrete conversions and
the final continuous conversion. Moreover, simultaneously optimizing hybrid
tasks with stronger task dependence will suffer from volatile issues where the
core regression task might have a larger influence on other tasks. In this
paper, we study the MTL problem with hybrid targets for the first time and
propose the model named Hybrid Targets Learning Network (HTLNet) to explore
task dependence and enhance optimization. Specifically, we introduce label
embedding for each task to explicitly transfer the label information among
these tasks, which can effectively explore logical task dependence. We also
further design the gradient adjustment regime between the final regression task
and other classification tasks to enhance the optimization. Extensive
experiments on two offline public datasets and one real-world industrial
dataset are conducted to validate the effectiveness of HTLNet. Moreover, online
A/B tests on the financial recommender system also show our model has superior
improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Masked Multi-Domain Network: Multi-Type and Multi-Scenario Conversion
  Rate <span class="highlight-title">Predict</span>ion with a Single Model <span class="chip">CIKM 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Ouyang, Xiuwu Zhang, Chaofeng Guo, Shukui Ren, Yupei Sui, Kun Zhang, Jinmei Luo, Yunfeng Chen, Dongbo Xu, Xiangzheng Liu, Yanlong Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world advertising systems, conversions have different types in nature
and ads can be shown in different display scenarios, both of which highly
impact the actual conversion rate (CVR). This results in the multi-type and
multi-scenario CVR prediction problem. A desired model for this problem should
satisfy the following requirements: 1) Accuracy: the model should achieve
fine-grained accuracy with respect to any conversion type in any display
scenario. 2) Scalability: the model parameter size should be affordable. 3)
Convenience: the model should not require a large amount of effort in data
partitioning, subset processing and separate storage. Existing approaches
cannot simultaneously satisfy these requirements. For example, building a
separate model for each (conversion type, display scenario) pair is neither
scalable nor convenient. Building a unified model trained on all the data with
conversion type and display scenario included as two features is not accurate
enough. In this paper, we propose the Masked Multi-domain Network (MMN) to
solve this problem. To achieve the accuracy requirement, we model
domain-specific parameters and propose a dynamically weighted loss to account
for the loss scale imbalance issue within each mini-batch. To achieve the
scalability requirement, we propose a parameter sharing and composition
strategy to reduce model parameters from a product space to a sum space. To
achieve the convenience requirement, we propose an auto-masking strategy which
can take mixed data from all the domains as input. It avoids the overhead
caused by data partitioning, individual processing and separate storage. Both
offline and online experimental results validate the superiority of MMN for
multi-type and multi-scenario CVR prediction. MMN is now the serving model for
real-time CVR prediction in UC Toutiao.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CIKM 2023 (larger figures)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> AFDGCF: Adaptive Feature De-correlation Graph Collaborative Filtering
  for <span class="highlight-title">Recommendation</span>s <span class="chip">SIGIR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17416v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17416v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Wu, Chao Wang, Dazhong Shen, Chuan Qin, Liyi Chen, <span class="highlight-author">Hui Xiong</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative filtering methods based on graph neural networks (GNNs) have
witnessed significant success in recommender systems (RS), capitalizing on
their ability to capture collaborative signals within intricate user-item
relationships via message-passing mechanisms. However, these GNN-based RS
inadvertently introduce excess linear correlation between user and item
embeddings, contradicting the goal of providing personalized recommendations.
While existing research predominantly ascribes this flaw to the over-smoothing
problem, this paper underscores the critical, often overlooked role of the
over-correlation issue in diminishing the effectiveness of GNN representations
and subsequent recommendation performance. Up to now, the over-correlation
issue remains unexplored in RS. Meanwhile, how to mitigate the impact of
over-correlation while preserving collaborative filtering signals is a
significant challenge. To this end, this paper aims to address the
aforementioned gap by undertaking a comprehensive study of the over-correlation
issue in graph collaborative filtering models. Firstly, we present empirical
evidence to demonstrate the widespread prevalence of over-correlation in these
models. Subsequently, we dive into a theoretical analysis which establishes a
pivotal connection between the over-correlation and over-smoothing issues.
Leveraging these insights, we introduce the Adaptive Feature De-correlation
Graph Collaborative Filtering (AFDGCF) framework, which dynamically applies
correlation penalties to the feature dimensions of the representation matrix,
effectively alleviating both over-correlation and over-smoothing issues. The
efficacy of the proposed framework is corroborated through extensive
experiments conducted with four representative graph collaborative filtering
models across four publicly available datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Domain <span class="highlight-title">Recommendation</span> to Attract Users via Domain Preference
  Modeling <span class="chip">AAAI'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyuunjun Ju, SeongKu Kang, Dongha Lee, Junyoung Hwang, Sanghwan Jang, Hwanjo Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, web platforms have been operating various service domains
simultaneously. Targeting a platform that operates multiple service domains, we
introduce a new task, Multi-Domain Recommendation to Attract Users (MDRAU),
which recommends items from multiple ``unseen'' domains with which each user
has not interacted yet, by using knowledge from the user's ``seen'' domains. In
this paper, we point out two challenges of MDRAU task. First, there are
numerous possible combinations of mappings from seen to unseen domains because
users have usually interacted with a different subset of service domains.
Second, a user might have different preferences for each of the target unseen
domains, which requires that recommendations reflect the user's preferences on
domains as well as items. To tackle these challenges, we propose DRIP framework
that models users' preferences at two levels (i.e., domain and item) and learns
various seen-unseen domain mappings in a unified way with masked domain
modeling. Our extensive experiments demonstrate the effectiveness of DRIP in
MDRAU task and its ability to capture users' domain-level preferences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Empirical Study of Training ID-Agnostic Multi-modal Sequential
  Recommenders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17372v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17372v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youhua Li, Hanwen Du, Yongxin Ni, Yuanqi He, Junchen Fu, Xiangyan Liu, Qi Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential Recommendation (SR) aims to predict future user-item interactions
based on historical interactions. While many SR approaches concentrate on user
IDs and item IDs, the human perception of the world through multi-modal
signals, like text and images, has inspired researchers to delve into
constructing SR from multi-modal information without using IDs. However, the
complexity of multi-modal learning manifests in diverse feature extractors,
fusion methods, and pre-trained models. Consequently, designing a simple and
universal \textbf{M}ulti-\textbf{M}odal \textbf{S}equential
\textbf{R}ecommendation (\textbf{MMSR}) framework remains a formidable
challenge. We systematically summarize the existing multi-modal related SR
methods and distill the essence into four core components: visual encoder, text
encoder, multimodal fusion module, and sequential architecture. Along these
dimensions, we dissect the model designs, and answer the following
sub-questions: First, we explore how to construct MMSR from scratch, ensuring
its performance either on par with or exceeds existing SR methods without
complex techniques. Second, we examine if MMSR can benefit from existing
multi-modal pre-training paradigms. Third, we assess MMSR's capability in
tackling common challenges like cold start and domain transferring. Our
experiment results across four real-world recommendation scenarios demonstrate
the great potential ID-agnostic multi-modal sequential recommendation. Our
framework can be found at: https://github.com/MMSR23/MMSR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>An Empirical Study of Training ID-Agnostic Multi-modal Sequential
  Recommenders</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cognitively Biased Users Interacting with Algorithmically Biased Results
  in Whole-Session Search on Controversial Topics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ben Wang, Jiqun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When interacting with information retrieval (IR) systems, users, affected by
confirmation biases, tend to select search results that confirm their existing
beliefs on socially significant contentious issues. To understand the judgments
and attitude changes of users searching online, our study examined how
cognitively biased users interact with algorithmically biased search engine
result pages (SERPs). We designed three-query search sessions on debated topics
under various bias conditions. We recruited 1,321 crowdsourcing participants
and explored their attitude changes, search interactions, and the effects of
confirmation bias. Three key findings emerged: 1) most attitude changes occur
in the initial query of a search session; 2) confirmation bias and result
presentation on SERPs affect search behaviors in the current query and
perceived familiarity with clicked results in subsequent queries. The bias
position also affect attitude changes of users with lower perceived openness to
conflicting opinions; 3) Interactions in the first query and and dwell time
throughout the session are associated with users' attitude changes in different
forms. Our study goes beyond traditional simulation-based evaluation settings
and simulated rational users, sheds light on the mixed effects of human biases
and algorithmic biases in controversial information retrieval tasks, and can
inform the design of bias-aware user models, human-centered bias mitigation
techniques, and socially responsible intelligent IR systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Decade of Scholarly Research on Open Knowledge Graphs <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.13186v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.13186v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Houcemeddine Turki, Abraham Toluwase Owodunni, Mohamed Ali Hadj Taieb, René Fabrice Bile, Mohamed Ben Aouicha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of open knowledge graphs has led to a surge in scholarly
research on the topic over the past decade. This paper presents a bibliometric
analysis of the scholarly literature on open knowledge graphs published between
2013 and 2023. The study aims to identify the trends, patterns, and impact of
research in this field, as well as the key topics and research questions that
have emerged. The work uses bibliometric techniques to analyze a sample of 4445
scholarly articles retrieved from Scopus. The findings reveal an
ever-increasing number of publications on open knowledge graphs published every
year, particularly in developed countries (+50 per year). These outputs are
published in highly-referred scholarly journals and conferences. The study
identifies three main research themes: (1) knowledge graph construction and
enrichment, (2) evaluation and reuse, and (3) fusion of knowledge graphs into
NLP systems. Within these themes, the study identifies specific tasks that have
received considerable attention, including entity linking, knowledge graph
embedding, and graph neural networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready edition for LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Large Language Model Alignment for Information Retrieval
  via Contrastive Feedback <span class="chip">SIGIR24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.17078v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.17078v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Dong, Yiding Liu, Qingyao Ai, Zhijing Wu, Haitao Li, Yiqun Liu, Shuaiqiang Wang, Dawei Yin, Shaoping Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable capabilities across
various research domains, including the field of Information Retrieval (IR).
However, the responses generated by off-the-shelf LLMs tend to be generic,
i.e., cannot capture the distinctiveness of each document with similar content.
This limits the performance of LLMs in IR because finding and distinguishing
relevant documents from substantial similar documents is a typical problem in
many IR tasks. To address this issue, we propose an unsupervised alignment
method, namely Reinforcement Learning from Contrastive Feedback (RLCF),
empowering LLMs to generate both high-quality and context-specific responses.
Our approach constructs unsupervised contrastive feedback signals based on
similar document groups, and adopts a reward function, named group-wise
reciprocal rank, to optimize LLMs within a standard Proximal Policy
Optimization. We conduct extensive experiments to evaluate the effectiveness of
RLCF on LLMs built with different languages and parameter sizes on multiple
downstream IR applications. RLCF significantly outperforms existing alignment
methods, and RLCF-optimized LLMs demonstrate considerable improvement in
generating responses with distinctiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing Feature Set for Click-Through Rate <span class="highlight-title">Predict</span>ion <span class="chip">WWW 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10909v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10909v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuyuan Lyu, Xing Tang, Dugang Liu, Liang Chen, Xiuqiang He, Xue Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Click-through prediction (CTR) models transform features into latent vectors
and enumerate possible feature interactions to improve performance based on the
input feature set. Therefore, when selecting an optimal feature set, we should
consider the influence of both feature and its interaction. However, most
previous works focus on either feature field selection or only select feature
interaction based on the fixed feature set to produce the feature set. The
former restricts search space to the feature field, which is too coarse to
determine subtle features. They also do not filter useless feature
interactions, leading to higher computation costs and degraded model
performance. The latter identifies useful feature interaction from all
available features, resulting in many redundant features in the feature set. In
this paper, we propose a novel method named OptFS to address these problems. To
unify the selection of feature and its interaction, we decompose the selection
of each feature interaction into the selection of two correlated features. Such
a decomposition makes the model end-to-end trainable given various feature
interaction operations. By adopting feature-level search space, we set a
learnable gate to determine whether each feature should be within the feature
set. Because of the large-scale search space, we develop a
learning-by-continuation training scheme to learn such gates. Hence, OptFS
generates the feature set only containing features which improve the final
prediction results. Experimentally, we evaluate OptFS on three public datasets,
demonstrating OptFS can optimize feature sets which enhance the model
performance and further reduce both the storage and computational cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW 2023 Research Tracks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Graph Signal Diffusion Model for Collaborative Filtering <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08744v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08744v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunqin Zhu, Chao Wang, Qi Zhang, <span class="highlight-author">Hui Xiong</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative filtering is a critical technique in recommender systems. Among
various methods, an increasingly popular paradigm is to reconstruct user-item
interactions based on the historical observations. This can be viewed as a
conditional generative task, where recently developed diffusion model
demonstrates great potential. However, existing studies on diffusion models
lack effective solutions for modeling implicit feedback data. Particularly, the
isotropic nature of the standard diffusion process fails to account for the
heterogeneous dependencies among items, leading to a misalignment with the
graphical structure of the interaction space. Meanwhile, random noise
destroying personalized information in interaction vectors, causing difficulty
in reverse reconstruction. In this paper, we make novel adaptions of diffusion
model and propose Graph Signal Diffusion Model for Collaborative Filtering
(named GiffCF). To better represent the high-dimensional and sparse
distribution of implicit feedback, we define a generalized form of denoising
diffusion using heat equation on the item-item similarity graph. Our forward
process smooths interaction signals with an advanced family of graph filters.
Hence, instead of losing information, it involves item-item similarities as
beneficial prior knowledge for recommendation. To reconstruct high-quality
interactions, our reverse process iteratively refines and sharpens preference
signals in a deterministic manner, where the update direction is conditioned on
the user history and computed from a carefully designed two-stage denoiser.
Finally, through extensive experiments, we show that GiffCF effectively
leverages the advantages of both diffusion model and graph signal processing,
and achieves state-of-the-art performance on three benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures, Accepted by SIGIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding Performance of Long-Document Ranking Models through
  Comprehensive Evaluation and Leaderboarding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.01262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.01262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonid Boytsov, David Akinpelu, Tianyi Lin, Fangwei Gao, Yutian Zhao, Jeffrey Huang, Eric Nyberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We evaluated 20+ Transformer models for ranking of long documents (including
recent LongP models trained with FlashAttention) and compared them with simple
FirstP baselines (applying the same model to input truncated to the first 512
tokens). We used MS MARCO Documents v1 as a primary training set and evaluated
models in the zero-shot scenario as well as after fine-tuning on other
collections.
  In our initial experiments with standard collections we found that
long-document models underperformed FirstP or outperformed it by at most 5% on
average in terms of MRR or NDCG. We then conjectured that this was not due to
models inability to process long context but rather due to a positional bias of
relevant passages, which tended to be among the first 512 document tokens. We
found evidence that this bias was, indeed, present in at least two test sets,
which motivated us to create a new collection MS MARCO FarRelevant where the
relevant passages were not present among the first 512 tokens.
  Unlike standard collections where we observed both little benefit from
incorporating longer contexts and limited variability in model performance
(within a few %), experiments on MS MARCO FarRelevant uncovered dramatic
differences among models. FirstP models performed roughly at the
random-baseline level in both zero-shot and fine-tuning scenarios. Simple
aggregation models (e.g., MaxP) had good zero-shot accuracy but benefited
little from fine-tuning. Most other models had poor zero-shot performance
(sometimes at a random baseline level) but outstripped MaxP by as much 13-28\%
after finetuning. Thus, positional bias not only diminishes benefits of
processing longer document contexts but also leads to model overfitting to this
bias and performing poorly in a zero-shot setting when a distribution of
relevant passages changes substantially.
  We make our software and MS MARCO FarRelevant available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Take Care of Your Prompt Bias! Investigating and Mitigating Prompt Bias
  in Factual Knowledge Extraction <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09963v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09963v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Xu, Keqin Peng, Liang Ding, Dacheng Tao, Xiliang Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research shows that pre-trained language models (PLMs) suffer from
"prompt bias" in factual knowledge extraction, i.e., prompts tend to introduce
biases toward specific labels. Prompt bias presents a significant challenge in
assessing the factual knowledge within PLMs. Therefore, this paper aims to
improve the reliability of existing benchmarks by thoroughly investigating and
mitigating prompt bias. We show that: 1) all prompts in the experiments exhibit
non-negligible bias, with gradient-based prompts like AutoPrompt and OptiPrompt
displaying significantly higher levels of bias; 2) prompt bias can amplify
benchmark accuracy unreasonably by overfitting the test datasets, especially on
imbalanced datasets like LAMA. Based on these findings, we propose a
representation-based approach to mitigate the prompt bias during inference
time. Specifically, we first estimate the biased representation using
prompt-only querying, and then remove it from the model's internal
representations to generate the debiased representations, which are used to
produce the final debiased outputs. Experiments across various prompts, PLMs,
and benchmarks show that our approach can not only correct the overfitted
performance caused by prompt bias, but also significantly improve the prompt
retrieval capability (up to 10% absolute performance gain). These results
indicate that our approach effectively alleviates prompt bias in knowledge
evaluation, thereby enhancing the reliability of benchmark assessments.
Hopefully, our plug-and-play approach can be a golden standard to strengthen
PLMs toward reliable knowledge bases. Code and data are released in
https://github.com/FelliYang/PromptBias.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Sequential <span class="highlight-title">Recommendation</span>s via Bidirectional Temporal Data
  Augmentation with Pre-training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.06460v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.06460v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juyong Jiang, Peiyan Zhang, Yingtao Luo, Chaozhuo Li, Jaeboum Kim, Kai Zhang, Senzhang Wang, Sunghun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation systems are integral to discerning temporal user
preferences. Yet, the task of learning from abbreviated user interaction
sequences poses a notable challenge. Data augmentation has been identified as a
potent strategy to enhance the informational richness of these sequences.
Traditional augmentation techniques, such as item randomization, may disrupt
the inherent temporal dynamics. Although recent advancements in reverse
chronological pseudo-item generation have shown promise, they can introduce
temporal discrepancies when assessed in a natural chronological context. In
response, we introduce a sophisticated approach, Bidirectional temporal data
Augmentation with pre-training (BARec). Our approach leverages bidirectional
temporal augmentation and knowledge-enhanced fine-tuning to synthesize
authentic pseudo-prior items that \emph{retain user preferences and capture
deeper item semantic correlations}, thus boosting the model's expressive power.
Our comprehensive experimental analysis confirms the superiority of BARec
across both short and elongated sequence contexts. Moreover, theoretical
examination and visual representation of item embeddings offer further insight
into the model's logical processes and interpretability. The source code for
our study is available at
\textcolor{blue}{\href{https://github.com/juyongjiang/BARec}{https://github.com/juyongjiang/BARec}}.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Network
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extending Network Calculus To Deal With Partially Negative And
  Decreasing Service Curves 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18042v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18042v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anja Hamscher, Vlad-Cristian Constantin, Jens B. Schmitt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Network Calculus (NC) is a versatile analytical methodology to efficiently
compute performance bounds in networked systems. The arrival and service curve
abstractions allow to model diverse and heterogeneous distributed systems. The
operations to compute residual service curves and to concatenate sequences of
systems enable an efficient and accurate calculation of per-flow timing
guarantees. Yet, in some scenarios involving multiple concurrent flows at a
system, the central notion of so-called min-plus service curves is too weak to
still be able to compute a meaningful residual service curve. In these cases,
one usually resorts to so-called strict service curves that enable the
computation of per-flow bounds. However, strict service curves are restrictive:
(1) there are service elements for which only min-plus service curves can be
provided but not strict ones and (2) strict service curves generally have no
concatenation property, i.e., a sequence of two strict systems does not yield a
strict service curve. In this report, we extend NC to deal with systems only
offering aggregate min-plus service curves to multiple flows. The key to this
extension is the exploitation of minimal arrival curves, i.e., lower bounds on
the arrival process. Technically speaking, we provide basic performance bounds
(backlog and delay) for the case of negative service curves. We also discuss
their accuracy and show them to be tight. In order to illustrate their
usefulness we also present patterns of application of these new results for:
(1) heterogeneous systems involving computation and communication resources and
(2) finite buffers that are shared between multiple flows.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in part in RTAS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">survey</span> on road safety and traffic efficiency vehicular applications
  based on C-V2X technologies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17851v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17851v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ignacio Soto, Maria Calderon, Oscar Amador, Manuel Urueña
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the use of cellular network technologies to provide
communication-based applications to vehicles has received considerable
attention. 3GPP, the standardization body responsible for cellular networks
specifications, is developing technologies to meet the requirements of
vehicular communication applications, and the research community is testing and
validating the ability of those technologies to implement different
applications. This survey presents the body of work dealing with the use of
cellular technologies to implement communication-based applications for the
connected vehicle. We focus on basic and advanced road safety and traffic
efficiency applications, which are critically important for the future of
vehicular networks. We start by describing the different cellular-related
technologies that have a role to play in providing services to the connected
vehicle, propose a classification of types of communication used in vehicular
applications, and then apply this classification to organize and present recent
research work on the topic. Finally, we identify the main challenges in the use
of cellular technologies to develop applications for the connected vehicle.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerating Radio Spectrum Regulation Workflows with Large Language
  Models (LLMs) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17819v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17819v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Ghasemi, Paul Guinand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wireless spectrum regulation is a complex and demanding process due to the
rapid pace of technological progress, increasing demand for spectrum, and a
multitude of stakeholders with potentially conflicting interests, alongside
significant economic implications. To navigate this, regulators must engage
effectively with all parties, keep pace with global technology trends, conduct
technical evaluations, issue licenses in a timely manner, and comply with
various legal and policy frameworks.
  In light of these challenges, this paper demonstrates example applications of
Large Language Models (LLMs) to expedite spectrum regulatory processes. We
explore various roles that LLMs can play in this context while identifying some
of the challenges to address. The paper also offers practical case studies and
insights, with appropriate experiments, highlighting the transformative
potential of LLMs in spectrum management.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Resource Management in Joint <span class="highlight-title">Communication</span> and
  Computing-Embedded SAGIN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17400v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17400v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Chen, Zheng Guo, Weixiao Meng, Shuai Han, Cheng Li, Tony Q. S. Quek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of the 6G era aims for ubiquitous connectivity, with the
integration of non-terrestrial networks (NTN) offering extensive coverage and
enhanced capacity. As manufacturing advances and user demands evolve,
space-air-ground integrated networks (SAGIN) with computational capabilities
emerge as a viable solution for services requiring low latency and high
computational power. Resource management within joint communication and
computing-embedded SAGIN (JCC-SAGIN) presents greater complexity than
traditional terrestrial networks. This complexity arises from the
spatiotemporal dynamics of network topology and service demand, the
interdependency of large-scale resource variables, and intricate tradeoffs
among various performance metrics. Thus, a thorough examination of resource
management strategies in JCC-SAGIN is crucial, emphasizing the role of
non-terrestrial platforms with processing capabilities in 6G. This paper begins
by reviewing the architecture, enabling technologies, and applications in
JCC-SAGIN. Then, we offer a detailed overview of resource management modeling
and optimization methods, encompassing both traditional optimization approaches
and learning-based intelligent decision-making frameworks. Finally, we outline
the prospective research directions in JCC-SAGIN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stability of Sequential Lateration and of Stress Minimization in the
  Presence of Noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10900v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10900v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ery Arias-Castro, Siddharth Vishwanath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential lateration is a class of methods for multidimensional scaling
where a suitable subset of nodes is first embedded by some method, e.g., a
clique embedded by classical scaling, and then the remaining nodes are
recursively embedded by lateration. A graph is a lateration graph when it can
be embedded by such a procedure. We provide a stability result for a particular
variant of sequential lateration. We do so in a setting where the
dissimilarities represent noisy Euclidean distances between nodes in a
geometric lateration graph. We then deduce, as a corollary, a perturbation
bound for stress minimization. To argue that our setting applies broadly, we
show that a (large) random geometric graph is a lateration graph with high
probability under mild conditions, extending a previous result of Aspnes et al
(2006).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2207.07218</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Redundancy Transmission in UAV-Aided LoRa Networks Featuring Wake-Up
  Radios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09782v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09782v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddhartha S. Borkotoky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a LoRa sensor network featuring a UAV-mounted gateway for
collecting sensor data (messages). Wake-up radios (WuR) are employed to inform
the sensors of the UAV's arrival. Building on an existing random access scheme
for such setups, we propose and evaluate two redundancy transmission protocols
for enhancing the reliability of the data transfer. One protocol employs
fountain-coded transmissions, whereas the other performs message replication.
Our results illustrate how redundancy transmission can be beneficial under the
time constraints imposed by the UAV's limited hovering duration, and how node
density, hovering time, and sensor energy budget impact the performance of the
schemes.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-25T00:00:00Z">2024-03-25</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EXPLORA: A teacher-apprentice methodology for eliciting natural
  child-computer interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17264v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17264v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vanessa Figueiredo, Catherine Ann Cameron
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Investigating child-computer interactions within their contexts is vital for
designing technology that caters to children's needs. However, determining what
aspects of context are relevant for designing child-centric technology remains
a challenge. We introduce EXPLORA, a multimodal, multistage online methodology
comprising three pivotal stages: (1) building a teacher-apprentice
relationship,(2) learning from child-teachers, and (3) assessing and
reinforcing researcher-apprentice learning. Central to EXPLORA is the
collection of attitudinal data through pre-observation interviews, offering
researchers a deeper understanding of children's characteristics and contexts.
This informs subsequent online observations, allowing researchers to focus on
frequent interactions. Furthermore, researchers can validate preliminary
assumptions with children. A means-ends analysis framework aids in the
systematic analysis of data, shedding light on context, agency and
homework-information searching processes children employ in their activities.
To illustrate EXPLORA's capabilities, we present nine single case studies
investigating Brazilian child-caregiver dyads' (children ages 9-11) use of
technology in homework information-searching.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CADGL: Context-Aware Deep Graph Learning for <span class="highlight-title">Predict</span>ing Drug-Drug
  Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Azmine Toushik Wasi, Taki Hasan Rafi, Raima Islam, Serbetar Karlo, Dong-Kyu Chae
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Examining Drug-Drug Interactions (DDIs) is a pivotal element in the process
of drug development. DDIs occur when one drug's properties are affected by the
inclusion of other drugs. Detecting favorable DDIs has the potential to pave
the way for creating and advancing innovative medications applicable in
practical settings. However, existing DDI prediction models continue to face
challenges related to generalization in extreme cases, robust feature
extraction, and real-life application possibilities. We aim to address these
challenges by leveraging the effectiveness of context-aware deep graph learning
by introducing a novel framework named CADGL. Based on a customized variational
graph autoencoder (VGAE), we capture critical structural and physio-chemical
information using two context preprocessors for feature extraction from two
different perspectives: local neighborhood and molecular context, in a
heterogeneous graphical structure. Our customized VGAE consists of a graph
encoder, a latent information encoder, and an MLP decoder. CADGL surpasses
other state-of-the-art DDI prediction models, excelling in predicting
clinically valuable novel DDIs, supported by rigorous case studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 Pages, 4 Figures; In review in IEEE/ACM Transactions on
  Computational Biology and Bioinformatics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generation of Asset Administration Shell with Large Language Model
  Agents: Interoperability in Digital Twins with Semantic Node 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Xia, Zhewen Xiao, Nasser Jazdi, Michael Weyrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research introduces a novel approach for assisting the creation of Asset
Administration Shell (AAS) instances for digital twin modeling within the
context of Industry 4.0, aiming to enhance interoperability in smart
manufacturing and reduce manual effort. We construct a "semantic node" data
structure to capture the semantic essence of textual data. Then, a system
powered by large language models is designed and implemented to process
"semantic node" and generate AAS instance models from textual technical data.
Our evaluation demonstrates a 62-79% effective generation rate, indicating a
substantial proportion of manual creation effort can be converted into easier
validation effort, thereby reducing the time and cost in creating AAS instance
models. In our evaluation, a comparative analysis of different LLMs and an
in-depth ablation study of Retrieval-Augmented Generation (RAG) mechanisms
provide insights into the effectiveness of LLM systems for interpreting
technical concepts. Our findings emphasize LLMs' capability in automating AAS
instance creation, enhancing semantic interoperability, and contributing to the
broader field of semantic interoperability for digital twins in industrial
applications. The prototype implementation and evaluation results are released
on our GitHub Repository with the link: https://github.com/YuchenXia/AASbyLLM
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print, submitted to IEEE ACCESS, under peer-review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GOLF: Goal-Oriented Long-term liFe tasks supported by human-AI
  collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17089v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17089v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ben Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of ChatGPT and similar large language models (LLMs) has
revolutionized the human-AI interaction and information-seeking process.
Leveraging LLMs as an alternative to search engines, users can now access
summarized information tailored to their queries, significantly reducing the
cognitive load associated with navigating vast information resources. This
shift underscores the potential of LLMs in redefining information access
paradigms. Drawing on the foundation of task-focused information retrieval and
LLMs' task planning ability, this research extends the scope of LLM
capabilities beyond routine task automation to support users in navigating
long-term and significant life tasks. It introduces the GOLF framework
(Goal-Oriented Long-term liFe tasks), which focuses on enhancing LLMs' ability
to assist in significant life decisions through goal orientation and long-term
planning. The methodology encompasses a comprehensive simulation study to test
the framework's efficacy, followed by model and human evaluations to develop a
dataset benchmark for long-term life tasks, and experiments across different
models and settings. By shifting the focus from short-term tasks to the broader
spectrum of long-term life goals, this research underscores the transformative
potential of LLMs in enhancing human decision-making processes and task
management, marking a significant step forward in the evolution of human-AI
collaboration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Learning-based Recommender Systems with Large Language
  Models for State Reward and Action Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16948v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16948v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Wang, Alexandros Karatzoglou, Ioannis Arapakis, Joemon M. Jose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL)-based recommender systems have demonstrated
promising performance in meeting user expectations by learning to make accurate
next-item recommendations from historical user-item interactions. However,
existing offline RL-based sequential recommendation methods face the challenge
of obtaining effective user feedback from the environment. Effectively modeling
the user state and shaping an appropriate reward for recommendation remains a
challenge. In this paper, we leverage language understanding capabilities and
adapt large language models (LLMs) as an environment (LE) to enhance RL-based
recommenders. The LE is learned from a subset of user-item interaction data,
thus reducing the need for large training data, and can synthesise user
feedback for offline data by: (i) acting as a state model that produces high
quality states that enrich the user representation, and (ii) functioning as a
reward model to accurately capture nuanced user preferences on actions.
Moreover, the LE allows to generate positive actions that augment the limited
offline training data. We propose a LE Augmentation (LEA) method to further
improve recommendation performance by optimising jointly the supervised
component and the RL policy, using the augmented actions and historical user
signals. We use LEA, the state and reward models in conjunction with
state-of-the-art RL recommenders and report experimental results on two
publicly available datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GloSIS: The Global Soil Information System Web Ontology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raul Palma, Bogusz Janiak, Luís Moreira de Sousa, Kathi Schleidt, Tomáš Řezník, Fenny van Egmond, Johan Leenaars, Dimitrios Moshou, Abdul Mouazen, Peter Wilson, David Medyckyj-Scott, Alistair Ritchie, Yusuf Yigini, Ronald Vargas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Established in 2012 by members of the Food and Agriculture Organisation
(FAO), the Global Soil Partnership (GSP) is a global network of stakeholders
promoting sound land and soil management practices towards a sustainable world
food system. However, soil survey largely remains a local or regional activity,
bound to heterogeneous methods and conventions. Recognising the relevance of
global and trans-national policies towards sustainable land management
practices, the GSP elected data harmonisation and exchange as one of its key
lines of action. Building upon international standards and previous work
towards a global soil data ontology, an improved domain model was eventually
developed within the GSP [54], the basis for a Global Soil Information System
(GloSIS). This work also identified the Semantic Web as a possible avenue to
operationalise the domain model. This article presents the GloSIS web ontology,
an implementation of the GloSIS domain model with the Web Ontology Language
(OWL). Thoroughly employing a host of Semantic Web standards (SOSA, SKOS,
GeoSPARQL, QUDT), GloSIS lays out not only a soil data ontology but also an
extensive set of ready-to-use code-lists for soil description and
physio-chemical analysis. Various examples are provided on the provision and
use of GloSIS-compliant linked data, showcasing the contribution of this
ontology to the discovery, exploration, integration and access of soil data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProCQA: A Large-scale Community-based Programming Question Answering
  <span class="highlight-title">Dataset</span> for Code Search <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehan Li, Jianfei Zhang, Chuantao Yin, Yuanxin Ouyang, Wenge Rong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-based code question answering seeks to match user queries in
natural language to relevant code snippets. Previous approaches typically rely
on pretraining models using crafted bi-modal and uni-modal datasets to align
text and code representations. In this paper, we introduce ProCQA, a
large-scale programming question answering dataset extracted from the
StackOverflow community, offering naturally structured mixed-modal QA pairs. To
validate its effectiveness, we propose a modality-agnostic contrastive
pre-training approach to improve the alignment of text and code representations
of current code language models. Compared to previous models that primarily
employ bimodal and unimodal pairs extracted from CodeSearchNet for
pre-training, our model exhibits significant performance improvements across a
wide range of code retrieval benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Augmentation for <span class="highlight-title">Recommendation</span> <span class="chip">ICDE 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianru Zhang, Lianghao Xia, Xuheng Cai, Siuming Yiu, Chao Huang, Christian S. Jensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph augmentation with contrastive learning has gained significant attention
in the field of recommendation systems due to its ability to learn expressive
user representations, even when labeled data is limited. However, directly
applying existing GCL models to real-world recommendation environments poses
challenges. There are two primary issues to address. Firstly, the lack of
consideration for data noise in contrastive learning can result in noisy
self-supervised signals, leading to degraded performance. Secondly, many
existing GCL approaches rely on graph neural network (GNN) architectures, which
can suffer from over-smoothing problems due to non-adaptive message passing. To
address these challenges, we propose a principled framework called GraphAug.
This framework introduces a robust data augmentor that generates denoised
self-supervised signals, enhancing recommender systems. The GraphAug framework
incorporates a graph information bottleneck (GIB)-regularized augmentation
paradigm, which automatically distills informative self-supervision information
and adaptively adjusts contrastive view generation. Through rigorous
experimentation on real-world datasets, we thoroughly assessed the performance
of our novel GraphAug model. The outcomes consistently unveil its superiority
over existing baseline methods. The source code for our model is publicly
available at: https://github.com/HKUDS/GraphAug.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages and accepted by ICDE 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A comparative analysis of <span class="highlight-title">embedding</span> models for patent similarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Grazia Sveva Ascione, Valerio Sterzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper makes two contributions to the field of text-based patent
similarity. First, it compares the performance of different kinds of
patent-specific pretrained embedding models, namely static word embeddings
(such as word2vec and doc2vec models) and contextual word embeddings (such as
transformers based models), on the task of patent similarity calculation.
Second, it compares specifically the performance of Sentence Transformers
(SBERT) architectures with different training phases on the patent similarity
task. To assess the models' performance, we use information about patent
interferences, a phenomenon in which two or more patent claims belonging to
different patent applications are proven to be overlapping by patent examiners.
Therefore, we use these interferences cases as a proxy for maximum similarity
between two patents, treating them as ground-truth to evaluate the performance
of the different embedding models. Our results point out that, first, Patent
SBERT-adapt-ub, the domain adaptation of the pretrained Sentence Transformer
architecture proposed in this research, outperforms the current
state-of-the-art in patent similarity. Second, they show that, in some cases,
large static models performances are still comparable to contextual ones when
trained on extensive data; thus, we believe that the superiority in the
performance of contextual embeddings may not be related to the actual
architecture but rather to the way the training phase is performed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liu Junhua, Tan Yong Keat, Fu Bin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Following the significant achievements of large language models (LLMs),
researchers have employed in-context learning for text classification tasks.
However, these studies focused on monolingual, single-turn classification
tasks. In this paper, we introduce LARA (Linguistic-Adaptive
Retrieval-Augmented Language Models), designed to enhance accuracy in
multi-turn classification tasks across six languages, accommodating numerous
intents in chatbot interactions. Multi-turn intent classification is notably
challenging due to the complexity and evolving nature of conversational
contexts. LARA tackles these issues by combining a fine-tuned smaller model
with a retrieval-augmented mechanism, integrated within the architecture of
LLMs. This integration allows LARA to dynamically utilize past dialogues and
relevant intents, thereby improving the understanding of the context.
Furthermore, our adaptive retrieval techniques bolster the cross-lingual
capabilities of LLMs without extensive retraining and fine-tune. Comprehensive
experiments demonstrate that LARA achieves state-of-the-art performance on
multi-turn intent classification tasks, enhancing the average accuracy by 3.67%
compared to existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InstUPR : Instruction-based Unsupervised Passage Reranking with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao-Wei Huang, Yun-Nung Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces InstUPR, an unsupervised passage reranking method based
on large language models (LLMs). Different from existing approaches that rely
on extensive training with query-document pairs or retrieval-specific
instructions, our method leverages the instruction-following capabilities of
instruction-tuned LLMs for passage reranking without any additional
fine-tuning. To achieve this, we introduce a soft score aggregation technique
and employ pairwise reranking for unsupervised passage reranking. Experiments
on the BEIR benchmark demonstrate that InstUPR outperforms unsupervised
baselines as well as an instruction-tuned reranker, highlighting its
effectiveness and superiority. Source code to reproduce all experiments is
open-sourced at https://github.com/MiuLab/InstUPR
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. This manuscript was originally written and submitted in
  June 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Experiment with the Use of ChatGPT for LCSH Subject Assignment on
  Electronic Theses and Dissertations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16424v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16424v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric H. C. Chow, TJ Kao, Xiaoli Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study delves into the potential use of Large Language Models (LLMs) for
generating Library of Congress Subject Headings (LCSH). The authors employed
ChatGPT to generate subject headings for electronic theses and dissertations
(ETDs) based on their titles and summaries. The results revealed that although
some generated subject headings were valid, there were issues regarding
specificity and exhaustiveness. The study showcases that LLMs can serve as a
strategic response to the backlog of items awaiting cataloging in academic
libraries, while also offering a cost-effective approach for promptly
generating LCSH. Nonetheless, human catalogers remain essential for verifying
and enhancing the validity, exhaustiveness, and specificity of LCSH generated
by LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Play to Your Strengths: Collaborative Intelligence of Conventional
  Recommender Models and Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunjia Xi, Weiwen Liu, Jianghao Lin, Chuhan Wu, Bo Chen, Ruiming Tang, Weinan Zhang, Yong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of large language models (LLMs) has opened new opportunities in
Recommender Systems (RSs) by enhancing user behavior modeling and content
understanding. However, current approaches that integrate LLMs into RSs solely
utilize either LLM or conventional recommender model (CRM) to generate final
recommendations, without considering which data segments LLM or CRM excel in.
To fill in this gap, we conduct experiments on MovieLens-1M and Amazon-Books
datasets, and compare the performance of a representative CRM (DCNv2) and an
LLM (LLaMA2-7B) on various groups of data samples. Our findings reveal that
LLMs excel in data segments where CRMs exhibit lower confidence and precision,
while samples where CRM excels are relatively challenging for LLM, requiring
substantial training data and a long training time for comparable performance.
This suggests potential synergies in the combination between LLM and CRM.
Motivated by these insights, we propose Collaborative Recommendation with
conventional Recommender and Large Language Model (dubbed \textit{CoReLLa}). In
this framework, we first jointly train LLM and CRM and address the issue of
decision boundary shifts through alignment loss. Then, the resource-efficient
CRM, with a shorter inference time, handles simple and moderate samples, while
LLM processes the small subset of challenging samples for CRM. Our experimental
results demonstrate that CoReLLa outperforms state-of-the-art CRM and LLM
methods significantly, underscoring its effectiveness in recommendation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncovering Selective State Space Model's Capabilities in Lifelong
  Sequential <span class="highlight-title">Recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16371v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16371v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiyuan Yang, Yuanzi Li, Jingyu Zhao, Hanbing Wang, Muyang Ma, Jun Ma, Zhaochun Ren, Mengqi Zhang, Xin Xin, Zhumin Chen, Pengjie Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential Recommenders have been widely applied in various online services,
aiming to model users' dynamic interests from their sequential interactions.
With users increasingly engaging with online platforms, vast amounts of
lifelong user behavioral sequences have been generated. However, existing
sequential recommender models often struggle to handle such lifelong sequences.
The primary challenges stem from computational complexity and the ability to
capture long-range dependencies within the sequence. Recently, a state space
model featuring a selective mechanism (i.e., Mamba) has emerged. In this work,
we investigate the performance of Mamba for lifelong sequential recommendation
(i.e., length>=2k). More specifically, we leverage the Mamba block to model
lifelong user sequences selectively. We conduct extensive experiments to
evaluate the performance of representative sequential recommendation models in
the setting of lifelong sequences. Experiments on two real-world datasets
demonstrate the superiority of Mamba. We found that RecMamba achieves
performance comparable to the representative model while significantly reducing
training duration by approximately 70% and memory costs by 80%. Codes and data
are available at \url{https://github.com/nancheng58/RecMamba}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced Facet Generation with LLM Editing <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16345v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16345v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joosung Lee, Jinhong Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In information retrieval, facet identification of a user query is an
important task. If a search service can recognize the facets of a user's query,
it has the potential to offer users a much broader range of search results.
Previous studies can enhance facet prediction by leveraging retrieved documents
and related queries obtained through a search engine. However, there are
challenges in extending it to other applications when a search engine operates
as part of the model. First, search engines are constantly updated. Therefore,
additional information may change during training and test, which may reduce
performance. The second challenge is that public search engines cannot search
for internal documents. Therefore, a separate search system needs to be built
to incorporate documents from private domains within the company. We propose
two strategies that focus on a framework that can predict facets by taking only
queries as input without a search engine. The first strategy is multi-task
learning to predict SERP. By leveraging SERP as a target instead of a source,
the proposed model deeply understands queries without relying on external
modules. The second strategy is to enhance the facets by combining Large
Language Model (LLM) and the small model. Overall performance improves when
small model and LLM are combined rather than facet generation individually.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Large Language Models as Generative User Simulators for
  Conversational <span class="highlight-title">Recommendation</span> <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09738v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09738v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Se-eun Yoon, Zhankui He, Jessica Maria Echterhoff, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic users are cost-effective proxies for real users in the evaluation
of conversational recommender systems. Large language models show promise in
simulating human-like behavior, raising the question of their ability to
represent a diverse population of users. We introduce a new protocol to measure
the degree to which language models can accurately emulate human behavior in
conversational recommendation. This protocol is comprised of five tasks, each
designed to evaluate a key property that a synthetic user should exhibit:
choosing which items to talk about, expressing binary preferences, expressing
open-ended preferences, requesting recommendations, and giving feedback.
Through evaluation of baseline simulators, we demonstrate these tasks
effectively reveal deviations of language models from human behavior, and offer
insights on how to reduce the deviations with model selection and prompting
strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NoteLLM: A Retrievable Large Language Model for Note <span class="highlight-title">Recommendation</span> <span class="chip">WWW'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01744v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01744v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Zhang, Shiwei Wu, Haoxin Zhang, Tong Xu, Yan Gao, Yao Hu, Di Wu, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  People enjoy sharing "notes" including their experiences within online
communities. Therefore, recommending notes aligned with user interests has
become a crucial task. Existing online methods only input notes into BERT-based
models to generate note embeddings for assessing similarity. However, they may
underutilize some important cues, e.g., hashtags or categories, which represent
the key concepts of notes. Indeed, learning to generate hashtags/categories can
potentially enhance note embeddings, both of which compress key note
information into limited content. Besides, Large Language Models (LLMs) have
significantly outperformed BERT in understanding natural languages. It is
promising to introduce LLMs into note recommendation. In this paper, we propose
a novel unified framework called NoteLLM, which leverages LLMs to address the
item-to-item (I2I) note recommendation. Specifically, we utilize Note
Compression Prompt to compress a note into a single special token, and further
learn the potentially related notes' embeddings via a contrastive learning
approach. Moreover, we use NoteLLM to summarize the note and generate the
hashtag/category automatically through instruction tuning. Extensive
validations on real scenarios demonstrate the effectiveness of our proposed
method compared with the online baseline and show major improvements in the
recommendation system of Xiaohongshu.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a WWW'24 full paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Word4Per: Zero-shot Composed Person Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16515v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16515v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Delong Liu, Haiwen Li, Zhicheng Zhao, Fei Su, Yuan Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Searching for specific person has great social benefits and security value,
and it often involves a combination of visual and textual information.
Conventional person retrieval methods, whether image-based or text-based,
usually fall short in effectively harnessing both types of information, leading
to the loss of accuracy. In this paper, a whole new task called Composed Person
Retrieval (CPR) is proposed to jointly utilize both image and text information
for target person retrieval. However, the supervised CPR requires very costly
manual annotation dataset, while there are currently no available resources. To
mitigate this issue, we firstly introduce the Zero-shot Composed Person
Retrieval (ZS-CPR), which leverages existing domain-related data to resolve the
CPR problem without expensive annotations. Secondly, to learn ZS-CPR model, we
propose a two-stage learning framework, Word4Per, where a lightweight Textual
Inversion Network (TINet) and a text-based person retrieval model based on
fine-tuned Contrastive Language-Image Pre-training (CLIP) network are learned
without utilizing any CPR data. Thirdly, a finely annotated Image-Text Composed
Person Retrieval (ITCPR) dataset is built as the benchmark to assess the
performance of the proposed Word4Per framework. Extensive experiments under
both Rank-1 and mAP demonstrate the effectiveness of Word4Per for the ZS-CPR
task, surpassing the comparative methods by over 10\%. The code and ITCPR
dataset will be publicly available at
https://github.com/Delong-liu-bupt/Word4Per.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GNNUERS: Fairness Explanation in GNNs for <span class="highlight-title">Recommendation</span> via
  Counterfactual Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.06182v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.06182v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giacomo Medda, Francesco Fabbri, Mirko Marras, Ludovico Boratto, Gianni Fenu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, research into personalization has been focusing on explainability
and fairness. Several approaches proposed in recent works are able to explain
individual recommendations in a post-hoc manner or by explanation paths.
However, explainability techniques applied to unfairness in recommendation have
been limited to finding user/item features mostly related to biased
recommendations. In this paper, we devised a novel algorithm that leverages
counterfactuality methods to discover user unfairness explanations in the form
of user-item interactions. In our counterfactual framework, interactions are
represented as edges in a bipartite graph, with users and items as nodes. Our
bipartite graph explainer perturbs the topological structure to find an altered
version that minimizes the disparity in utility between the protected and
unprotected demographic groups. Experiments on four real-world graphs coming
from various domains showed that our method can systematically explain user
unfairness on three state-of-the-art GNN-based recommendation models. Moreover,
an empirical evaluation of the perturbed network uncovered relevant patterns
that justify the nature of the unfairness discovered by the generated
explanations. The source code and the preprocessed data sets are available at
https://github.com/jackmedda/RS-BGExplainer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the resilience of Collaborative Learning-based Recommender Systems
  Against Community Detection Attack 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.08929v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.08929v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yacine Belal, Sonia Ben Mokhtar, Mohamed Maouche, Anthony Simonet-Boulogne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative-learning-based recommender systems emerged following the
success of collaborative learning techniques such as Federated Learning (FL)
and Gossip Learning (GL). In these systems, users participate in the training
of a recommender system while maintaining their history of consumed items on
their devices. While these solutions seemed appealing for preserving the
privacy of the participants at first glance, recent studies have revealed that
collaborative learning can be vulnerable to various privacy attacks. In this
paper, we study the resilience of collaborative learning-based recommender
systems against a novel privacy attack called Community Detection Attack (CDA).
This attack enables an adversary to identify community members based on a
chosen set of items (eg., identifying users interested in specific
points-of-interest). Through experiments on three real recommendation datasets
using two state-of-the-art recommendation models, we evaluate the sensitivity
of an FL-based recommender system as well as two flavors of Gossip
Learning-based recommender systems to CDA. The results show that across all
models and datasets, the FL setting is more vulnerable to CDA compared to
Gossip settings. Furthermore, we assess two off-the-shelf mitigation
strategies, namely differential privacy (DP) and a \emph{Share less} policy,
which consists of sharing a subset of less sensitive model parameters. The
findings indicate a more favorable privacy-utility trade-off for the
\emph{Share less} strategy, particularly in FedRecs.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Network
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Relational Network Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17277v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17277v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xieyang Xu, Yifei Yuan, Zachary Kincaid, Arvind Krishnamurthy, Ratul Mahajan, David Walker, Ennan Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relational network verification is a new approach to validating network
changes. In contrast to traditional network verification, which analyzes
specifications for a single network snapshot, relational network verification
analyzes specifications concerning two network snapshots (e.g., pre- and
post-change snapshots) and captures their similarities and differences.
Relational change specifications are compact and precise because they specify
the flows or paths that change between snapshots and then simply mandate that
other behaviors of the network "stay the same", without enumerating them. To
achieve similar guarantees, single-snapshot specifications need to enumerate
all flow and path behaviors that are not expected to change, so we can check
that nothing has accidentally changed. Thus, precise single-snapshot
specifications are proportional to network size, which makes them impractical
to generate for many real-world networks.
  To demonstrate the value of relational reasoning, we develop a high-level
relational specification language and a tool called Rela to validate network
changes. Rela first compiles input specifications and network snapshot
representations to finite state transducers. It then checks compliance using
decision procedures for automaton equivalence. Our experiments using data on
complex changes to a global backbone (with over 10^3 routers) find that Rela
specifications need fewer than 10 terms for 93% of them and it validates 80% of
them within 20 minutes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Relative Representations for Goal-Oriented Semantic
  <span class="highlight-title">Communication</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16986v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16986v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simone Fiorellino, Claudio Battiloro, Emilio Calvanese Strinati, Paolo Di Lorenzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In future 6G wireless networks, semantic and effectiveness aspects of
communications will play a fundamental role, incorporating meaning and
relevance into transmissions. However, obstacles arise when devices employ
diverse languages, logic, or internal representations, leading to semantic
mismatches that might jeopardize understanding. In latent space communication,
this challenge manifests as misalignment within high-dimensional
representations where deep neural networks encode data. This paper presents a
novel framework for goal-oriented semantic communication, leveraging relative
representations to mitigate semantic mismatches via latent space alignment. We
propose a dynamic optimization strategy that adapts relative representations,
communication parameters, and computation resources for energy-efficient,
low-latency, goal-oriented semantic communications. Numerical results
demonstrate our methodology's effectiveness in mitigating mismatches among
devices, while optimizing energy consumption, delay, and effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Network-Assisted Delivery of Adaptive Video <span class="highlight-title">Stream</span>ing Services through
  CDN, SDN, and MEC 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16951v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16951v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reza Farahani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimedia applications, mainly video streaming services, are currently the
dominant source of network load worldwide. In recent Video-on-Demand (VoD) and
live video streaming services, traditional streaming delivery techniques have
been replaced by adaptive solutions based on the HTTP protocol. Current trends
toward high-resolution (e.g., 8K) and/or low-latency VoD and live video
streaming pose new challenges to end-to-end (E2E) bandwidth demand and have
stringent delay requirements. To do this, video providers typically rely on
Content Delivery Networks (CDNs) to ensure that they provide scalable video
streaming services. To support future streaming scenarios involving millions of
users, it is necessary to increase the CDNs' efficiency. It is widely agreed
that these requirements may be satisfied by adopting emerging networking
techniques to present Network-Assisted Video Streaming (NAVS) methods.
Motivated by this, this thesis goes one step beyond traditional pure
client-based HAS algorithms by incorporating (an) in-network component(s) with
a broader view of the network to present completely transparent NAVS solutions
for HAS clients.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PhD thesis defended in 22.08.2023
  (https://netlibrary.aau.at/obvuklhs/content/titleinfo/9173622)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic-Aware Remote Estimation of Multiple Markov Sources Under
  Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiping Luo, Nikolaos Pappas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies semantic-aware communication for remote estimation of
multiple Markov sources over a lossy and rate-constrained channel. Unlike most
existing studies that treat all source states equally, we exploit the semantics
of information and consider that the remote actuator has different tolerances
for the estimation errors of different states. We aim to find an optimal
scheduling policy that minimizes the long-term state-dependent costs of
estimation errors under a transmission frequency constraint. We theoretically
show the structure of the optimal policy by leveraging the average-cost
Constrained Markov Decision Process (CMDP) theory and the Lagrangian dynamic
programming. By exploiting the optimal structural results, we develop a novel
policy search algorithm, termed intersection search plus relative value
iteration (Insec-RVI), that can find the optimal policy using only a few
iterations. To avoid the ``curse of dimensionality'' of MDPs, we propose an
online low-complexity drift-plus-penalty (DPP) scheduling algorithm based on
the Lyapunov optimization theorem. We also design an efficient average-cost
Q-learning algorithm to estimate the optimal policy without knowing a priori
the channel and source statistics. Numerical results show that continuous
transmission is inefficient, and remarkably, our semantic-aware policies can
attain the optimum by strategically utilizing fewer transmissions by exploiting
the timing of the important information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring <span class="highlight-title">Communication</span> Technologies, Standards, and Challenges in
  Electrified Vehicle Charging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Ma, Yuan Zhou, Hanwen Zhang, Qun Wang, Haijian Sun, Hongjie Wang, Rose Qingyang Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As public awareness of environmental protection continues to grow, the trend
of integrating more electric vehicles (EVs) into the transportation sector is
rising. Unlike conventional internal combustion engine (ICE) vehicles, EVs can
minimize carbon emissions and potentially achieve autonomous driving. However,
several obstacles hinder the widespread adoption of EVs, such as their
constrained driving range and the extended time required for charging. One
alternative solution to address these challenges is implementing dynamic
wireless power transfer (DWPT), charging EVs in motion on the road. Moreover,
charging stations with static wireless power transfer (SWPT) infrastructure can
replace existing gas stations, enabling users to charge EVs in parking lots or
at home. This paper surveys the communication infrastructure for static and
dynamic wireless charging in electric vehicles. It encompasses all
communication aspects involved in the wireless charging process. The
architecture and communication requirements for static and dynamic wireless
charging are presented separately. Additionally, a comprehensive comparison of
existing communication standards is provided. The communication with the grid
is also explored in detail. The survey gives attention to security and privacy
issues arising during communications. In summary, the paper addresses the
challenges and outlines upcoming trends in communication for EV wireless
charging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IET Communication as a survey paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FOOL: Addressing the Downlink Bottleneck in Satellite Computing with
  Neural Feature Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Furutanpey, Qiyang Zhang, Philipp Raith, Tobias Pfandzelter, Shangguang Wang, Schahram Dustdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nanosatellite constellations equipped with sensors capturing large geographic
regions provide unprecedented opportunities for Earth observation. As
constellation sizes increase, network contention poses a downlink bottleneck.
Orbital Edge Computing (OEC) leverages limited onboard compute resources to
reduce transfer costs by processing the raw captures at the source. However,
current solutions have limited practicability due to reliance on crude
filtering methods or over-prioritizing particular downstream tasks.
  This work presents FOOL, an OEC-native and task-agnostic feature compression
method that preserves prediction performance. FOOL partitions high-resolution
satellite imagery to maximize throughput. Further, it embeds context and
leverages inter-tile dependencies to lower transfer costs with negligible
overhead. While FOOL is a feature compressor, it can recover images with
competitive scores on perceptual quality measures at lower bitrates. We
extensively evaluate transfer cost reduction by including the peculiarity of
intermittently available network connections in low earth orbit. Lastly, we
test the feasibility of our system for standardized nanosatellite form factors.
We demonstrate that FOOL permits downlinking over 100x the data volume without
relying on prior information on the downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, double column, 19 figures, 7 tables, Initial Submission to
  IEEE Transactions on Mobile Computing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ XAV: A High-Performance Regular Expression Matching Engine for Packet
  Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jincheng Zhong, Shuhui Chen, Chuan Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regular expression matching is the core function of various network security
applications such as network intrusion detection systems. With the network
bandwidth increases, it is a great challenge to implement regular expression
matching for line rate packet processing. To this end, a novel scheme named XAV
targeting high-performance regular expression matching is proposed in this
paper. XAV first employs anchor DFA to tackle the state explosion problem of
DFA. Then based on anchor DFA, two techniques including pre-filtering and regex
decomposition are utilized to improve the average time complexity. Through
implementing XAV with an FPGA-CPU architecture, comprehensive experiments show
that a high matching throughput of up to 75 Gbps can be achieved for the large
and complex Snort rule-set. Compared to state-of-the-art software schemes, XAV
achieves two orders of magnitude of performance improvement. While compared to
state-of-the-art FPGA-based schemes, XAV achieves more than 2.5x performance
improvement with the same hardware resource consumption.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 12 figures, work of 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accuracy-Aware Cooperative Sensing and Computing for Connected
  Autonomous Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuehan Ye, Kaige Qu, Weihua Zhuang, Xuemin Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To maintain high perception performance among connected and autonomous
vehicles (CAVs), in this paper, we propose an accuracy-aware and
resource-efficient raw-level cooperative sensing and computing scheme among
CAVs and road-side infrastructure. The scheme enables fined-grained partial raw
sensing data selection, transmission, fusion, and processing in per-object
granularity, by exploiting the parallelism among object classification subtasks
associated with each object. A supervised learning model is trained to capture
the relationship between the object classification accuracy and the data
quality of selected object sensing data, facilitating accuracy-aware sensing
data selection. We formulate an optimization problem for joint sensing data
selection, subtask placement and resource allocation among multiple object
classification subtasks, to minimize the total resource cost while satisfying
the delay and accuracy requirements. A genetic algorithm based iterative
solution is proposed for the optimization problem. Simulation results
demonstrate the accuracy awareness and resource efficiency achieved by the
proposed cooperative sensing and computing scheme, in comparison with benchmark
solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CacheGen: KV Cache Compression and <span class="highlight-title">Stream</span>ing for Fast Language Model
  Serving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07240v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07240v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhan Liu, Hanchen Li, Yihua Cheng, Siddhant Ray, Yuyang Huang, Qizheng Zhang, Kuntai Du, Jiayi Yao, Shan Lu, Ganesh Ananthanarayanan, Michael Maire, Henry Hoffmann, Ari Holtzman, Junchen Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) take on complex tasks, their inputs are
supplemented with longer contexts that incorporate domain knowledge or
user-specific information. Yet using long contexts poses a challenge for
responsive LLM systems, as nothing can be generated until the whole context is
processed by the LLM. While the context-processing delay can be reduced by
reusing the KV cache of a context across different inputs, fetching the KV
cache, which contains large tensors, over the network can cause extra network
delays.
  CacheGen is a fast context-loading module for LLM systems. First, CacheGen
uses a custom tensor encoder, which embraces KV cache's distributional
properties, to encode a KV cache into more compact bitstream representations
with negligible encoding/decoding overhead. This reduces the bandwidth demand
to fetch the KV cache. Second, to maintain low context-loading delay and high
generation quality, CacheGen adapts the streaming strategies to cope with
changes in available bandwidth. When available bandwidth drops, CacheGen may
raise the compression level for a part of the context or choose to recompute
its KV cache on the fly. We test CacheGen on four popular LLMs of various sizes
and four datasets (662 contexts in total). Compared to the recent systems that
reuse the KV cache, CacheGen reduces the KV cache size by 3.7-4.3x and the
total delay in fetching and processing contexts by 2.7-3.2x while having
negligible impact on the LLM response quality in accuracy or perplexity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Implications of Decentralization in Blockchained <span class="highlight-title">Federated</span> Learning:
  Evaluating the Impact of Model Staleness and Inconsistencies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07471v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07471v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesc Wilhelmi, Nima Afraz, Elia Guerra, Paolo Dini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Blockchain promises to enhance distributed machine learning (ML) approaches
such as federated learning (FL) by providing further decentralization,
security, immutability, and trust, which are key properties for enabling
collaborative intelligence in next-generation applications. Nonetheless, the
intrinsic decentralized operation of peer-to-peer (P2P) blockchain nodes leads
to an uncharted setting for FL, whereby the concepts of FL round and global
model become meaningless, as devices' synchronization is lost without the
figure of a central orchestrating server. In this paper, we study the practical
implications of outsourcing the orchestration of FL to a democratic setting
such as in a blockchain. In particular, we focus on the effects that model
staleness and inconsistencies, endorsed by blockchains' modus operandi, have on
the training procedure held by FL devices asynchronously. Using simulation, we
evaluate the blockchained FL operation by applying two different ML models
(ranging from low to high complexity) on the well-known MNIST and CIFAR-10
datasets, respectively, and focus on the accuracy and timeliness of the
solutions. Our results show the high impact of model inconsistencies on the
accuracy of the models (up to a ~35% decrease in prediction accuracy), which
underscores the importance of properly designing blockchain systems based on
the characteristics of the underlying FL application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Plotinus: A Satellite Internet Digital Twin System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08515v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08515v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Gao, Kun Qiu, Zhe Chen, Wenjun Zhu, Qi Zhang, Handong Luo, Quanwei Lin, Ziheng Yang, Wenhao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of an integrated space-air-ground network (SAGIN) requires
sophisticated satellite Internet emulation tools that can handle complex,
dynamic topologies and offer in-depth analysis. Existing emulation platforms
struggle with challenges like the need for detailed implementation across all
network layers, real-time response, and scalability. This paper proposes a
digital twin system based on microservices for satellite Internet emulation,
namely Plotinus, which aims to solve these problems. Plotinus features a
modular design, allowing for easy replacement of the physical layer to emulate
different aerial vehicles and analyze channel interference. It also enables
replacing path computation methods to simplify testing and deploying
algorithms. In particular, Plotinus allows for real-time emulation with live
network traffic, enhancing practical network models. The evaluation result
shows Plotinus's effective emulation of dynamic satellite networks with
real-world devices. Its adaptability for various communication models and
algorithm testing highlights Plotinus's role as a vital tool for developing and
analyzing SAGIN systems, offering a cross-layer, real-time and scalable digital
twin system.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-24T00:00:00Z">2024-03-24</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ultra Low-Cost Two-Stage Multimodal System for Non-Normative Behavior
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Albert Lu, Stephen Cranefield
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The online community has increasingly been inundated by a toxic wave of
harmful comments. In response to this growing challenge, we introduce a
two-stage ultra-low-cost multimodal harmful behavior detection method designed
to identify harmful comments and images with high precision and recall rates.
We first utilize the CLIP-ViT model to transform tweets and images into
embeddings, effectively capturing the intricate interplay of semantic meaning
and subtle contextual clues within texts and images. Then in the second stage,
the system feeds these embeddings into a conventional machine learning
classifier like SVM or logistic regression, enabling the system to be trained
rapidly and to perform inference at an ultra-low cost. By converting tweets
into rich multimodal embeddings through the CLIP-ViT model and utilizing them
to train conventional machine learning classifiers, our system is not only
capable of detecting harmful textual information with near-perfect performance,
achieving precision and recall rates above 99\% but also demonstrates the
ability to zero-shot harmful images without additional training, thanks to its
multimodal embedding input. This capability empowers our system to identify
unseen harmful images without requiring extensive and costly image datasets.
Additionally, our system quickly adapts to new harmful content; if a new
harmful content pattern is identified, we can fine-tune the classifier with the
corresponding tweets' embeddings to promptly update the system. This makes it
well suited to addressing the ever-evolving nature of online harmfulness,
providing online communities with a robust, generalizable, and cost-effective
tool to safeguard their communities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to be appear in International Workshop on Coordination,
  Organizations, Institutions, Norms and Ethics for Governance of Multi-Agent
  Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Complementary <span class="highlight-title">Recommendation</span> in E-commerce: Definition, Approaches, and
  Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linyue Li, Zhijuan Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, complementary recommendation has received extensive
attention in the e-commerce domain. In this paper, we comprehensively summarize
and compare 34 representative studies conducted between 2009 and 2024. Firstly,
we compare the data and methods used for modeling complementary relationships
between products, including simple complementarity and more complex scenarios
such as asymmetric complementarity, the coexistence of substitution and
complementarity relationships between products, and varying degrees of
complementarity between different pairs of products. Next, we classify and
compare the models based on the research problems of complementary
recommendation, such as diversity, personalization, and cold-start.
Furthermore, we provide a comparative analysis of experimental results from
different studies conducted on the same dataset, which helps identify the
strengths and weaknesses of the research. Compared to previous surveys, this
paper provides a more updated and comprehensive summary of the research,
discusses future research directions, and contributes to the advancement of
this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages,9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RankingSHAP -- Listwise Feature Attribution Explanations for Ranking
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Heuss, Maarten de Rijke, Avishek Anand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feature attributions are a commonly used explanation type, when we want to
posthoc explain the prediction of a trained model. Yet, they are not very well
explored in IR. Importantly, feature attribution has rarely been rigorously
defined, beyond attributing the most important feature the highest value. What
it means for a feature to be more important than others is often left vague.
Consequently, most approaches focus on just selecting the most important
features and under utilize or even ignore the relative importance within
features. In this work, we rigorously define the notion of feature attribution
for ranking models, and list essential properties that a valid attribution
should have. We then propose RankingSHAP as a concrete instantiation of a
list-wise ranking attribution method. Contrary to current explanation
evaluation schemes that focus on selections, we propose two novel evaluation
paradigms for evaluating attributions over learning-to-rank models. We evaluate
RankingSHAP for commonly used learning-to-rank datasets to showcase the more
nuanced use of an attribution method while highlighting the limitations of
selection-based explanations. In a simulated experiment we design an
interpretable model to demonstrate how list-wise ranking attributes can be used
to investigate model decisions and evaluate the explanations qualitatively.
Because of the contrastive nature of the ranking task, our understanding of
ranking model decisions can substantially benefit from feature attribution
explanations like RankingSHAP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge-aware Dual-side Attribute-enhanced <span class="highlight-title">Recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16037v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16037v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taotian Pang, Xingyu Lou, Fei Zhao, Zhen Wu, Kuiyao Dong, Qiuying Peng, Yue Qi, Xinyu Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  \textit{Knowledge-aware} recommendation methods (KGR) based on \textit{graph
neural networks} (GNNs) and \textit{contrastive learning} (CL) have achieved
promising performance. However, they fall short in modeling fine-grained user
preferences and further fail to leverage the \textit{preference-attribute
connection} to make predictions, leading to sub-optimal performance. To address
the issue, we propose a method named \textit{\textbf{K}nowledge-aware
\textbf{D}ual-side \textbf{A}ttribute-enhanced \textbf{R}ecommendation} (KDAR).
Specifically, we build \textit{user preference representations} and
\textit{attribute fusion representations} upon the attribute information in
knowledge graphs, which are utilized to enhance \textit{collaborative
filtering} (CF) based user and item representations, respectively. To
discriminate the contribution of each attribute in these two types of
attribute-based representations, a \textit{multi-level collaborative alignment
contrasting} mechanism is proposed to align the importance of attributes with
CF signals. Experimental results on four benchmark datasets demonstrate the
superiority of KDAR over several state-of-the-art baselines. Further analyses
verify the effectiveness of our method. The code of KDAR is released at:
\href{https://github.com/TJTP/KDAR}{https://github.com/TJTP/KDAR}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Our Model Achieves Excellent Performance on MovieLens: What Does it
  Mean? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09985v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09985v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-chen Fan, Yitong Ji, Jie Zhang, Aixin Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A typical benchmark dataset for recommender system (RecSys) evaluation
consists of user-item interactions generated on a platform within a time
period. The interaction generation mechanism partially explains why a user
interacts with (e.g., like, purchase, rate) an item, and the context of when a
particular interaction happened. In this study, we conduct a meticulous
analysis of the MovieLens dataset and explain the potential impact of using the
dataset for evaluating recommendation algorithms. We make a few main findings
from our analysis. First, there are significant differences in user
interactions at the different stages when a user interacts with the MovieLens
platform. The early interactions largely define the user portrait which affects
the subsequent interactions. Second, user interactions are highly affected by
the candidate movies that are recommended by the platform's internal
recommendation algorithm(s). Third, changing the order of user interactions
makes it more difficult for sequential algorithms to capture the progressive
interaction process. We further discuss the discrepancy between the interaction
generation mechanism that is employed by the MovieLens system and that of
typical real-world recommendation scenarios. In summary, the MovieLens platform
demonstrates an efficient and effective way of collecting user preferences to
address cold-starts. However, models that achieve excellent recommendation
accuracy on the MovieLens dataset may not demonstrate superior performance in
practice, for at least two kinds of differences: (i) the differences in the
contexts of user-item interaction generation, and (ii) the differences in user
knowledge about the item collections. While results on MovieLens can be useful
as a reference, they should not be solely relied upon as the primary
justification for the effectiveness of a recommendation system model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spacerini: Plug-and-play Search Engines with Pyserini and Hugging Face 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.14534v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.14534v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Akiki, Odunayo Ogundepo, Aleksandra Piktus, Xinyu Zhang, Akintunde Oladipo, Jimmy Lin, Martin Potthast
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Spacerini, a tool that integrates the Pyserini toolkit for
reproducible information retrieval research with Hugging Face to enable the
seamless construction and deployment of interactive search engines. Spacerini
makes state-of-the-art sparse and dense retrieval models more accessible to
non-IR practitioners while minimizing deployment effort. This is useful for NLP
researchers who want to better understand and validate their research by
performing qualitative analyses of training corpora, for IR researchers who
want to demonstrate new retrieval models integrated into the growing Pyserini
ecosystem, and for third parties reproducing the work of other researchers.
Spacerini is open source and includes utilities for loading, preprocessing,
indexing, and deploying search engines locally and remotely. We demonstrate a
portfolio of 13 search engines created with Spacerini for different use cases.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Network
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Q-adaptive: A Multi-Agent Reinforcement Learning Based Routing on
  Dragonfly Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Kang, Xin Wang, Zhiling Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  on adaptive routing to balance network traffic for optimum performance.
Ideally, adaptive routing attempts to forward packets between minimal and
non-minimal paths with the least congestion. In practice, current adaptive
routing algorithms estimate routing path congestion based on local information
such as output queue occupancy. Using local information to estimate global path
congestion is inevitably inaccurate because a router has no precise knowledge
of link states a few hops away. This inaccuracy could lead to interconnect
congestion. In this study, we present Q-adaptive routing, a multi-agent
reinforcement learning routing scheme for Dragonfly systems. Q-adaptive routing
enables routers to learn to route autonomously by leveraging advanced
reinforcement learning technology. The proposed Q-adaptive routing is highly
scalable thanks to its fully distributed nature without using any shared
information between routers. Furthermore, a new two-level Q-table is designed
for Q-adaptive to make it computational lightly and saves 50% of router memory
usage compared with the previous Q-routing. We implement the proposed
Q-adaptive routing in SST/Merlin simulator. Our evaluation results show that
Q-adaptive routing achieves up to 10.5% system throughput improvement and 5.2x
average packet latency reduction compared with adaptive routing algorithms.
Remarkably, Q-adaptive can even outperform the optimal VALn non-minimal routing
under the ADV+1 adversarial traffic pattern with up to 3% system throughput
improvement and 75% average packet latency reduction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Study of Workload Interference with Intelligent Routing on Dragonfly 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16288v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16288v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Kang, Xin Wang, Zhiling Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dragonfly interconnect is a crucial network technology for supercomputers. To
support exascale systems, network resources are shared such that links and
routers are not dedicated to any node pair. While link utilization is
increased, workload performance is often offset by network contention.
Recently, intelligent routing built on reinforcement learning demonstrates
higher network throughput with lower packet latency. However, its effectiveness
in reducing workload interference is unknown. In this work, we present
extensive network simulations to study multi-workload contention under
different routing mechanisms, intelligent routing and adaptive routing, on a
large-scale Dragonfly system. We develop an enhanced network simulation
toolkit, along with a suite of workloads with distinctive communication
patterns. We also present two metrics to characterize application communication
intensity. Our analysis focuses on examining how different workloads interfere
with each other under different routing mechanisms by inspecting both
application-level and network-level metrics. Several key insights are made from
the analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation of Greedy and CBF for ETSI non-area GeoNetworking: The impact
  of DCC 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16237v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16237v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oscar Amador, Maria Calderon, Manuel Urueña, Ignacio Soto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper evaluates the performance of the two ETSI non-area forwarding
algorithms in the GeoNetworking specification: Greedy Forwarding and Non-Area
Contention-Based Forwarding (CBF). Non-area forwarding occurs when a packet is
sent to a geographical Destination Area from a node located outside of this
area, e.g., when a vehicle wants to alert of hazardous events to other vehicles
located in a distant geographical area. The evaluation has been carried out
both in urban and highway scenarios and takes into account the complete ETSI
Architecture, including the interaction with the Decentralized Congestion
Control (DCC) mechanism. We have also compared ETSI-defined mechanisms with
optimizations found in the literature. Our main findings are that Greedy
Forwarding, when combined with DCC, is extremely ineffective even with
optimizations, and Non-Area CBFs (both ETSI CBF and an optimized version called
S-FoT+) outperform Greedy Forwarding both in highway and urban scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interference Management for Integrated Sensing and <span class="highlight-title">Communication</span>
  Systems: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16189v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16189v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangyang Niu, Zhiqing Wei, Lin Wang, Huici Wu, Zhiyong Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emerging applications such as autonomous driving and Internet of things (IoT)
services put forward the demand for simutaneous sensing and communication
functions in the same system. Integrated sensing and communication (ISAC) has
the potential to meet the demands of ubiquitous communication and
high-precision sensing due to the advantages of spectrum and hardware resource
sharing, as well as the mutual enhancement of sensing and communication.
However, ISAC system faces severe interference requiring effective interference
suppression, avoidance, and exploitation techniques. This article provides a
comprehensive survey on the interference management techniques in ISAC systems,
involving network architecture, system design, signal processing, and resource
allocation. We first review the channel modeling and performance metrics of the
ISAC system. Then, the methods for managing self-interference (SI), mutual
interference (MI), and clutter in a single base station (BS) system are
summarized, including interference suppression, interference avoidance and
interference exploitation methods. Furthermore, cooperative interference
management methods are studied to address the cross-link interference (CLI) in
a coordinated multipoint ISAC (CoMP-ISAC) system. Finally, future trends are
revealed. This article may provide a reference for the study of interference
management in ISAC systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Digital Twin Assisted Intelligent Network Management for Vehicular
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16021v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16021v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaige Qu, Weihua Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emerging data-driven methods based on artificial intelligence (AI) have
paved the way for intelligent, flexible, and adaptive network management in
vehicular applications. To enhance network management towards network
automation, this article presents a digital twin (DT) assisted two-tier
learning framework, which facilitates the automated life-cycle management of
machine learning based intelligent network management functions (INMFs).
Specifically, at a high tier, meta learning is employed to capture different
levels of general features for the INMFs under nonstationary network
conditions. At a low tier, individual learning models are customized for local
networks based on fast model adaptation. Hierarchical DTs are deployed at the
edge and cloud servers to assist the two-tier learning process, through
closed-loop interactions with the physical network domain. Finally, a case
study demonstrates the fast and accurate model adaptation ability of meta
learning in comparison with benchmark schemes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prioritized Multi-Tenant Traffic Engineering for Dynamic QoS
  Provisioning in Autonomous SDN-OpenFlow Edge Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15975v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15975v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Sajid Shahriar, Faisal Ahmed, Genshe Chen, Khanh D. Pham, Suresh Subramaniam, Motoharu Matsuura, Hiroshi Hasegawa, Shih-Chun Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This letter indicates the critical need for prioritized multi-tenant
quality-of-service (QoS) management by emerging mobile edge systems,
particularly for high-throughput beyond fifth-generation networks. Existing
traffic engineering tools utilize complex functions baked into closed,
proprietary infrastructures, largely limiting design flexibility, scalability,
and adaptiveness. Hence, this study introduces a software-defined networking
(SDN)-based dynamic QoS provisioning scheme that prioritizes multi-tenant
network traffic while focusing on the base station-edge cloud scenario. The
designed scheme first separates control and data planes and enables traffic
management automation using SDN programmability. It then implements dynamic QoS
management via the SDN-OpenFlow protocol, which ensures ample bandwidth for
multiple priority flows and efficiently manages the remaining bandwidth for
non-priority traffic. Empirical experiments are conducted with a Mininet
network emulator and an OpenDayLight controller. Performance evaluation
validates the proposed scheme's effectiveness in meeting multi-tenant QoS
criteria, offering a robust solution for traffic prioritization in SDN-based
edge networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical <span class="highlight-title">Federated</span> Learning in Wireless Networks: Pruning Tackles
  Bandwidth Scarcity and System Heterogeneity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.01562v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.01562v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Ferdous Pervej, Richeng Jin, Huaiyu Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While a practical wireless network has many tiers where end users do not
directly communicate with the central server, the users' devices have limited
computation and battery powers, and the serving base station (BS) has a fixed
bandwidth. Owing to these practical constraints and system models, this paper
leverages model pruning and proposes a pruning-enabled hierarchical federated
learning (PHFL) in heterogeneous networks (HetNets). We first derive an upper
bound of the convergence rate that clearly demonstrates the impact of the model
pruning and wireless communications between the clients and the associated BS.
Then we jointly optimize the model pruning ratio, central processing unit (CPU)
frequency and transmission power of the clients in order to minimize the
controllable terms of the convergence bound under strict delay and energy
constraints. However, since the original problem is not convex, we perform
successive convex approximation (SCA) and jointly optimize the parameters for
the relaxed convex problem. Through extensive simulation, we validate the
effectiveness of our proposed PHFL algorithm in terms of test accuracy, wall
clock time, energy consumption and bandwidth requirement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publications in the IEEE Transactions on Wireless
  Communications (TWC); \copyright 2024 IEEE. Personal use of this material is
  permitted. Permission from IEEE must be obtained for all other uses</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-23T00:00:00Z">2024-03-23</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model, Analyze, and Comprehend User Interactions and Various Attributes
  within a Social Media Platform 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Kaykobad Reza, S M Maksudul Alam, Yiran Luo, Youzhe Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How can we effectively model, analyze, and comprehend user interactions and
various attributes within a social media platform based on post-comment
relationship? In this study, we propose a novel graph-based approach to model
and analyze user interactions within a social media platform based on
post-comment relationship. We construct a user interaction graph from social
media data and analyze it to gain insights into community dynamics, user
behavior, and content preferences. Our investigation reveals that while 56.05%
of the active users are strongly connected within the community, only 0.8% of
them significantly contribute to its dynamics. Moreover, we observe temporal
variations in community activity, with certain periods experiencing heightened
engagement. Additionally, our findings highlight a correlation between user
activity and popularity showing that more active users are generally more
popular. Alongside these, a preference for positive and informative content is
also observed where 82.41% users preferred positive and informative content.
Overall, our study provides a comprehensive framework for understanding and
managing online communities, leveraging graph-based techniques to gain valuable
insights into user behavior and community dynamics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 Pages, 8 Figures, 3 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Human-Like Machine Comprehension: Few-Shot Relational Learning
  in Visually-Rich Documents <span class="chip">COLING2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Wang, Tang Li, Chenhui Chu, Nengjun Zhu, Rui Wang, Pinpin Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Key-value relations are prevalent in Visually-Rich Documents (VRDs), often
depicted in distinct spatial regions accompanied by specific color and font
styles. These non-textual cues serve as important indicators that greatly
enhance human comprehension and acquisition of such relation triplets. However,
current document AI approaches often fail to consider this valuable prior
information related to visual and spatial features, resulting in suboptimal
performance, particularly when dealing with limited examples. To address this
limitation, our research focuses on few-shot relational learning, specifically
targeting the extraction of key-value relation triplets in VRDs. Given the
absence of a suitable dataset for this task, we introduce two new few-shot
benchmarks built upon existing supervised benchmark datasets. Furthermore, we
propose a variational approach that incorporates relational 2D-spatial priors
and prototypical rectification techniques. This approach aims to generate
relation representations that are more aware of the spatial context and unseen
relation in a manner similar to human perception. Experimental results
demonstrate the effectiveness of our proposed method by showcasing its ability
to outperform existing methods. This study also opens up new possibilities for
practical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures, accepted by LERC-COLING2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ User-Side Realization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15757v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15757v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryoma Sato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Users are dissatisfied with services. Since the service is not tailor-made
for a user, it is natural for dissatisfaction to arise. The problem is, that
even if users are dissatisfied, they often do not have the means to resolve
their dissatisfaction. The user cannot alter the source code of the service,
nor can they force the service provider to change. The user has no choice but
to remain dissatisfied or quit the service. User-side realization offers
proactive solutions to this problem by providing general algorithms to deal
with common problems on the user's side. These algorithms run on the user's
side and solve the problems without having the service provider change the
service itself.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Doctoral Thesis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ghost Sentence: A Tool for Everyday Users to Copyright Data from Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Zhao, Linchao Zhu, Ruijie Quan, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Web user data plays a central role in the ecosystem of pre-trained large
language models (LLMs) and their fine-tuned variants. Billions of data are
crawled from the web and fed to LLMs. How can \textit{\textbf{everyday web
users}} confirm if LLMs misuse their data without permission? In this work, we
suggest that users repeatedly insert personal passphrases into their documents,
enabling LLMs to memorize them. These concealed passphrases in user documents,
referred to as \textit{ghost sentences}, once they are identified in the
generated content of LLMs, users can be sure that their data is used for
training. To explore the effectiveness and usage of this copyrighting tool, we
define the \textit{user training data identification} task with ghost
sentences. Multiple datasets from various sources at different scales are
created and tested with LLMs of different sizes. For evaluation, we introduce a
last $k$ words verification manner along with two metrics: document and user
identification accuracy. In the specific case of instruction tuning of a 3B
LLaMA model, 11 out of 16 users with ghost sentences identify their data within
the generation content. These 16 users contribute 383 examples to $\sim$1.8M
training documents. For continuing pre-training of a 1.1B TinyLlama model, 61
out of 64 users with ghost sentences identify their data within the LLM output.
These 64 users contribute 1156 examples to $\sim$10M training documents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QueryExplorer: An Interactive Query Generation Assistant for Search and
  Exploration <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15667v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15667v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaustubh D. Dhole, Shivam Bajaj, Ramraj Chandradevan, Eugene Agichtein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Formulating effective search queries remains a challenging task, particularly
when users lack expertise in a specific domain or are not proficient in the
language of the content. Providing example documents of interest might be
easier for a user. However, such query-by-example scenarios are prone to
concept drift, and the retrieval effectiveness is highly sensitive to the query
generation method, without a clear way to incorporate user feedback. To enable
exploration and to support Human-In-The-Loop experiments we propose
QueryExplorer -- an interactive query generation, reformulation, and retrieval
interface with support for HuggingFace generation models and PyTerrier's
retrieval pipelines and datasets, and extensive logging of human feedback. To
allow users to create and modify effective queries, our demo supports
complementary approaches of using LLMs interactively, assisting the user with
edits and feedback at multiple stages of the query formulation process. With
support for recording fine-grained interactions and user annotations,
QueryExplorer can serve as a valuable experimental and research platform for
annotation, qualitative evaluation, and conducting Human-in-the-Loop (HITL)
experiments for complex search tasks where users struggle to formulate queries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024 Demonstration Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models for Generative <span class="highlight-title">Recommendation</span>: A <span class="highlight-title">Survey</span> and
  Visionary Discussions <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.01157v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.01157v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Li, Yongfeng Zhang, Dugang Liu, Li Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLM) not only have revolutionized the field of natural
language processing (NLP) but also have the potential to reshape many other
fields, e.g., recommender systems (RS). However, most of the related work
treats an LLM as a component of the conventional recommendation pipeline (e.g.,
as a feature extractor), which may not be able to fully leverage the generative
power of LLM. Instead of separating the recommendation process into multiple
stages, such as score computation and re-ranking, this process can be
simplified to one stage with LLM: directly generating recommendations from the
complete pool of items. This survey reviews the progress, methods, and future
directions of LLM-based generative recommendation by examining three questions:
1) What generative recommendation is, 2) Why RS should advance to generative
recommendation, and 3) How to implement LLM-based generative recommendation for
various RS tasks. We hope that this survey can provide the context and guidance
needed to explore this interesting and emerging topic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ No more optimization rules: LLM-enabled policy-based multi-modal query
  optimizer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13597v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13597v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Wang, Haodi Ma, Daisy Zhe Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model (LLM) has marked a pivotal moment in the field of
machine learning and deep learning. Recently its capability for query planning
has been investigated, including both single-modal and multi-modal queries.
However, there is no work on the query optimization capability of LLM. As a
critical (or could even be the most important) step that significantly impacts
the execution performance of the query plan, such analysis and attempts should
not be missed. From another aspect, existing query optimizers are usually
rule-based or rule-based + cost-based, i.e., they are dependent on manually
created rules to complete the query plan rewrite/transformation. Given the fact
that modern optimizers include hundreds to thousands of rules, designing a
multi-modal query optimizer following a similar way is significantly
time-consuming since we will have to enumerate as many multi-modal optimization
rules as possible, which has not been well addressed today. In this paper, we
investigate the query optimization ability of LLM and use LLM to design LaPuda,
a novel LLM and Policy based multi-modal query optimizer. Instead of
enumerating specific and detailed rules, LaPuda only needs a few abstract
policies to guide LLM in the optimization, by which much time and human effort
are saved. Furthermore, to prevent LLM from making mistakes or negative
optimization, we borrow the idea of gradient descent and propose a guided cost
descent (GCD) algorithm to perform the optimization, such that the optimization
can be kept in the correct direction. In our evaluation, our methods
consistently outperform the baselines in most cases. For example, the optimized
plans generated by our methods result in 1~3x higher execution speed than those
by the baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Yifan and Haodi contribute equally to the work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-context Learning with Retrieved Demonstrations for Language Models: A
  <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11624v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11624v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Man Luo, Xin Xu, Yue Liu, Panupong Pasupat, Mehran Kazemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models, especially pre-trained large language models, have showcased
remarkable abilities as few-shot in-context learners (ICL), adept at adapting
to new tasks with just a few demonstrations in the input context. However, the
model's ability to perform ICL is sensitive to the choice of the few-shot
demonstrations. Instead of using a fixed set of demonstrations, one recent
development is to retrieve demonstrations tailored to each input query. The
implementation of demonstration retrieval is relatively straightforward,
leveraging existing databases and retrieval systems. This not only improves the
efficiency and scalability of the learning process but also has been shown to
reduce biases inherent in manual example selection. In light of the encouraging
results and growing research in ICL with retrieved demonstrations, we conduct
an extensive review of studies in this area. In this survey, we discuss and
compare different design choices for retrieval models, retrieval training
procedures, and inference algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Network
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Delay-Optimal Forwarding and Computation Offloading for Service Chain
  Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15936v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15936v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinkun Zhang, Yuezhou Liu, Edmund Yeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emerging edge computing paradigms enable heterogeneous devices to collaborate
on complex computation applications. However, for congestible links and
computing units, delay-optimal forwarding and offloading for service chain
tasks (e.g., DNN with vertical split) in edge computing networks remains an
open problem. In this paper, we formulate the service chain forwarding and
offloading problem with arbitrary topology and heterogeneous
transmission/computation capability, and aim to minimize the aggregated network
cost. We consider congestion-aware nonlinear cost functions that cover various
performance metrics and constraints, such as average queueing delay with
limited processor capacity. We solve the non-convex optimization problem
globally by analyzing the KKT condition and proposing a sufficient condition
for optimality. We then propose a distributed algorithm that converges to the
global optimum. The algorithm adapts to changes in input rates and network
topology, and can be implemented as an online algorithm. Numerical evaluation
shows that our method significantly outperforms baselines in multiple network
instances, especially in congested scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2310.06141,
  arXiv:2205.07178</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LOAM: Low-latency <span class="highlight-title">Communication</span>, Caching, and Computation Placement in
  Data-Intensive Computing Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15927v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15927v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinkun Zhang, Edmund Yeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deploying data- and computation-intensive applications such as large-scale AI
into heterogeneous dispersed computing networks can significantly enhance
application performance by mitigating bottlenecks caused by limited network
resources, including bandwidth, storage, and computing power. However, current
resource allocation methods in dispersed computing do not provide a
comprehensive solution that considers arbitrary topology, elastic resource
amount, reuse of computation results, and nonlinear congestion-dependent
optimization objectives. In this paper, we propose LOAM, a low-latency joint
communication, caching, and computation placement framework with a rigorous
analytical foundation that incorporates the above aspects. We tackle the
NP-hard aggregated cost minimization problem with two methods: an offline
method with a 1/2 approximation and an online adaptive method with a bounded
gap from the optimum. Through extensive simulation, the proposed framework
outperforms multiple baselines in both synthesis and real-world network
scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deciphering the Digital Veil: Exploring the Ecosystem of DNS HTTPS
  Resource Records 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongying Dong, Yizhe Zhang, Hyeonmin Lee, Shumon Huque, Yixin Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The DNS HTTPS resource record is a new DNS record type designed for the
delivery of configuration information and parameters required to initiate
connections to HTTPS network services. It provides the ability to perform zone
apex redirection to a third-party provider, which the existing CNAME record
cannot do. In addition, it is a key enabler for TLS Encrypted ClientHello (ECH)
by providing the cryptographic keying material needed to encrypt the initial
exchange. To understand the adoption and security of this new DNS HTTPS record,
we perform a longitudinal study on the server-side deployment of DNS HTTPS for
Tranco top 1 million domains over 8 months, as well as the client-side support
for DNS HTTPS from major browsers. To the best of knowledge, our work is the
first longitudinal study on DNS HTTPS server deployment, and the first known
study on client-side support for DNS HTTPS. Despite the rapidly growing trend
of DNS HTTPS adoption, our study highlights concerns in the deployment by both
servers and clients, such as the complexity in properly maintaining HTTPS
records and the concerning hardfail mechanisms in browser when using HTTPS
records.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Non-Terrestrial Networks Architecture: All Optical LEO
  Constellations with High-Altitude Ground Stations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo G. Madoery, Juan A. Fraire, Jorge M. Finochietto, Halim Yanikomeroglu, Gunes Karabulut Kurt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of low Earth orbit (LEO) satellite mega-constellations is
dynamically transforming the space sector. While free-space optical (FSO) links
efficiently facilitate intersatellite data forwarding, they suffer from
atmospheric/weather conditions in the space-to-ground link. This study delves
into utilizing high-altitude platform stations (HAPS) as elevated relay
stations strategically positioned above terrestrial ground stations. We
introduce the concept of high-altitude ground stations (HAGS), an innovative
approach to enabling the development of all optical LEO satellite
constellations. The first contribution is an analysis of the HAGS-based network
architecture where the LEO spacecraft only hosts FSO transceivers. Secondly, we
execute an extensive simulation campaign to determine the gain of HAGS,
including a new equivalency model with the traditional ground station approach.
Finally, we examine the research challenges of implementing HAGS-based, all
optical LEO mega-constellations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE International Conference on Communications 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Files to <span class="highlight-title">Stream</span>s: Revisiting Web History and Exploring Potentials
  for Future Prospects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07828v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07828v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Vogel, Thomas Springer, Matthias Wählisch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the last 30 years, the World Wide Web has changed significantly. In this
paper, we argue that common practices to prepare web pages for delivery
conflict with many efforts to present content with minimal latency, one
fundamental goal that pushed changes in the WWW. To bolster our arguments, we
revisit reasons that led to changes of HTTP and compare them systematically
with techniques to prepare web pages. We found that the structure of many web
pages leverages features of HTTP/1.1 but hinders the use of recent HTTP
features to present content quickly. To improve the situation in the future, we
propose fine-grained content segmentation. This would allow to exploit
streaming capabilities of recent HTTP versions and to render content as quickly
as possible without changing underlying protocols or web browsers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Offloading and Quality Control for AI Generated Content Services in 6G
  <span class="highlight-title">Mobile</span> Edge Computing Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06203v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06203v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yitong Wang, Chang Liu, Jun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI-Generated Content (AIGC), as a novel manner of providing Metaverse
services in the forthcoming Internet paradigm, can resolve the obstacles of
immersion requirements. Concurrently, edge computing, as an evolutionary
paradigm of computing in communication systems, effectively augments real-time
interactive services. In pursuit of enhancing the accessibility of AIGC
services, the deployment of AIGC models (e.g., diffusion models) to edge
servers and local devices has become a prevailing trend. Nevertheless, this
approach faces constraints imposed by battery life and computational resources
when tasks are offloaded to local devices, limiting the capacity to deliver
high-quality content to users while adhering to stringent latency requirements.
So there will be a tradeoff between the utility of AIGC models and offloading
decisions in the edge computing paradigm. This paper proposes a joint
optimization algorithm for offloading decisions, computation time, and
diffusion steps of the diffusion models in the reverse diffusion stage.
Moreover, we take the average error into consideration as the metric for
evaluating the quality of the generated results. Experimental results
conclusively demonstrate that the proposed algorithm achieves superior joint
optimization performance compared to the baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper appears in the 2024 IEEE 99th Vehicular Technology
  Conference (VTC)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-22T00:00:00Z">2024-03-22</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spectral Initialization for High-Dimensional Phase Retrieval with Biased
  Spatial Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15548v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15548v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierre Bousseyroux, Marc Potters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore a spectral initialization method that plays a central role in
contemporary research on signal estimation in nonconvex scenarios. In a
noiseless phase retrieval framework, we precisely analyze the method's
performance in the high-dimensional limit when sensing vectors follow a
multivariate Gaussian distribution for two rotationally invariant models of the
covariance matrix C. In the first model C is a projector on a lower dimensional
space while in the second it is a Wishart matrix. Our analytical results extend
the well-established case when C is the identity matrix. Our examination shows
that the introduction of biased spatial directions leads to a substantial
improvement in the spectral method's effectiveness, particularly when the
number of measurements is less than the signal's dimension. This extension also
consistently reveals a phase transition phenomenon dependent on the ratio
between sample size and signal dimension. Surprisingly, both of these models
share the same threshold value.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LimGen: Probing the LLMs for Generating Suggestive Limitations of
  Research Papers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15529v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15529v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdur Rahman Bin Md Faizullah, Ashok Urlana, Rahul Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Examining limitations is a crucial step in the scholarly research reviewing
process, revealing aspects where a study might lack decisiveness or require
enhancement. This aids readers in considering broader implications for further
research. In this article, we present a novel and challenging task of
Suggestive Limitation Generation (SLG) for research papers. We compile a
dataset called LimGen, encompassing 4068 research papers and their associated
limitations from the ACL anthology. We investigate several approaches to
harness large language models (LLMs) for producing suggestive limitations, by
thoroughly examining the related challenges, practical insights, and potential
opportunities. Our LimGen dataset and code can be accessed at
https://github.com/armbf/LimGen.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fundus: A Simple-to-Use News Scraper Optimized for High Quality
  Extractions <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Dallabetta, Conrad Dobberstein, Adrian Breiding, Alan Akbik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Fundus, a user-friendly news scraper that enables users
to obtain millions of high-quality news articles with just a few lines of code.
Unlike existing news scrapers, we use manually crafted, bespoke content
extractors that are specifically tailored to the formatting guidelines of each
supported online newspaper. This allows us to optimize our scraping for quality
such that retrieved news articles are textually complete and without HTML
artifacts. Further, our framework combines both crawling (retrieving HTML from
the web or large web archives) and content extraction into a single pipeline.
By providing a unified interface for a predefined collection of newspapers, we
aim to make Fundus broadly usable even for non-technical users. This paper
gives an overview of the framework, discusses our design choices, and presents
a comparative evaluation against other popular news scrapers. Our evaluation
shows that Fundus yields significantly higher quality extractions (complete and
artifact-free news articles) than prior work. The framework is available on
GitHub under https://github.com/flairNLP/fundus and can be simply installed
using pip.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, submitted to ACL 2024, for a screencast see
  https://www.youtube.com/watch?v=9GJExMelhdI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FollowIR: Evaluating and Teaching Information Retrieval Models to Follow
  Instructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Orion Weller, Benjamin Chang, Sean MacAvaney, Kyle Lo, Arman Cohan, Benjamin Van Durme, Dawn Lawrie, Luca Soldaini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern Large Language Models (LLMs) are capable of following long and complex
instructions that enable a diverse amount of user tasks. However, despite
Information Retrieval (IR) models using LLMs as the backbone of their
architectures, nearly all of them still only take queries as input, with no
instructions. For the handful of recent models that do take instructions, it's
unclear how they use them. We introduce our dataset FollowIR, which contains a
rigorous instruction evaluation benchmark as well as a training set for helping
IR models learn to better follow real-world instructions. FollowIR builds off
the long history of the TREC conferences: as TREC provides human annotators
with instructions (also known as narratives) to determine document relevance,
so should IR models be able to understand and decide relevance based on these
detailed instructions. Our evaluation benchmark starts with three deeply judged
TREC collections and alters the annotator instructions, re-annotating relevant
documents. Through this process, we can measure how well IR models follow
instructions, through a new pairwise evaluation framework. Our results indicate
that existing retrieval models fail to correctly use instructions, using them
for basic keywords and struggling to understand long-form information. However,
we show that it is possible for IR models to learn to follow complex
instructions: our new FollowIR-7B model has significant improvements (over 13%)
after fine-tuning on our training set.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GTC: GNN-Transformer Co-contrastive Learning for Self-supervised
  Heterogeneous Graph Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yundong Sun, Dongjie Zhu, Yansong Wang, Zhaoshuo Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have emerged as the most powerful weapon for
various graph tasks due to the message-passing mechanism's great local
information aggregation ability. However, over-smoothing has always hindered
GNNs from going deeper and capturing multi-hop neighbors. Unlike GNNs,
Transformers can model global information and multi-hop interactions via
multi-head self-attention and a proper Transformer structure can show more
immunity to the over-smoothing problem. So, can we propose a novel framework to
combine GNN and Transformer, integrating both GNN's local information
aggregation and Transformer's global information modeling ability to eliminate
the over-smoothing problem? To realize this, this paper proposes a
collaborative learning scheme for GNN-Transformer and constructs GTC
architecture. GTC leverages the GNN and Transformer branch to encode node
information from different views respectively, and establishes contrastive
learning tasks based on the encoded cross-view information to realize
self-supervised heterogeneous graph representation. For the Transformer branch,
we propose Metapath-aware Hop2Token and CG-Hetphormer, which can cooperate with
GNN to attentively encode neighborhood information from different levels. As
far as we know, this is the first attempt in the field of graph representation
learning to utilize both GNN and Transformer to collaboratively capture
different view information and conduct cross-view contrastive learning. The
experiments on real datasets show that GTC exhibits superior performance
compared with state-of-the-art methods. Codes can be available at
https://github.com/PHD-lanyu/GTC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bilateral Unsymmetrical Graph Contrastive Learning for <span class="highlight-title">Recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15075v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15075v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaheng Yu, Jing Li, Yue He, Kai Zhu, Shuyi Zhang, Wen Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent methods utilize graph contrastive Learning within graph-structured
user-item interaction data for collaborative filtering and have demonstrated
their efficacy in recommendation tasks. However, they ignore that the
difference relation density of nodes between the user- and item-side causes the
adaptability of graphs on bilateral nodes to be different after multi-hop graph
interaction calculation, which limits existing models to achieve ideal results.
To solve this issue, we propose a novel framework for recommendation tasks
called Bilateral Unsymmetrical Graph Contrastive Learning (BusGCL) that
consider the bilateral unsymmetry on user-item node relation density for sliced
user and item graph reasoning better with bilateral slicing contrastive
training. Especially, taking into account the aggregation ability of
hypergraph-based graph convolutional network (GCN) in digging implicit
similarities is more suitable for user nodes, embeddings generated from three
different modules: hypergraph-based GCN, GCN and perturbed GCN, are sliced into
two subviews by the user- and item-side respectively, and selectively combined
into subview pairs bilaterally based on the characteristics of inter-node
relation structure. Furthermore, to align the distribution of user and item
embeddings after aggregation, a dispersing loss is leveraged to adjust the
mutual distance between all embeddings for maintaining learning ability.
Comprehensive experiments on two public datasets have proved the superiority of
BusGCL in comparison to various recommendation methods. Other models can simply
utilize our bilateral slicing contrastive learning to enhance recommending
performance without incurring extra expenses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Modeling for Content-enriched <span class="highlight-title">Recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10435v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10435v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junzhe Jiang, Shang Qu, Mingyue Cheng, Qi Liu, Zhiding Liu, Hao Zhang, Rujiao Zhang, Kai Zhang, Rui Li, Jiatong Li, Min Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are indispensable in the realm of online applications,
and sequential recommendation has enjoyed considerable prevalence due to its
capacity to encapsulate the dynamic shifts in user interests. However, previous
sequential modeling methods still have limitations in capturing contextual
information. The primary reason is the lack of understanding of domain-specific
knowledge and item-related textual content by language models. Fortunately, the
emergence of powerful language models has unlocked the potential to incorporate
extensive world knowledge into recommendation algorithms, enabling them to go
beyond simple item attributes and truly understand the world surrounding user
preferences. To achieve this, we propose LANCER, which leverages the semantic
understanding capabilities of pre-trained language models to generate
personalized recommendations. Our approach bridges the gap between language
models and recommender systems, resulting in more human-like recommendations.
We demonstrate the effectiveness of our approach through a series of
experiments conducted on multiple benchmark datasets, showing promising results
and providing valuable insights into the influence of our model on sequential
recommendation tasks. Furthermore, our experimental codes are publicly
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Image Search in Histopathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.08699v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.08699v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        H. R. Tizhoosh, Liron Pantanowitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pathology images of histopathology can be acquired from camera-mounted
microscopes or whole slide scanners. Utilizing similarity calculations to match
patients based on these images holds significant potential in research and
clinical contexts. Recent advancements in search technologies allow for
implicit quantification of tissue morphology across diverse primary sites,
facilitating comparisons and enabling inferences about diagnosis, and
potentially prognosis, and predictions for new patients when compared against a
curated database of diagnosed and treated cases. In this paper, we
comprehensively review the latest developments in image search technologies for
histopathology, offering a concise overview tailored for computational
pathology researchers seeking effective, fast and efficient image search
methods in their work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A chapter in the Book "Artificial INtelligence in Digital Pathology"
  by Cohen and Chauhan, 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Network
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Modular, End-to-End Next-Generation Network Testbed: Towards a Fully
  Automated Network Management Platform 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Chouman, Dimitrios Michael Manias, Abdallah Shami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Experimentation in practical, end-to-end (E2E) next-generation networks
deployments is becoming increasingly prevalent and significant in the realm of
modern networking and wireless communications research. The prevalence of
fifth-generation technology (5G) testbeds and the emergence of developing
networks systems, for the purposes of research and testing, focus on the
capabilities and features of analytics, intelligence, and automated management
using novel testbed designs and architectures, ranging from simple simulations
and setups to complex networking systems; however, with the ever-demanding
application requirements for modern and future networks, 5G-and-beyond (denoted
as 5G+) testbed experimentation can be useful in assessing the creation of
large-scale network infrastructures that are capable of supporting E2E
virtualized mobile network services. To this end, this paper presents a
functional, modular E2E 5G+ system, complete with the integration of a Radio
Access Network (RAN) and handling the connection of User Equipment (UE) in
real-world scenarios. As well, this paper assesses and evaluates the
effectiveness of emulating full network functionalities and capabilities,
including a complete description of user-plane data, from UE registrations to
communications sequences, and leads to the presentation of a future outlook in
powering new experimentation for 6G and next-generation networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was submitted to IEEE Transactions on Network and Service
  Management</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Network Calculus Characterization of Congestion Control for Time-Varying
  Traffic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15303v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15303v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harvinder Lehal, Natchanon Luangsomboon, Jörg Liebeherr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Models for the dynamics of congestion control generally involve systems of
coupled differential equations. Universally, these models assume that traffic
sources saturate the maximum transmissions allowed by the congestion control
method. This is not suitable for studying congestion control of intermittent
but bursty traffic sources. In this paper, we present a characterization of
congestion control for arbitrary time-varying traffic that applies to
rate-based as well as window-based congestion control. We leverage the
capability of network calculus to precisely describe the input-output
relationship at network elements for arbitrary source traffic. We show that our
characterization can closely track the dynamics of even complex congestion
control algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Blockchain-based Pseudonym Management for Vehicle Twin Migrations in
  Vehicular Edge Metaverse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawen Kang, Xiaofeng Luo, Jiangtian Nie, Tianhao Wu, Haibo Zhou, Yonghua Wang, Dusit Niyato, Shiwen Mao, Shengli Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Driven by the great advances in metaverse and edge computing technologies,
vehicular edge metaverses are expected to disrupt the current paradigm of
intelligent transportation systems. As highly computerized avatars of Vehicular
Metaverse Users (VMUs), the Vehicle Twins (VTs) deployed in edge servers can
provide valuable metaverse services to improve driving safety and on-board
satisfaction for their VMUs throughout journeys. To maintain uninterrupted
metaverse experiences, VTs must be migrated among edge servers following the
movements of vehicles. This can raise concerns about privacy breaches during
the dynamic communications among vehicular edge metaverses. To address these
concerns and safeguard location privacy, pseudonyms as temporary identifiers
can be leveraged by both VMUs and VTs to realize anonymous communications in
the physical space and virtual spaces. However, existing pseudonym management
methods fall short in meeting the extensive pseudonym demands in vehicular edge
metaverses, thus dramatically diminishing the performance of privacy
preservation. To this end, we present a cross-metaverse empowered dual
pseudonym management framework. We utilize cross-chain technology to enhance
management efficiency and data security for pseudonyms. Furthermore, we propose
a metric to assess the privacy level and employ a Multi-Agent Deep
Reinforcement Learning (MADRL) approach to obtain an optimal pseudonym
generating strategy. Numerical results demonstrate that our proposed schemes
are high-efficiency and cost-effective, showcasing their promising applications
in vehicular edge metaverses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Experimental Studies of Metaverse <span class="highlight-title">Stream</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haopeng Wang, Roberto Martinez-Velazquez, Haiwei Dong, Abdulmotaleb El Saddik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metaverse aims to construct a large, unified, immersive, and shared digital
realm by combining various technologies, namely XR (extended reality),
blockchain, and digital twin, among others. This article explores the Metaverse
from the perspective of multimedia communication by conducting and analyzing
real-world experiments on four different Metaverse platforms: VR (virtual
reality) Vircadia, VR Mozilla Hubs, VRChat, and MR (mixed reality) Virtual
City. We first investigate the traffic patterns and network performance in the
three VR platforms. After raising the challenges of the Metaverse streaming and
investigating the potential methods to enhance Metaverse performance, we
propose a remote rendering architecture and verify its advantages through a
prototype involving the campus network and MR multimodal interaction by
comparison with local rendering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Consumer Electronics Magazine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CODA: A COst-efficient Test-time Domain Adaptation Mechanism for HAR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14922v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14922v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghui Qiu, Yandao Huang, Lin Chen, Lu Wang, Kaishun Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, emerging research on mobile sensing has led to novel
scenarios that enhance daily life for humans, but dynamic usage conditions
often result in performance degradation when systems are deployed in real-world
settings. Existing solutions typically employ one-off adaptation schemes based
on neural networks, which struggle to ensure robustness against uncertain
drifting conditions in human-centric sensing scenarios. In this paper, we
propose CODA, a COst-efficient Domain Adaptation mechanism for mobile sensing
that addresses real-time drifts from the data distribution perspective with
active learning theory, ensuring cost-efficient adaptation directly on the
device. By incorporating a clustering loss and importance-weighted active
learning algorithm, CODA retains the relationship between different clusters
during cost-effective instance-level updates, preserving meaningful structure
within the data distribution. We also showcase its generalization by seamlessly
integrating it with Neural Network-based solutions for Human Activity
Recognition tasks. Through meticulous evaluations across diverse datasets,
including phone-based, watch-based, and integrated sensor-based sensing tasks,
we demonstrate the feasibility and potential of online adaptation with CODA.
The promising results achieved by CODA, even without learnable parameters, also
suggest the possibility of realizing unobtrusive adaptation through specific
application designs with sufficient feedback.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Traffic Management Framework for On-Demand Urban Air Mobility Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.07139v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.07139v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Milad Pooladsanj, Ketan Savla, Petros A. Ioannou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Urban Air Mobility (UAM) offers a solution to current traffic congestion by
providing on-demand air mobility in urban areas. Effective traffic management
is crucial for efficient operation of UAM systems, especially for high-demand
scenarios. In this paper, we present a centralized traffic management framework
for on-demand UAM systems. Specifically, we provide a scheduling policy, called
VertiSync, which schedules the aircraft for either servicing trip requests or
rebalancing in the system subject to aircraft safety margins and energy
requirements. We characterize the system-level throughput of VertiSync, which
determines the demand threshold at which passenger waiting times transition
from being stabilized to being increasing over time. We show that the proposed
policy is able to maximize throughput for sufficiently large fleet sizes. We
demonstrate the performance of VertiSync through a case study for the city of
Los Angeles, and show that it significantly reduces passenger waiting times
compared to a first-come first-serve scheduling policy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LogPrécis: Unleashing Language Models for Automated Malicious Log
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.08309v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.08309v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Boffa, Rodolfo Vieira Valentim, Luca Vassio, Danilo Giordano, Idilio Drago, Marco Mellia, Zied Ben Houidi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The collection of security-related logs holds the key to understanding attack
behaviors and diagnosing vulnerabilities. Still, their analysis remains a
daunting challenge. Recently, Language Models (LMs) have demonstrated unmatched
potential in understanding natural and programming languages. The question
arises whether and how LMs could be also useful for security experts since
their logs contain intrinsically confused and obfuscated information. In this
paper, we systematically study how to benefit from the state-of-the-art in LM
to automatically analyze text-like Unix shell attack logs. We present a
thorough design methodology that leads to LogPr\'ecis. It receives as input raw
shell sessions and automatically identifies and assigns the attacker tactic to
each portion of the session, i.e., unveiling the sequence of the attacker's
goals. We demonstrate LogPr\'ecis capability to support the analysis of two
large datasets containing about 400,000 unique Unix shell attacks. LogPr\'ecis
reduces them into about 3,000 fingerprints, each grouping sessions with the
same sequence of tactics. The abstraction it provides lets the analyst better
understand attacks, identify fingerprints, detect novelty, link similar
attacks, and track families and mutations. Overall, LogPr\'ecis, released as
open source, paves the way for better and more responsive defense against
cyberattacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, Computer&Security
  (https://www.sciencedirect.com/science/article/pii/S0167404824001068), code
  available at https://github.com/SmartData-Polito/logprecis, models available
  at https://huggingface.co/SmartDataPolito</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sparse Mean Field Load Balancing in Large Localized Queueing Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12973v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12973v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anam Tahir, Kai Cui, Heinz Koeppl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scalable load balancing algorithms are of great interest in cloud networks
and data centers, necessitating the use of tractable techniques to compute
optimal load balancing policies for good performance. However, most existing
scalable techniques, especially asymptotically scaling methods based on mean
field theory, have not been able to model large queueing networks with strong
locality. Meanwhile, general multi-agent reinforcement learning techniques can
be hard to scale and usually lack a theoretical foundation. In this work, we
address this challenge by leveraging recent advances in sparse mean field
theory to learn a near-optimal load balancing policy in sparsely connected
queueing networks in a tractable manner, which may be preferable to global
approaches in terms of wireless communication overhead. Importantly, we obtain
a general load balancing framework for a large class of sparse bounded-degree
wireless topologies. By formulating a novel mean field control problem in the
context of graphs with bounded degree, we reduce the otherwise difficult
multi-agent problem to a single-agent problem. Theoretically, the approach is
justified by approximation guarantees. Empirically, the proposed methodology
performs well on several realistic and scalable wireless network topologies as
compared to a number of well-known load balancing heuristics and existing
scalable multi-agent reinforcement learning methods.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-21T00:00:00Z">2024-03-21</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating the Performance of LLMs on Technical Language Processing
  tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15503v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15503v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Kernycky, David Coleman, Christopher Spence, Udayan Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we present the results of an evaluation study of the
perfor-mance of LLMs on Technical Language Processing tasks. Humans are often
confronted with tasks in which they have to gather information from dispar-ate
sources and require making sense of large bodies of text. These tasks can be
significantly complex for humans and often require deep study including
rereading portions of a text. Towards simplifying the task of gathering
in-formation we evaluated LLMs with chat interfaces for their ability to
provide answers to standard questions that a human can be expected to answer
based on their reading of a body of text. The body of text under study is Title
47 of the United States Code of Federal Regulations (CFR) which describes
regula-tions for commercial telecommunications as governed by the Federal
Com-munications Commission (FCC). This has been a body of text of interest
be-cause our larger research concerns the issue of making sense of information
related to Wireless Spectrum Governance and usage in an automated manner to
support Dynamic Spectrum Access. The information concerning this wireless
spectrum domain is found in many disparate sources, with Title 47 of the CFR
being just one of many. Using a range of LLMs and providing the required CFR
text as context we were able to quantify the performance of those LLMs on the
specific task of answering the questions below.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Medical Support in the Arabic Language Through Personalized
  ChatGPT Assistance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Issa, Ahmed Abdelwahed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This Paper discusses the growing popularity of online medical diagnosis as an
alternative to traditional doctor visits. It highlights the limitations of
existing tools and emphasizes the advantages of using ChatGPT, which provides
real-time, personalized medical diagnosis at no cost. The paragraph summarizes
a research study that evaluated the performance of ChatGPT in Arabic medical
diagnosis. The study involved compiling a dataset of disease information and
generating multiple messages for each disease using different prompting
techniques. ChatGPT's performance was assessed by measuring the similarity
between its responses and the actual diseases. The results showed promising
performance, with average scores of around 76% for similarity measures. Various
prompting techniques were used, and chain prompting demonstrated a relative
advantage. The study also recorded an average response time of 6.12 seconds for
the ChatGPT API, which is considered acceptable but has room for improvement.
While ChatGPT cannot replace human doctors entirely, the findings suggest its
potential in emergency cases and addressing general medical inquiries. Overall,
the study highlights ChatGPT's viability as a valuable tool in the medical
field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was presented at The International conference for Arabic
  language and applied linguistics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ gTBLS: Generating Tables from Text by Conditional Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anirudh Sundar, Christopher Richardson, Larry Heck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distilling large, unstructured text into a structured, condensed form such as
tables is an open research problem. One of the primary challenges in
automatically generating tables is ensuring their syntactic validity. Prior
approaches address this challenge by including additional parameters in the
Transformer's attention mechanism to attend to specific rows and column
headers. In contrast to this single-stage method, this paper presents a
two-stage approach called Generative Tables (gTBLS). The first stage infers
table structure (row and column headers) from the text. The second stage
formulates questions using these headers and fine-tunes a causal language model
to answer them. Furthermore, the gTBLS approach is amenable to the utilization
of pre-trained Large Language Models in a zero-shot configuration, presenting a
solution for table generation in situations where fine-tuning is not feasible.
gTBLS improves prior approaches by up to 10% in BERTScore on the table
construction task and up to 20% on the table content generation task of the
E2E, WikiTableText, WikiBio, and RotoWire datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge-Enhanced <span class="highlight-title">Recommendation</span> with User-Centric Subgraph Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangyi Liu, Quanming Yao, Yongqi Zhang, Lei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommendation systems, as widely implemented nowadays on various platforms,
recommend relevant items to users based on their preferences. The classical
methods which rely on user-item interaction matrices has limitations,
especially in scenarios where there is a lack of interaction data for new
items. Knowledge graph (KG)-based recommendation systems have emerged as a
promising solution. However, most KG-based methods adopt node embeddings, which
do not provide personalized recommendations for different users and cannot
generalize well to the new items. To address these limitations, we propose
Knowledge-enhanced User-Centric subgraph Network (KUCNet), a subgraph learning
approach with graph neural network (GNN) for effective recommendation. KUCNet
constructs a U-I subgraph for each user-item pair that captures both the
historical information of user-item interactions and the side information
provided in KG. An attention-based GNN is designed to encode the U-I subgraphs
for recommendation. Considering efficiency, the pruned user-centric computation
graph is further introduced such that multiple U-I subgraphs can be
simultaneously computed and that the size can be pruned by Personalized
PageRank. Our proposed method achieves accurate, efficient, and interpretable
recommendations especially for new items. Experimental results demonstrate the
superiority of KUCNet over state-of-the-art KG-based and collaborative
filtering (CF)-based methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FIT-RAG: Black-Box RAG with Factual Information and Token Reduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuren Mao, Xuemei Dong, Wenyi Xu, Yunjun Gao, Bin Wei, Ying Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the extraordinarily large number of parameters, fine-tuning Large
Language Models (LLMs) to update long-tail or out-of-date knowledge is
impractical in lots of applications. To avoid fine-tuning, we can alternatively
treat a LLM as a black-box (i.e., freeze the parameters of the LLM) and augment
it with a Retrieval-Augmented Generation (RAG) system, namely black-box RAG.
Recently, black-box RAG has achieved success in knowledge-intensive tasks and
has gained much attention. Existing black-box RAG methods typically fine-tune
the retriever to cater to LLMs' preferences and concatenate all the retrieved
documents as the input, which suffers from two issues: (1) Ignorance of Factual
Information. The LLM preferred documents may not contain the factual
information for the given question, which can mislead the retriever and hurt
the effectiveness of black-box RAG; (2) Waste of Tokens. Simply concatenating
all the retrieved documents brings large amounts of unnecessary tokens for
LLMs, which degenerates the efficiency of black-box RAG. To address these
issues, this paper proposes a novel black-box RAG framework which utilizes the
factual information in the retrieval and reduces the number of tokens for
augmentation, dubbed FIT-RAG. FIT-RAG utilizes the factual information by
constructing a bi-label document scorer. Besides, it reduces the tokens by
introducing a self-knowledge recognizer and a sub-document-level token reducer.
FIT-RAG achieves both superior effectiveness and efficiency, which is validated
by extensive experiments across three open-domain question-answering datasets:
TriviaQA, NQ and PopQA. FIT-RAG can improve the answering accuracy of
Llama2-13B-Chat by 14.3\% on TriviaQA, 19.9\% on NQ and 27.5\% on PopQA,
respectively. Furthermore, it can save approximately half of the tokens on
average across the three datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding the Ranking Loss for <span class="highlight-title">Recommendation</span> with Sparse User
  Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14144v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14144v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhutian Lin, Junwei Pan, Shangyu Zhang, Ximei Wang, Xi Xiao, Shudong Huang, Lei Xiao, Jie Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Click-through rate (CTR) prediction holds significant importance in the realm
of online advertising. While many existing approaches treat it as a binary
classification problem and utilize binary cross entropy (BCE) as the
optimization objective, recent advancements have indicated that combining BCE
loss with ranking loss yields substantial performance improvements. However,
the full efficacy of this combination loss remains incompletely understood. In
this paper, we uncover a new challenge associated with BCE loss in scenarios
with sparse positive feedback, such as CTR prediction: the gradient vanishing
for negative samples. Subsequently, we introduce a novel perspective on the
effectiveness of ranking loss in CTR prediction, highlighting its ability to
generate larger gradients on negative samples, thereby mitigating their
optimization issues and resulting in improved classification ability. Our
perspective is supported by extensive theoretical analysis and empirical
evaluation conducted on publicly available datasets. Furthermore, we
successfully deployed the ranking loss in Tencent's online advertising system,
achieving notable lifts of 0.70% and 1.26% in Gross Merchandise Value (GMV) for
two main scenarios. The code for our approach is openly accessible at the
following GitHub repository:
https://github.com/SkylerLinn/Understanding-the-Ranking-Loss.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M3: A Multi-Task Mixed-Objective Learning Framework for Open-Domain
  Multi-Hop Dense Sentence Retrieval <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Bai, Anthony Colas, Christan Grant, Daisy Zhe Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent research, contrastive learning has proven to be a highly effective
method for representation learning and is widely used for dense retrieval.
However, we identify that relying solely on contrastive learning can lead to
suboptimal retrieval performance. On the other hand, despite many retrieval
datasets supporting various learning objectives beyond contrastive learning,
combining them efficiently in multi-task learning scenarios can be challenging.
In this paper, we introduce M3, an advanced recursive Multi-hop dense sentence
retrieval system built upon a novel Multi-task Mixed-objective approach for
dense text representation learning, addressing the aforementioned challenges.
Our approach yields state-of-the-art performance on a large-scale open-domain
fact verification benchmark dataset, FEVER. Code and data are available at:
https://github.com/TonyBY/M3
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EasyInstruct: An Easy-to-use Instruction Processing Framework for Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03049v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03049v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixin Ou, Ningyu Zhang, Honghao Gui, Ziwen Xu, Shuofei Qiao, Yida Xue, Runnan Fang, Kangwei Liu, Lei Li, Zhen Bi, Guozhou Zheng, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, instruction tuning has gained increasing attention and
emerged as a crucial technique to enhance the capabilities of Large Language
Models (LLMs). To construct high-quality instruction datasets, many instruction
processing approaches have been proposed, aiming to achieve a delicate balance
between data quantity and data quality. Nevertheless, due to inconsistencies
that persist among various instruction processing methods, there is no standard
open-source instruction processing implementation framework available for the
community, which hinders practitioners from further developing and advancing.
To facilitate instruction processing research and development, we present
EasyInstruct, an easy-to-use instruction processing framework for LLMs, which
modularizes instruction generation, selection, and prompting, while also
considering their combination and interaction. EasyInstruct is publicly
released and actively maintained at https://github.com/zjunlp/EasyInstruct,
along with an online demo app and a demo video for quick-start, calling for
broader research centered on instruction data and synthetic data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://zjunlp.github.io/project/EasyInstruct Code:
  https://github.com/zjunlp/EasyInstruct Video: https://youtu.be/rfQOWYfziFo
  Demo: https://huggingface.co/spaces/zjunlp/EasyInstruct</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discrete Semantic Tokenization for Deep CTR <span class="highlight-title">Predict</span>ion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08206v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08206v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qijiong Liu, Hengchang Hu, Jiahao Wu, Jieming Zhu, Min-Yen Kan, Xiao-Ming Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incorporating item content information into click-through rate (CTR)
prediction models remains a challenge, especially with the time and space
constraints of industrial scenarios. The content-encoding paradigm, which
integrates user and item encoders directly into CTR models, prioritizes space
over time. In contrast, the embedding-based paradigm transforms item and user
semantics into latent embeddings, subsequently caching them to optimize
processing time at the expense of space. In this paper, we introduce a new
semantic-token paradigm and propose a discrete semantic tokenization approach,
namely UIST, for user and item representation. UIST facilitates swift training
and inference while maintaining a conservative memory footprint. Specifically,
UIST quantizes dense embedding vectors into discrete tokens with shorter
lengths and employs a hierarchical mixture inference module to weigh the
contribution of each user--item token pair. Our experimental results on news
recommendation showcase the effectiveness and efficiency (about 200-fold space
compression) of UIST for CTR prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TheWebConf 2024 accepted paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pushing the Limits: Concurrency Detection in Acyclic Sound Free-Choice
  Workflow Nets in $O(P^2 + T^2)$ 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.16097v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.16097v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas M. Prinz, Julien Klaus, Nick R. T. P. van Beest
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concurrency is an important aspect of Petri nets to describe and simulate the
behavior of complex systems. Knowing which places and transitions could be
executed in parallel helps to understand nets and enables analysis techniques
and the computation of other properties, such as causality, exclusivity, etc..
All techniques based on concurrency detection depend on the efficiency of this
detection methodology. Kovalyov and Esparza have developed algorithms that
compute all concurrent places in $O\big((P+T)TP^2\big)$ for live and bounded
nets (where $P$ and $T$ are the numbers of places and transitions) and in
$O\big(P(P+T)^2\big)$ for live and bounded free-choice nets. Although these
algorithms have a reasonably good computational complexity, large numbers of
concurrent pairs of nodes may still lead to long computation times. This paper
complements the palette of concurrency detection algorithms with the Concurrent
Paths (CP) algorithm for sound free-choice workflow nets. The algorithm allows
parallelization and has a worst-case computational complexity of $O(P^2 + T^2)$
for acyclic nets and of $O(P^3 + PT^2)$ for cyclic nets. Although the
computational complexity of cyclic nets has not improved, the evaluation shows
the benefits of CP, especially, if the net contains many nodes in concurrency
relation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 14 figures, 5 algorithms</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TensorBank: Tensor Lakehouse for Foundation Model Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.02094v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.02094v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Romeo Kienzler, Leonardo Pondian Tizzei, Benedikt Blumenstiel, Zoltan Arnold Nagy, S. Karthik Mukkavilli, Johannes Schmude, Marcus Freitag, Michael Behrendt, Daniel Salles Civitarese, Naomi Simumba, Daiki Kimura, Hendrik Hamann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Storing and streaming high dimensional data for foundation model training
became a critical requirement with the rise of foundation models beyond natural
language. In this paper we introduce TensorBank, a petabyte scale tensor
lakehouse capable of streaming tensors from Cloud Object Store (COS) to GPU
memory at wire speed based on complex relational queries. We use Hierarchical
Statistical Indices (HSI) for query acceleration. Our architecture allows to
directly address tensors on block level using HTTP range reads. Once in GPU
memory, data can be transformed using PyTorch transforms. We provide a generic
PyTorch dataset type with a corresponding dataset factory translating
relational queries and requested transformations as an instance. By making use
of the HSI, irrelevant blocks can be skipped without reading them as those
indices contain statistics on their content at different hierarchical
resolution levels. This is an opinionated architecture powered by open
standards and making heavy use of open-source technology. Although, hardened
for production use using geospatial-temporal data, this architecture
generalizes to other use case like computer vision, computational neuroscience,
biological sequence analysis and more.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Network
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CASPER: Carbon-Aware Scheduling and Provisioning for Distributed Web
  Services 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14792v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14792v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abel Souza, Shruti Jasoria, Basundhara Chakrabarty, Alexander Bridgwater, Axel Lundberg, Filip Skogh, Ahmed Ali-Eldin, David Irwin, Prashant Shenoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been a significant societal push towards sustainable practices,
including in computing. Modern interactive workloads such as geo-distributed
web-services exhibit various spatiotemporal and performance flexibility,
enabling the possibility to adapt the location, time, and intensity of
processing to align with the availability of renewable and low-carbon energy.
An example is a web application hosted across multiple cloud regions, each with
varying carbon intensity based on their local electricity mix. Distributed
load-balancing enables the exploitation of low-carbon energy through load
migration across regions, reducing web applications carbon footprint. In this
paper, we present CASPER, a carbon-aware scheduling and provisioning system
that primarily minimizes the carbon footprint of distributed web services while
also respecting their Service Level Objectives (SLO). We formulate CASPER as an
multi-objective optimization problem that considers both the variable carbon
intensity and latency constraints of the network. Our evaluation reveals the
significant potential of CASPER in achieving substantial reductions in carbon
emissions. Compared to baseline methods, CASPER demonstrates improvements of up
to 70% with no latency performance degradation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing queues with deadlines under infrequent monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Faraz Farahvash, Ao Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we aim to improve the percentage of packets meeting their
deadline in discrete-time M/M/1 queues with infrequent monitoring. More
specifically, we look into policies that only monitor the system (and
subsequently take actions) after a packet arrival. We model the system as an
MDP and provide the optimal policy for some special cases. Furthermore, we
introduce a heuristic algorithm called "AB-n" for general deadlines. Finally,
we provide numerical results demonstrating the desirable performance of "AB-n"
policies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Mathematical Introduction to Deep Reinforcement Learning for 5G/6G
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farhad Rezazadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Algorithmic innovation can unleash the potential of the beyond 5G (B5G)/6G
communication systems. Artificial intelligence (AI)-driven zero-touch network
slicing is envisaged as a promising cutting-edge technology to harness the full
potential of heterogeneous 6G networks and enable the automation of
demand-aware management and orchestration (MANO). The network slicing continues
towards numerous slices with micro or macro services in 6G networks, and
thereby, designing a robust, stable, and distributed learning mechanism is
considered a necessity. In this regard, robust brain-inspired and dopamine-like
learning methods, such as Actor-Critic approaches, can play a vital role. The
tutorial begins with an introduction to network slicing, reinforcement learning
(RL), and recent state-of-the-art (SoA) algorithms. Then, the paper elaborates
on the combination of value-based and policy-based methods in the form of
Actor-Critic techniques tailored to the needs of future wireless networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures. arXiv admin note: text overlap with
  arXiv:2211.03430</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerating Time-to-Science by <span class="highlight-title">Stream</span>ing Detector Data Directly into
  Perlmutter Compute Nodes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14352v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14352v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel S. Welborn, Bjoern Enders, Chris Harris, Peter Ercius, Deborah J. Bard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in detector technology have significantly increased the
size and complexity of experimental data, and high-performance computing (HPC)
provides a path towards more efficient and timely data processing. However,
movement of large data sets from acquisition systems to HPC centers introduces
bottlenecks owing to storage I/O at both ends. This manuscript introduces a
streaming workflow designed for an high data rate electron detector that
streams data directly to compute node memory at the National Energy Research
Scientific Computing Center (NERSC), thereby avoiding storage I/O. The new
workflow deploys ZeroMQ-based services for data production, aggregation, and
distribution for on-the-fly processing, all coordinated through a distributed
key-value store. The system is integrated with the detector's science gateway
and utilizes the NERSC Superfacility API to initiate streaming jobs through a
web-based frontend. Our approach achieves up to a 14-fold increase in data
throughput and enhances predictability and reliability compared to a I/O-heavy
file-based transfer workflow. Our work highlights the transformative potential
of streaming workflows to expedite data analysis for time-sensitive
experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Developing Algorithms for the Internet of Flying Things Through
  Environments With Varying Degrees of Realism -- Extended Version 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12753v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12753v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thiago de Souza Lamenza, Josef Kamysek, Bruno Jose Olivieri de Souza, Markus Endler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work discusses the benefits of having multiple simulated environments
with different degrees of realism for the development of algorithms in
scenarios populated by autonomous nodes capable of communication and mobility.
This approach aids the development experience and generates robust algorithms.
It also proposes GrADyS-SIM NextGen as a solution that enables development on a
single programming language and toolset over multiple environments with varying
levels of realism. Finally, we illustrate the usefulness of this approach with
a toy problem that makes use of the simulation framework, taking advantage of
the proposed environments to iteratively develop a robust solution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-20T00:00:00Z">2024-03-20</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging High-Resolution Features for Improved Deep Hashing-based
  Image Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13747v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13747v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aymene Berriche, Mehdi Adjal Zakaria, Riyadh Baghdadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep hashing techniques have emerged as the predominant approach for
efficient image retrieval. Traditionally, these methods utilize pre-trained
convolutional neural networks (CNNs) such as AlexNet and VGG-16 as feature
extractors. However, the increasing complexity of datasets poses challenges for
these backbone architectures in capturing meaningful features essential for
effective image retrieval. In this study, we explore the efficacy of employing
high-resolution features learned through state-of-the-art techniques for image
retrieval tasks. Specifically, we propose a novel methodology that utilizes
High-Resolution Networks (HRNets) as the backbone for the deep hashing task,
termed High-Resolution Hashing Network (HHNet). Our approach demonstrates
superior performance compared to existing methods across all tested benchmark
datasets, including CIFAR-10, NUS-WIDE, MS COCO, and ImageNet. This performance
improvement is more pronounced for complex datasets, which highlights the need
to learn high-resolution features for intricate image retrieval tasks.
Furthermore, we conduct a comprehensive analysis of different HRNet
configurations and provide insights into the optimal architecture for the deep
hashing task
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Large Language Model Enhanced Sequential Recommender for Joint Video
  and Comment <span class="highlight-title">Recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Zheng, Zihan Lin, Enze Liu, Chen Yang, Enyang Bai, Cheng Ling, Wayne Xin Zhao, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In online video platforms, reading or writing comments on interesting videos
has become an essential part of the video watching experience. However,
existing video recommender systems mainly model users' interaction behaviors
with videos, lacking consideration of comments in user behavior modeling. In
this paper, we propose a novel recommendation approach called LSVCR by
leveraging user interaction histories with both videos and comments, so as to
jointly conduct personalized video and comment recommendation. Specifically,
our approach consists of two key components, namely sequential recommendation
(SR) model and supplemental large language model (LLM) recommender. The SR
model serves as the primary recommendation backbone (retained in deployment) of
our approach, allowing for efficient user preference modeling. Meanwhile, we
leverage the LLM recommender as a supplemental component (discarded in
deployment) to better capture underlying user preferences from heterogeneous
interaction behaviors. In order to integrate the merits of the SR model and the
supplemental LLM recommender, we design a twostage training paradigm. The first
stage is personalized preference alignment, which aims to align the preference
representations from both components, thereby enhancing the semantics of the SR
model. The second stage is recommendation-oriented fine-tuning, in which the
alignment-enhanced SR model is fine-tuned according to specific objectives.
Extensive experiments in both video and comment recommendation tasks
demonstrate the effectiveness of LSVCR. Additionally, online A/B testing on the
KuaiShou platform verifies the actual benefits brought by our approach. In
particular, we achieve a significant overall gain of 4.13% in comment watch
time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unified Optimal Transport Framework for Cross-Modal Retrieval with
  Noisy Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haochen Han, Minnan Luo, Huan Liu, Fang Nan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-modal retrieval (CMR) aims to establish interaction between different
modalities, among which supervised CMR is emerging due to its flexibility in
learning semantic category discrimination. Despite the remarkable performance
of previous supervised CMR methods, much of their success can be attributed to
the well-annotated data. However, even for unimodal data, precise annotation is
expensive and time-consuming, and it becomes more challenging with the
multimodal scenario. In practice, massive multimodal data are collected from
the Internet with coarse annotation, which inevitably introduces noisy labels.
Training with such misleading labels would bring two key challenges --
enforcing the multimodal samples to \emph{align incorrect semantics} and
\emph{widen the heterogeneous gap}, resulting in poor retrieval performance. To
tackle these challenges, this work proposes UOT-RCL, a Unified framework based
on Optimal Transport (OT) for Robust Cross-modal Retrieval. First, we propose a
semantic alignment based on partial OT to progressively correct the noisy
labels, where a novel cross-modal consistent cost function is designed to blend
different modalities and provide precise transport cost. Second, to narrow the
discrepancy in multi-modal data, an OT-based relation alignment is proposed to
infer the semantic-level cross-modal matching. Both of these two components
leverage the inherent correlation among multi-modal data to facilitate
effective cost function. The experiments on three widely-used cross-modal
retrieval datasets demonstrate that our UOT-RCL surpasses the state-of-the-art
approaches and significantly improves the robustness against noisy labels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DESIRE-ME: Domain-Enhanced Supervised Information REtrieval using
  Mixture-of-Experts <span class="chip">ECIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13468v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13468v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranav Kasela, Gabriella Pasi, Raffaele Perego, Nicola Tonellotto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-domain question answering requires retrieval systems able to cope with
the diverse and varied nature of questions, providing accurate answers across a
broad spectrum of query types and topics. To deal with such topic heterogeneity
through a unique model, we propose DESIRE-ME, a neural information retrieval
model that leverages the Mixture-of-Experts framework to combine multiple
specialized neural models. We rely on Wikipedia data to train an effective
neural gating mechanism that classifies the incoming query and that weighs the
predictions of the different domain-specific experts correspondingly. This
allows DESIRE-ME to specialize adaptively in multiple domains. Through
extensive experiments on publicly available datasets, we show that our proposal
can effectively generalize domain-enhanced neural models. DESIRE-ME excels in
handling open-domain questions adaptively, boosting by up to 12% in NDCG@10 and
22% in P@1, the underlying state-of-the-art dense retrieval model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 46th European Conference on Information Retrieval
  (ECIR 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ USE: Dynamic User Modeling with Stateful Sequence Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihan Zhou, Qixiang Fang, Leonardo Neves, Francesco Barbieri, Yozen Liu, Han Liu, Maarten W. Bos, Ron Dotsch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User embeddings play a crucial role in user engagement forecasting and
personalized services. Recent advances in sequence modeling have sparked
interest in learning user embeddings from behavioral data. Yet behavior-based
user embedding learning faces the unique challenge of dynamic user modeling. As
users continuously interact with the apps, user embeddings should be
periodically updated to account for users' recent and long-term behavior
patterns. Existing methods highly rely on stateless sequence models that lack
memory of historical behavior. They have to either discard historical data and
use only the most recent data or reprocess the old and new data jointly. Both
cases incur substantial computational overhead. To address this limitation, we
introduce User Stateful Embedding (USE). USE generates user embeddings and
reflects users' evolving behaviors without the need for exhaustive reprocessing
by storing previous model states and revisiting them in the future.
Furthermore, we introduce a novel training objective named future W-behavior
prediction to transcend the limitations of next-token prediction by forecasting
a broader horizon of upcoming user behaviors. By combining it with the Same
User Prediction, a contrastive learning-based objective that predicts whether
different segments of behavior sequences belong to the same user, we further
improve the embeddings' distinctiveness and representativeness. We conducted
experiments on 8 downstream tasks using Snapchat users' behavioral logs in both
static (i.e., fixed user behavior sequences) and dynamic (i.e., periodically
updated user behavior sequences) settings. We demonstrate USE's superior
performance over established baselines. The results underscore USE's
effectiveness and efficiency in integrating historical and recent user behavior
sequences into user embeddings in dynamic user modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Harnessing Large Language Models for Text-Rich Sequential <span class="highlight-title">Recommendation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13325v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13325v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhi Zheng, Wenshuo Chao, Zhaopeng Qiu, Hengshu Zhu, <span class="highlight-author">Hui Xiong</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Large Language Models (LLMs) have been changing the
paradigm of Recommender Systems (RS). However, when items in the recommendation
scenarios contain rich textual information, such as product descriptions in
online shopping or news headlines on social media, LLMs require longer texts to
comprehensively depict the historical user behavior sequence. This poses
significant challenges to LLM-based recommenders, such as over-length
limitations, extensive time and space overheads, and suboptimal model
performance. To this end, in this paper, we design a novel framework for
harnessing Large Language Models for Text-Rich Sequential Recommendation
(LLM-TRSR). Specifically, we first propose to segment the user historical
behaviors and subsequently employ an LLM-based summarizer for summarizing these
user behavior blocks. Particularly, drawing inspiration from the successful
application of Convolutional Neural Networks (CNN) and Recurrent Neural
Networks (RNN) models in user modeling, we introduce two unique summarization
techniques in this paper, respectively hierarchical summarization and recurrent
summarization. Then, we construct a prompt text encompassing the user
preference summary, recent user interactions, and candidate item information
into an LLM-based recommender, which is subsequently fine-tuned using
Supervised Fine-Tuning (SFT) techniques to yield our final recommendation
model. We also use Low-Rank Adaptation (LoRA) for Parameter-Efficient
Fine-Tuning (PEFT). We conduct experiments on two public datasets, and the
results clearly demonstrate the effectiveness of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Flickr30K-CFQ: A Compact and Fragmented Query <span class="highlight-title">Dataset</span> for Text-image
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Liu, Yaoxian Song, Xuwu Wang, Zhu Xiangru, Zhixu Li, Wei Song, Tiefeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the explosive growth of multi-modal information on the Internet,
unimodal search cannot satisfy the requirement of Internet applications.
Text-image retrieval research is needed to realize high-quality and efficient
retrieval between different modalities. Existing text-image retrieval research
is mostly based on general vision-language datasets (e.g. MS-COCO, Flickr30K),
in which the query utterance is rigid and unnatural (i.e. verbosity and
formality). To overcome the shortcoming, we construct a new Compact and
Fragmented Query challenge dataset (named Flickr30K-CFQ) to model text-image
retrieval task considering multiple query content and style, including compact
and fine-grained entity-relation corpus. We propose a novel query-enhanced
text-image retrieval method using prompt engineering based on LLM. Experiments
show that our proposed Flickr30-CFQ reveals the insufficiency of existing
vision-language datasets in realistic text-image tasks. Our LLM-based
Query-enhanced method applied on different existing text-image retrieval models
improves query understanding performance both on public dataset and our
challenge set Flickr30-CFQ with over 0.9% and 2.4% respectively. Our project
can be available anonymously in https://sites.google.com/view/Flickr30K-cfq.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Semantic Search Engine for Mathlib4 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoxiong Gao, Haocheng Ju, Jiedong Jiang, Zihan Qin, Bin Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The interactive theorem prover, Lean, enables the verification of formal
mathematical proofs and is backed by an expanding community. Central to this
ecosystem is its mathematical library, mathlib4, which lays the groundwork for
the formalization of an expanding range of mathematical theories. However,
searching for theorems in mathlib4 can be challenging. To successfully search
in mathlib4, users often need to be familiar with its naming conventions or
documentation strings. Therefore, creating a semantic search engine that can be
used easily by individuals with varying familiarity with mathlib4 is very
important. In this paper, we present a semantic search engine for mathlib4 that
accepts informal queries and finds the relevant theorems. We also establish a
benchmark for assessing the performance of various search engines for mathlib4.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Analysis on Matching Mechanisms and Token Pruning for
  Late-interaction Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13291v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13291v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Liu, Gang Guo, Jiaxin Mao, Zhicheng Dou, Ji-Rong Wen, Hao Jiang, Xinyu Zhang, Zhao Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of pre-trained language models, the dense retrieval
models have become promising alternatives to the traditional retrieval models
that rely on exact match and sparse bag-of-words representations. Different
from most dense retrieval models using a bi-encoder to encode each query or
document into a dense vector, the recently proposed late-interaction
multi-vector models (i.e., ColBERT and COIL) achieve state-of-the-art retrieval
effectiveness by using all token embeddings to represent documents and queries
and modeling their relevance with a sum-of-max operation. However, these
fine-grained representations may cause unacceptable storage overhead for
practical search systems. In this study, we systematically analyze the matching
mechanism of these late-interaction models and show that the sum-of-max
operation heavily relies on the co-occurrence signals and some important words
in the document. Based on these findings, we then propose several simple
document pruning methods to reduce the storage overhead and compare the
effectiveness of different pruning methods on different late-interaction
models. We also leverage query pruning methods to further reduce the retrieval
latency. We conduct extensive experiments on both in-domain and out-domain
datasets and show that some of the used pruning methods can significantly
improve the efficiency of these late-interaction models without substantially
hurting their retrieval effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM Transactions on Information Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Legal Case Retrieval with Brain Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13242v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13242v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruizhe Zhang, Qingyao Ai, Ziyi Ye, Yueyue Wu, Xiaohui Xie, Yiqun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The tasks of legal case retrieval have received growing attention from the IR
community in the last decade. Relevance feedback techniques with implicit user
feedback (e.g., clicks) have been demonstrated to be effective in traditional
search tasks (e.g., Web search). In legal case retrieval, however, collecting
relevance feedback faces a couple of challenges that are difficult to resolve
under existing feedback paradigms. First, legal case retrieval is a complex
task as users often need to understand the relationship between legal cases in
detail to correctly judge their relevance. Traditional feedback signal such as
clicks is too coarse to use as they do not reflect any fine-grained relevance
information. Second, legal case documents are usually long, users often need
even tens of minutes to read and understand them. Simple behavior signal such
as clicks and eye-tracking fixations can hardly be useful when users almost
click and examine every part of the document. In this paper, we explore the
possibility of solving the feedback problem in legal case retrieval with brain
signal. Recent advances in brain signal processing have shown that human
emotional can be collected in fine grains through Brain-Machine Interfaces
(BMI) without interrupting the users in their tasks. Therefore, we propose a
framework for legal case retrieval that uses EEG signal to optimize retrieval
results. We collected and create a legal case retrieval dataset with users EEG
signal and propose several methods to extract effective EEG features for
relevance feedback. Our proposed features achieve a 71% accuracy for feedback
prediction with an SVM-RFE model, and our proposed ranking method that takes
into account the diverse needs of users can significantly improve user
satisfaction for legal case retrieval. Experiment results show that re-ranked
result list make user more satisfied.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RecMind: Large Language Model Powered Agent For <span class="highlight-title">Recommendation</span> <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.14296v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.14296v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yancheng Wang, Ziyan Jiang, Zheng Chen, Fan Yang, Yingxue Zhou, Eunah Cho, Xing Fan, Xiaojiang Huang, Yanbin Lu, Yingzhen Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the recommendation system (RS) has advanced significantly through deep
learning, current RS approaches usually train and fine-tune models on
task-specific datasets, limiting their generalizability to new recommendation
tasks and their ability to leverage external knowledge due to model scale and
data size constraints. Thus, we designed an LLM-powered autonomous recommender
agent, RecMind, which is capable of leveraging external knowledge, utilizing
tools with careful planning to provide zero-shot personalized recommendations.
We propose a Self-Inspiring algorithm to improve the planning ability. At each
intermediate step, the LLM self-inspires to consider all previously explored
states to plan for the next step. This mechanism greatly improves the model's
ability to comprehend and utilize historical information in planning for
recommendation. We evaluate RecMind's performance in various recommendation
scenarios. Our experiment shows that RecMind outperforms existing zero/few-shot
LLM-based recommendation baseline methods in various tasks and achieves
comparable performance to a fully trained recommendation model P5.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NAACL 2024 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigating the Effects of Sparse Attention on Cross-Encoders <span class="chip">ECIR'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17649v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17649v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ferdinand Schlatt, Maik Fröbe, Matthias Hagen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-encoders are effective passage and document re-rankers but less
efficient than other neural or classic retrieval models. A few previous studies
have applied windowed self-attention to make cross-encoders more efficient.
However, these studies did not investigate the potential and limits of
different attention patterns or window sizes. We close this gap and
systematically analyze how token interactions can be reduced without harming
the re-ranking effectiveness. Experimenting with asymmetric attention and
different window sizes, we find that the query tokens do not need to attend to
the passage or document tokens for effective re-ranking and that very small
window sizes suffice. In our experiments, even windows of 4 tokens still yield
effectiveness on par with previous cross-encoders while reducing the memory
requirements by at least 22% / 59% and being 1% / 43% faster at inference time
for passages / documents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECIR'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WaZI: A Learned and Workload-aware Z-Index 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04268v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04268v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sachith Pai, Michael Mathioudakis, Yanhao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learned indexes fit machine learning (ML) models to the data and use them to
make query operations more time and space-efficient. Recent works propose using
learned spatial indexes to improve spatial query performance by optimizing the
storage layout or internal search structures according to the data
distribution. However, only a few learned indexes exploit the query workload
distribution to enhance their performance. In addition, building and updating
learned spatial indexes are often costly on large datasets due to the
inefficiency of (re)training ML models. In this paper, we present WaZI, a
learned and workload-aware variant of the Z-index, which jointly optimizes the
storage layout and search structures, as a viable solution for the above
challenges of spatial indexing. Specifically, we first formulate a cost
function to measure the performance of a Z-index on a dataset for a range-query
workload. Then, we optimize the Z-index structure by minimizing the cost
function through adaptive partitioning and ordering for index construction.
Moreover, we design a novel page-skipping mechanism to improve the query
performance of WaZI by reducing access to irrelevant data pages. Our extensive
experiments show that the WaZI index improves range query time by 40% on
average over the baselines while always performing better or comparably to
state-of-the-art spatial indexes. Additionally, it also maintains good point
query performance. Generally, WaZI provides favorable tradeoffs among query
latency, construction time, and index size.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version accepted to EDBT 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ERASE: <span class="highlight-title">Benchmark</span>ing Feature Selection Methods for Deep Recommender
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12660v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12660v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengyue Jia, Yejing Wang, Zhaocheng Du, Xiangyu Zhao, Yichao Wang, Bo Chen, Wanyu Wang, Huifeng Guo, Ruiming Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Recommender Systems (DRS) are increasingly dependent on a large number
of feature fields for more precise recommendations. Effective feature selection
methods are consequently becoming critical for further enhancing the accuracy
and optimizing storage efficiencies to align with the deployment demands. This
research area, particularly in the context of DRS, is nascent and faces three
core challenges. Firstly, variant experimental setups across research papers
often yield unfair comparisons, obscuring practical insights. Secondly, the
existing literature's lack of detailed analysis on selection attributes, based
on large-scale datasets and a thorough comparison among selection techniques
and DRS backbones, restricts the generalizability of findings and impedes
deployment on DRS. Lastly, research often focuses on comparing the peak
performance achievable by feature selection methods, an approach that is
typically computationally infeasible for identifying the optimal
hyperparameters and overlooks evaluating the robustness and stability of these
methods. To bridge these gaps, this paper presents ERASE, a comprehensive
bEnchmaRk for feAture SElection for DRS. ERASE comprises a thorough evaluation
of eleven feature selection methods, covering both traditional and deep
learning approaches, across four public datasets, private industrial datasets,
and a real-world commercial platform, achieving significant enhancement. Our
code is available online for ease of reproduction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Aligning and Training Framework for Multimodal <span class="highlight-title">Recommendation</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12384v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12384v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Liu, Kangning Zhang, Xiangyuan Ren, Yanhua Huang, Jiarui Jin, Yingjie Qin, Ruilong Su, Ruiwen Xu, Weinan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of multimedia applications, multimodal recommendations
are playing an essential role, as they can leverage rich contexts beyond user
interactions. Existing methods mainly regard multimodal information as an
auxiliary, using them to help learn ID features; however, there exist semantic
gaps among multimodal content features and ID features, for which directly
using multimodal information as an auxiliary would lead to misalignment in
representations of users and items. In this paper, we first systematically
investigate the misalignment issue in multimodal recommendations, and propose a
solution named AlignRec. In AlignRec, the recommendation objective is
decomposed into three alignments, namely alignment within contents, alignment
between content and categorical ID, and alignment between users and items. Each
alignment is characterized by a specific objective function and is integrated
into our multimodal recommendation framework. To effectively train our
AlignRec, we propose starting from pre-training the first alignment to obtain
unified multimodal features and subsequently training the following two
alignments together with these features as input. As it is essential to analyze
whether each multimodal feature helps in training, we design three new classes
of metrics to evaluate intermediate performance. Our extensive experiments on
three real-world datasets consistently verify the superiority of AlignRec
compared to nine baselines. We also find that the multimodal features generated
by AlignRec are better than currently used ones, which are to be open-sourced.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, add some necessary explanations, revise typos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ I^3 Retriever: Incorporating Implicit Interaction in Pre-trained
  Language Models for Passage Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02371v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02371v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Dong, Yiding Liu, Qingyao Ai, Haitao Li, Shuaiqiang Wang, Yiqun Liu, Dawei Yin, Shaoping Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Passage retrieval is a fundamental task in many information systems, such as
web search and question answering, where both efficiency and effectiveness are
critical concerns. In recent years, neural retrievers based on pre-trained
language models (PLM), such as dual-encoders, have achieved huge success. Yet,
studies have found that the performance of dual-encoders are often limited due
to the neglecting of the interaction information between queries and candidate
passages. Therefore, various interaction paradigms have been proposed to
improve the performance of vanilla dual-encoders. Particularly, recent
state-of-the-art methods often introduce late-interaction during the model
inference process. However, such late-interaction based methods usually bring
extensive computation and storage cost on large corpus. Despite their
effectiveness, the concern of efficiency and space footprint is still an
important factor that limits the application of interaction-based neural
retrieval models. To tackle this issue, we incorporate implicit interaction
into dual-encoders, and propose I^3 retriever. In particular, our implicit
interaction paradigm leverages generated pseudo-queries to simulate
query-passage interaction, which jointly optimizes with query and passage
encoders in an end-to-end manner. It can be fully pre-computed and cached, and
its inference process only involves simple dot product operation of the query
vector and passage vector, which makes it as efficient as the vanilla dual
encoders. We conduct comprehensive experiments on MSMARCO and TREC2019 Deep
Learning Datasets, demonstrating the I^3 retriever's superiority in terms of
both effectiveness and efficiency. Moreover, the proposed implicit interaction
is compatible with special pre-training and knowledge distillation for passage
retrieval, which brings a new state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Network
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-Knowledge Proof of Distinct Identity: a Standard-compatible
  Sybil-resistant Pseudonym Extension for C-ITS 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14020v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14020v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye Tao, Hongyi Wu, Ehsan Javanmardi, Manabu Tsukada, Hiroshi Esaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pseudonyms are widely used in Cooperative Intelligent Transport Systems
(C-ITS) to protect the location privacy of vehicles. However, the unlinkability
nature of pseudonyms also enables Sybil attacks, where a malicious vehicle can
pretend to be multiple vehicles at the same time. In this paper, we propose a
novel protocol called zero-knowledge Proof of Distinct Identity (zk-PoDI,)
which allows a vehicle to prove that it is not the owner of another pseudonym
in the local area, without revealing its actual identity. Zk-PoDI is based on
the Diophantine equation and zk-SNARK, and does not rely on any specific
pseudonym design or infrastructure assistance. We show that zk-PoDI satisfies
all the requirements for a practical Sybil-resistance pseudonym system, and it
has low latency, adjustable difficulty, moderate computation overhead, and
negligible communication cost. We also discuss the future work of implementing
and evaluating zk-PoDI in a realistic city-scale simulation environment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Secure and Efficient Group Handover Protocol in 5G Non-Terrestrial
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13936v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13936v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohan Zhang, Peng Hu, Ahmad Akbari Azirani, Mohammad A. Salahuddin, Diogo Barradas, Noura Limam, Raouf Boutaba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing low-Earth orbit (LEO) satellite constellations have become an
essential part of the fifth-generation (5G) non-terrestrial network (NTN)
market. These satellites can enable direct-to-cell connectivity for mobile
devices and support various applications with ubiquitous coverage for 5G and
beyond networks. However, satellite-based NTNs bring several challenges to the
5G handover protocol design. The high mobility of satellites can lead to
signaling storms and security compromises during handovers. This paper
addresses these challenges by proposing a secure and efficient group handover
protocol. The protocol's effectiveness is evaluated on a custom discrete-event
simulator and compared against the baseline 5G handover scheme. The simulator
is made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 2024 IEEE International Conference on Communications
  (ICC), 9-13 June 2024, Denver, CO, USA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models meet Network Slicing Management and Orchestration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13721v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13721v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdulhalim Dandoush, Viswanath Kumarskandpriya, Mueen Uddin, Usman Khalil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Network slicing, a cornerstone technology for future networks, enables the
creation of customized virtual networks on a shared physical infrastructure.
This fosters innovation and agility by providing dedicated resources tailored
to specific applications. However, current orchestration and management
approaches face limitations in handling the complexity of new service demands
within multi-administrative domain environments. This paper proposes a future
vision for network slicing powered by Large Language Models (LLMs) and
multi-agent systems, offering a framework that can be integrated with existing
Management and Orchestration (MANO) frameworks. This framework leverages LLMs
to translate user intent into technical requirements, map network functions to
infrastructure, and manage the entire slice lifecycle, while multi-agent
systems facilitate collaboration across different administrative domains. We
also discuss the challenges associated with implementing this framework and
potential solutions to mitigate them.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Network Calculus Bounds for Time-Sensitive Networks: A Revisit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuming Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Network calculus (NC), particularly its min-plus branch, has been extensively
utilized to construct service models and compute delay bounds for
time-sensitive networks (TSNs). This paper provides a revisit to the
fundamental results. In particular, counterexamples to the most basic min-plus
service models, which have been proposed for TSNs and used for computing delay
bounds, indicate that the packetization effect has often been overlooked. To
address, the max-plus branch of NC is also considered in this paper, whose
models handle packetized traffic more explicitly. It is found that mapping the
min-plus models to the max-plus models may bring in an immediate improvement
over delay bounds derived from the min-plus analysis. In addition, an
integrated analytical approach that combines models from both the min-plus and
the max-plus NC branches is introduced. In this approach, the max-plus
$g$-server model is extended and the extended model, called $g^{x}$-server, is
used together with the min-plus arrival curve traffic model. By applying the
integrated NC approach, service and delay bounds are derived for several
settings that are fundamental in TSNs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Densify & Conquer: Densified, smaller base-stations can conquer the
  increasing carbon footprint problem in nextG wireless 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Agrim Gupta, Adel Heidari, Jiaming Jin, Dinesh Bharadia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Connectivity on-the-go has been one of the most impressive technological
achievements in the 2010s decade. However, multiple studies show that this has
come at an expense of increased carbon footprint, that also rivals the entire
aviation sector's carbon footprint. The two major contributors of this
increased footprint are (a) smartphone batteries which affect the embodied
footprint and (b) base-stations that occupy ever-increasing energy footprint to
provide the last mile wireless connectivity to smartphones. The root-cause of
both these turn out to be the same, which is communicating over the last-mile
lossy wireless medium. We show in this paper, titled DensQuer, how base-station
densification, which is to replace a single larger base-station with multiple
smaller ones, reduces the effect of the last-mile wireless, and in effect
conquers both these adverse sources of increased carbon footprint. Backed by a
open-source ray-tracing computation framework (Sionna), we show how a strategic
densification strategy can minimize the number of required smaller
base-stations to practically achievable numbers, which lead to about 3x
power-savings in the base-station network. Also, DensQuer is able to also
reduce the required deployment height of base-stations to as low as 15m, that
makes the smaller cells easily deployable on trees/street poles instead of
requiring a dedicated tower. Further, by utilizing newly introduced hardware
power rails in Google Pixel 7a and above phones, we also show that this
strategic densified network leads to reduction in mobile transmit power by
10-15 dB, leading to about 3x reduction in total cellular power consumption,
and about 50% increase in smartphone battery life when it communicates data via
the cellular network.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Starlink on the Road: A First Look at <span class="highlight-title">Mobile</span> Starlink Performance in
  Central Europe 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominic Laniewski, Eric Lanfer, Simon Beginn, Jan Dunker, Michael Dückers, Nils Aschenbruck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low Earth Orbit Satellite Networks such as Starlink promise to provide
world-wide Internet access. While traditionally designed for stationary use, a
new dish, released in April 2023 in Europe, provides mobile Internet access
including in-motion usage, e.g., while mounted on a car. In this paper, we
design and build a mobile measurement setup. Our goal is to fully autonomously
conduct continuous Starlink measurements while the car is in motion. We share
our practical experiences, including challenges regarding the permanent power
supply. We measure the Starlink performance over the span of two months from
mid-January to mid-March 2024 when the car is in motion. The measurements
consist of all relevant network parameters, such as the download and upload
throughput, the RTT, and packet loss, as well as detailed power consumption
data. We analyze our dataset to assess Starlink's mobile performance in Central
Europe, Germany, and compare it to stationary measurements in proximity. We
find that the mobile performance is significantly worse than stationary
performance. The power consumption of the new dish is higher, but seems to be
more correlated to the heating function of the dish than to the speed of the
vehicle.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the 2024 Network Traffic Measurement
  and Analysis Conference (TMA) for possible publication. Copyright may be
  transferred without notice, after which this version may no longer be
  accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BFT-PoLoc: A Byzantine Fortified Trigonometric Proof of Location
  Protocol using Internet Delays 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13230v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13230v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiyao Sheng, Vishal Sevani, Himanshu Tyagi, Pramod Viswanath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Internet platforms depend on accurately determining the geographical
locations of online users to deliver targeted services (e.g., advertising). The
advent of decentralized platforms (blockchains) emphasizes the importance of
geographically distributed nodes, making the validation of locations more
crucial. In these decentralized settings, mutually non-trusting participants
need to {\em prove} their locations to each other. The incentives for claiming
desired location include decentralization properties (validators of a
blockchain), explicit rewards for improving coverage (physical infrastructure
blockchains) and regulatory compliance -- and entice participants towards
prevaricating their true location malicious via VPNs, tampering with internet
delays, or compromising other parties (challengers) to misrepresent their
location. Traditional delay-based geolocation methods focus on reducing the
noise in measurements and are very vulnerable to wilful divergences from
prescribed protocol.
  In this paper we use Internet delay measurements to securely prove the
location of IP addresses while being immune to a large fraction of Byzantine
actions. Our core methods are to endow Internet telemetry tools (e.g., ping)
with cryptographic primitives (signatures and hash functions) together with
Byzantine resistant data inferences subject to Euclidean geometric constraints.
We introduce two new networking protocols, robust against Byzantine actions:
Proof of Internet Geometry (PoIG) converts delay measurements into precise
distance estimates across the Internet; Proof of Location (PoLoc) enables
accurate and efficient multilateration of a specific IP address. The key
algorithmic innovations are in conducting ``Byzantine fortified trigonometry"
(BFT) inferences of data, endowing low rank matrix completion methods with
Byzantine resistance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 5G cellular systems supported by UAVs, RESs, and RISs (PL: Systemy
  komórkowe 5G wspierane przez BSP, OZE oraz IPR) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12296v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12296v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Samorzewski, Adrian Kliks
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper considers energy consumption in 5G cellular networks powered by
Renewable Energy Sources (RESs) and equipped with Reconfigurable Intelligent
Surfaces (RISs) and tethered Unmanned Aerial Vehicles (UAVs), acting as mobile
access points. The study was focused on the energy side of the Radio Access
Network (RAN) located in the city of Pozna\'n in Poland. The profit associated
with the use of renewable energy generators, i.e. photovoltaic panels (PVP) for
base stations (BSs) is presented in the form of two factors: the average number
of UAV charges (ANUC) to provide continuous access to mobile services for
connected user equipment (UE) terminals, and the average reduction in energy
consumption (AREC) of the wireless system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The scientific paper is written in Polish. The contribution of the
  paper was presented during the 2023 Radiocommunications and ICT Conference
  (pl. Konferencja Radiokomunikacji i Teleinformatyki - KRiT 2023) in Cracow,
  Poland</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-03-28T05:26:25.171709312Z">
            2024-03-28 05:26:25 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
